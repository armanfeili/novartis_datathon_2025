# LightGBM model configuration for Novartis Datathon 2025

model:
  name: "lightgbm"
  task: "regression"  # regression, binary, multiclass

# LightGBM hyperparameters
params:
  # Core parameters
  boosting_type: "gbdt"       # gbdt, dart, goss
  objective: "regression"     # regression, binary, multiclass
  metric: "rmse"              # rmse, mae, binary_logloss, multi_logloss
  
  # Tree parameters
  num_leaves: 31
  max_depth: -1               # -1 means no limit
  min_data_in_leaf: 20
  min_sum_hessian_in_leaf: 1e-3
  
  # Learning parameters
  learning_rate: 0.05
  n_estimators: 1000
  
  # Regularization
  lambda_l1: 0.0
  lambda_l2: 0.0
  feature_fraction: 0.8
  bagging_fraction: 0.8
  bagging_freq: 5
  
  # Categorical handling
  cat_smooth: 10
  
  # Early stopping
  early_stopping_rounds: 50
  
  # Other
  verbose: -1
  n_jobs: -1
  seed: 42

# Training settings
training:
  use_early_stopping: true
  eval_metric: "rmse"
  verbose_eval: 100

# Hyperparameter tuning (Optuna)
tuning:
  enabled: false
  n_trials: 100
  timeout: 3600  # seconds
  search_space:
    num_leaves: [15, 255]
    max_depth: [3, 12]
    learning_rate: [0.01, 0.3]
    feature_fraction: [0.5, 1.0]
    bagging_fraction: [0.5, 1.0]
    lambda_l1: [0.0, 10.0]
    lambda_l2: [0.0, 10.0]
