{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d532273",
   "metadata": {},
   "source": [
    "# üß¨ Novartis Datathon 2025 - Main Training Notebook\n",
    "\n",
    "This notebook provides full access to the project pipeline:\n",
    "1. **Environment Setup** - Mount Drive, clone repo, install dependencies\n",
    "2. **Data Exploration** - Load and explore the raw data  \n",
    "3. **Feature Engineering** - Build features from raw data\n",
    "4. **Model Training** - Train models with cross-validation\n",
    "5. **Evaluation** - Analyze model performance\n",
    "6. **Inference** - Generate predictions for submission\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7cb47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"‚úÖ Google Drive mounted successfully\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not running in Colab - using local paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd0aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "REPO_URL = \"https://github.com/armanfeili/novartis_datathon_2025.git\"\n",
    "BRANCH = \"Arman\"  # Change this if you are working on a different branch\n",
    "\n",
    "# Paths depend on environment\n",
    "if IN_COLAB:\n",
    "    DRIVE_BASE = \"/content/drive/MyDrive\"\n",
    "    PROJECT_PATH = f\"{DRIVE_BASE}/novartis_datathon_2025\"\n",
    "    DATA_PATH = f\"{DRIVE_BASE}/novartis-datathon-2025/data\"\n",
    "    ARTIFACTS_PATH = f\"{DRIVE_BASE}/novartis-datathon-2025/artifacts\"\n",
    "    SUBMISSIONS_PATH = f\"{DRIVE_BASE}/novartis-datathon-2025/submissions\"\n",
    "else:\n",
    "    # Local development paths (relative to repo root)\n",
    "    PROJECT_PATH = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))))\n",
    "    DATA_PATH = os.path.join(PROJECT_PATH, \"data\")\n",
    "    ARTIFACTS_PATH = os.path.join(PROJECT_PATH, \"artifacts\")\n",
    "    SUBMISSIONS_PATH = os.path.join(PROJECT_PATH, \"submissions\")\n",
    "\n",
    "# ---------------------\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not os.path.exists(PROJECT_PATH):\n",
    "        print(f\"üì• Cloning repository to {PROJECT_PATH}...\")\n",
    "        !git clone {REPO_URL} {PROJECT_PATH}\n",
    "    else:\n",
    "        print(f\"üìÇ Repository exists at {PROJECT_PATH}. Pulling latest changes...\")\n",
    "        %cd {PROJECT_PATH}\n",
    "        !git fetch origin {BRANCH}\n",
    "        !git reset --hard origin/{BRANCH}\n",
    "\n",
    "    %cd {PROJECT_PATH}\n",
    "    \n",
    "# Create required directories\n",
    "for path in [DATA_PATH, ARTIFACTS_PATH, SUBMISSIONS_PATH]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    os.makedirs(os.path.join(path, \"raw\") if \"data\" in path else path, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìÅ Project Path: {PROJECT_PATH}\")\n",
    "print(f\"üìÅ Data Path: {DATA_PATH}\")\n",
    "print(f\"üìÅ Artifacts Path: {ARTIFACTS_PATH}\")\n",
    "print(f\"üìÅ Submissions Path: {SUBMISSIONS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd7f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Verify key packages\n",
    "import importlib\n",
    "packages = ['torch', 'numpy', 'pandas', 'lightgbm', 'xgboost', 'catboost', 'sklearn', 'yaml']\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        importlib.import_module(pkg if pkg != 'sklearn' else 'sklearn')\n",
    "        print(f\"  ‚úÖ {pkg}\")\n",
    "    except ImportError:\n",
    "        print(f\"  ‚ùå {pkg} - Installing...\")\n",
    "        !pip install -q {pkg if pkg != 'yaml' else 'pyyaml'}\n",
    "        !pip install -q {pkg if pkg != 'sklearn' else 'scikit-learn'}\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a8cd44",
   "metadata": {},
   "source": [
    "## 2. Import Project Modules\n",
    "\n",
    "Import all necessary modules from the `src/` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63474d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure project root is in path\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.insert(0, os.getcwd())\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Project imports\n",
    "from src.utils import load_config, set_seed, setup_logging, timer, get_device\n",
    "from src.data import DataManager\n",
    "from src.features import FeatureEngineer\n",
    "from src.validation import Validator\n",
    "from src.evaluate import Evaluator\n",
    "from src.train import run_experiment, get_model_class\n",
    "\n",
    "# Model imports\n",
    "from src.models.lgbm_model import LGBMModel\n",
    "from src.models.xgb_model import XGBModel\n",
    "from src.models.cat_model import CatBoostModel\n",
    "from src.models.linear import LinearModel\n",
    "from src.models.nn import NNModel\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check GPU\n",
    "device = get_device()\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "if 'cuda' in str(device):\n",
    "    import torch\n",
    "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "print(\"\\n‚úÖ All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286fbdcc",
   "metadata": {},
   "source": [
    "## 3. Load Configurations\n",
    "\n",
    "Load all configuration files for data, features, and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all configurations\n",
    "data_config = load_config('configs/data.yaml')\n",
    "features_config = load_config('configs/features.yaml')\n",
    "run_config = load_config('configs/run_defaults.yaml')\n",
    "\n",
    "# Available model configs\n",
    "model_configs = {\n",
    "    'lightgbm': load_config('configs/model_lgbm.yaml'),\n",
    "    'xgboost': load_config('configs/model_xgb.yaml'),\n",
    "    'catboost': load_config('configs/model_cat.yaml'),\n",
    "    'linear': load_config('configs/model_linear.yaml'),\n",
    "    'neural_network': load_config('configs/model_nn.yaml'),\n",
    "}\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = run_config['reproducibility']['seed']\n",
    "set_seed(SEED)\n",
    "\n",
    "print(\"üìã Configurations loaded:\")\n",
    "print(f\"  - Data config: {list(data_config.keys())}\")\n",
    "print(f\"  - Features config: {list(features_config.keys())}\")\n",
    "print(f\"  - Run config: {list(run_config.keys())}\")\n",
    "print(f\"  - Model configs: {list(model_configs.keys())}\")\n",
    "print(f\"\\nüé≤ Random seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafc46c9",
   "metadata": {},
   "source": [
    "## 4. Data Loading & Exploration\n",
    "\n",
    "Load raw data and perform exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4881a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataManager\n",
    "data_mgr = DataManager(data_config)\n",
    "\n",
    "# Check data directories\n",
    "print(\"üìÇ Data Directories:\")\n",
    "print(f\"  Raw: {data_mgr.raw_dir} (exists: {data_mgr.raw_dir.exists()})\")\n",
    "print(f\"  Interim: {data_mgr.interim_dir} (exists: {data_mgr.interim_dir.exists()})\")\n",
    "print(f\"  Processed: {data_mgr.processed_dir} (exists: {data_mgr.processed_dir.exists()})\")\n",
    "\n",
    "# List available data files\n",
    "if data_mgr.raw_dir.exists():\n",
    "    raw_files = list(data_mgr.raw_dir.glob(\"*.csv\")) + list(data_mgr.raw_dir.glob(\"*.parquet\"))\n",
    "    print(f\"\\nüìÑ Available raw files ({len(raw_files)}):\")\n",
    "    for f in raw_files:\n",
    "        size_mb = f.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  - {f.name} ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Raw data directory not found. Please upload your data files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c8aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "# Note: Update configs/data.yaml with your actual file names\n",
    "raw_data = data_mgr.load_raw_data()\n",
    "\n",
    "print(f\"üìä Loaded {len(raw_data)} datasets:\")\n",
    "for name, df in raw_data.items():\n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"    Shape: {df.shape}\")\n",
    "    print(f\"    Columns: {list(df.columns)[:10]}{'...' if len(df.columns) > 10 else ''}\")\n",
    "    print(f\"    Memory: {df.memory_usage(deep=True).sum() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ae59ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick EDA helper functions\n",
    "def quick_eda(df, name=\"Dataset\"):\n",
    "    \"\"\"Perform quick exploratory data analysis on a dataframe.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä EDA: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\nüìê Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nüìã Data Types:\")\n",
    "    for dtype, count in df.dtypes.value_counts().items():\n",
    "        print(f\"  - {dtype}: {count}\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è Missing Values:\")\n",
    "        for col in missing[missing > 0].index:\n",
    "            pct = missing[col] / len(df) * 100\n",
    "            print(f\"  - {col}: {missing[col]:,} ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ No missing values\")\n",
    "    \n",
    "    # Numeric summary\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nüìà Numeric Summary (first 5 columns):\")\n",
    "        display(df[numeric_cols[:5]].describe())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run EDA on loaded data\n",
    "for name, df in raw_data.items():\n",
    "    quick_eda(df, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba3b589",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "Build features from the raw/interim data using the FeatureEngineer module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96056c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interim data from raw data\n",
    "interim_df = data_mgr.make_interim(raw_data)\n",
    "\n",
    "# Initialize FeatureEngineer\n",
    "fe = FeatureEngineer(features_config)\n",
    "\n",
    "# Build features\n",
    "if not interim_df.empty:\n",
    "    processed_df = fe.build_features(interim_df)\n",
    "    print(f\"‚úÖ Features built: {processed_df.shape}\")\n",
    "    print(f\"üìã Feature columns: {list(processed_df.columns)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No interim data available. Please check your data loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22178dd4",
   "metadata": {},
   "source": [
    "## 6. Model Training\n",
    "\n",
    "Train models using cross-validation. Choose from available models:\n",
    "- `lightgbm` - LightGBM gradient boosting\n",
    "- `xgboost` - XGBoost gradient boosting\n",
    "- `catboost` - CatBoost gradient boosting\n",
    "- `linear` - Linear/Ridge regression\n",
    "- `neural_network` - PyTorch neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22078bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experiment Configuration ---\n",
    "MODEL_NAME = \"lightgbm\"  # Options: lightgbm, xgboost, catboost, linear, neural_network\n",
    "RUN_NAME = \"colab_experiment_01\"  # Custom name for this run\n",
    "MODEL_CONFIG_PATH = f\"configs/model_lgbm.yaml\"  # Path to model config\n",
    "\n",
    "# Map model names to config paths\n",
    "MODEL_CONFIG_MAP = {\n",
    "    'lightgbm': 'configs/model_lgbm.yaml',\n",
    "    'xgboost': 'configs/model_xgb.yaml',\n",
    "    'catboost': 'configs/model_cat.yaml',\n",
    "    'linear': 'configs/model_linear.yaml',\n",
    "    'neural_network': 'configs/model_nn.yaml',\n",
    "}\n",
    "\n",
    "MODEL_CONFIG_PATH = MODEL_CONFIG_MAP.get(MODEL_NAME, MODEL_CONFIG_PATH)\n",
    "# --------------------------------\n",
    "\n",
    "print(f\"üèÉ Experiment Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Config: {MODEL_CONFIG_PATH}\")\n",
    "print(f\"  Run Name: {RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417260a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "print(f\"\\nüöÄ Starting training run: {RUN_NAME}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "try:\n",
    "    run_id, metrics = run_experiment(\n",
    "        model_name=MODEL_NAME,\n",
    "        model_config_path=MODEL_CONFIG_PATH,\n",
    "        run_name=RUN_NAME,\n",
    "        config_path='configs/run_defaults.yaml'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ Training complete!\")\n",
    "    print(f\"üìÅ Run ID: {run_id}\")\n",
    "    print(f\"\\nüìä Final Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  - {metric}: {value:.6f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    print(\"\\nüí° Make sure you have:\")\n",
    "    print(\"  1. Uploaded data to the raw directory\")\n",
    "    print(\"  2. Updated configs/data.yaml with correct file names\")\n",
    "    print(\"  3. Set the target column in configs/data.yaml\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd0994",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Analysis\n",
    "\n",
    "Analyze model performance and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from the latest run\n",
    "artifacts_base = Path(run_config['paths']['artifacts_dir'])\n",
    "\n",
    "# Find the latest run (or use specific run_id)\n",
    "if 'run_id' in dir():\n",
    "    latest_run = artifacts_base / run_id\n",
    "else:\n",
    "    runs = sorted(artifacts_base.glob(\"*\"), key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    latest_run = runs[0] if runs else None\n",
    "\n",
    "if latest_run and latest_run.exists():\n",
    "    print(f\"üìÇ Analyzing run: {latest_run.name}\\n\")\n",
    "    \n",
    "    # Load metrics\n",
    "    metrics_path = latest_run / \"metrics.json\"\n",
    "    if metrics_path.exists():\n",
    "        with open(metrics_path) as f:\n",
    "            saved_metrics = json.load(f)\n",
    "        print(\"üìä Saved Metrics:\")\n",
    "        for k, v in saved_metrics.items():\n",
    "            print(f\"  - {k}: {v:.6f}\")\n",
    "    \n",
    "    # Load OOF predictions\n",
    "    oof_path = latest_run / \"oof_preds.csv\"\n",
    "    if oof_path.exists():\n",
    "        oof_df = pd.read_csv(oof_path)\n",
    "        print(f\"\\nüìà OOF Predictions: {len(oof_df)} samples\")\n",
    "        \n",
    "        # Plot actual vs predicted\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Scatter plot\n",
    "        axes[0].scatter(oof_df['actual'], oof_df['pred'], alpha=0.5, s=10)\n",
    "        axes[0].plot([oof_df['actual'].min(), oof_df['actual'].max()], \n",
    "                     [oof_df['actual'].min(), oof_df['actual'].max()], 'r--', lw=2)\n",
    "        axes[0].set_xlabel('Actual')\n",
    "        axes[0].set_ylabel('Predicted')\n",
    "        axes[0].set_title('Actual vs Predicted')\n",
    "        \n",
    "        # Residuals distribution\n",
    "        residuals = oof_df['actual'] - oof_df['pred']\n",
    "        axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "        axes[1].set_xlabel('Residual (Actual - Predicted)')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].set_title('Residuals Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    # List saved model files\n",
    "    models = list(latest_run.glob(\"model_fold_*.bin\"))\n",
    "    print(f\"\\nüíæ Saved Models: {len(models)} fold(s)\")\n",
    "    for m in models:\n",
    "        print(f\"  - {m.name}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No runs found. Please run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b8c8e",
   "metadata": {},
   "source": [
    "## 8. Inference & Submission\n",
    "\n",
    "Generate predictions on test data and create submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09beced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def generate_submission(run_id: str, test_df: pd.DataFrame, id_col: str = 'id'):\n",
    "    \"\"\"Generate predictions using trained models and create submission file.\"\"\"\n",
    "    \n",
    "    artifacts_dir = Path(run_config['paths']['artifacts_dir']) / run_id\n",
    "    submissions_dir = Path(run_config['paths']['submissions_dir'])\n",
    "    submissions_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load all fold models\n",
    "    model_paths = sorted(artifacts_dir.glob(\"model_fold_*.bin\"))\n",
    "    print(f\"üìÇ Loading {len(model_paths)} model(s) from {run_id}\")\n",
    "    \n",
    "    all_preds = []\n",
    "    for model_path in model_paths:\n",
    "        model = joblib.load(model_path)\n",
    "        \n",
    "        # Get feature columns (exclude id and target)\n",
    "        target_col = data_config['columns']['target']\n",
    "        feature_cols = [c for c in test_df.columns if c not in [id_col, target_col]]\n",
    "        \n",
    "        preds = model.predict(test_df[feature_cols])\n",
    "        all_preds.append(preds)\n",
    "        print(f\"  ‚úÖ Loaded {model_path.name}\")\n",
    "    \n",
    "    # Average predictions across folds\n",
    "    final_preds = np.mean(all_preds, axis=0)\n",
    "    \n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({\n",
    "        id_col: test_df[id_col] if id_col in test_df.columns else range(len(final_preds)),\n",
    "        'prediction': final_preds\n",
    "    })\n",
    "    \n",
    "    # Save submission\n",
    "    submission_path = submissions_dir / f\"submission_{run_id}.csv\"\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    print(f\"\\n‚úÖ Submission saved to: {submission_path}\")\n",
    "    print(f\"üìä Shape: {submission.shape}\")\n",
    "    print(f\"\\nüìã Preview:\")\n",
    "    display(submission.head(10))\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# Example usage (uncomment and modify when you have test data):\n",
    "# test_df = pd.read_csv(data_mgr.raw_dir / \"test.csv\")\n",
    "# test_df = fe.build_features(test_df)  # Apply same feature engineering\n",
    "# submission = generate_submission(run_id, test_df)\n",
    "\n",
    "print(\"üí° To generate submission, uncomment the code above and ensure:\")\n",
    "print(\"  1. Test data is available in the raw directory\")\n",
    "print(\"  2. Run ID is defined from a successful training run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb662d",
   "metadata": {},
   "source": [
    "## 9. Experiment Tracking & History\n",
    "\n",
    "View all experiment runs and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb04ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all experiment runs\n",
    "artifacts_base = Path(run_config['paths']['artifacts_dir'])\n",
    "\n",
    "if artifacts_base.exists():\n",
    "    runs = sorted(artifacts_base.glob(\"*\"), key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    \n",
    "    if runs:\n",
    "        print(f\"üìä Experiment History ({len(runs)} runs):\\n\")\n",
    "        \n",
    "        run_data = []\n",
    "        for run_dir in runs[:20]:  # Show last 20 runs\n",
    "            run_info = {'run_id': run_dir.name}\n",
    "            \n",
    "            # Load metrics if available\n",
    "            metrics_path = run_dir / \"metrics.json\"\n",
    "            if metrics_path.exists():\n",
    "                with open(metrics_path) as f:\n",
    "                    metrics = json.load(f)\n",
    "                run_info.update(metrics)\n",
    "            \n",
    "            # Get timestamp from folder\n",
    "            run_info['created'] = datetime.fromtimestamp(run_dir.stat().st_mtime).strftime('%Y-%m-%d %H:%M')\n",
    "            \n",
    "            run_data.append(run_info)\n",
    "        \n",
    "        runs_df = pd.DataFrame(run_data)\n",
    "        display(runs_df)\n",
    "        \n",
    "        # Plot metrics comparison\n",
    "        if 'rmse' in runs_df.columns and len(runs_df) > 1:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.bar(range(len(runs_df)), runs_df['rmse'])\n",
    "            plt.xticks(range(len(runs_df)), runs_df['run_id'], rotation=45, ha='right')\n",
    "            plt.xlabel('Run ID')\n",
    "            plt.ylabel('RMSE')\n",
    "            plt.title('RMSE Comparison Across Runs')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No experiment runs found.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Artifacts directory not found. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6308c791",
   "metadata": {},
   "source": [
    "## 10. Utilities & Helpers\n",
    "\n",
    "Useful utility functions for common operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfcc4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for common operations\n",
    "\n",
    "def sync_to_drive():\n",
    "    \"\"\"Sync local changes to Google Drive (Colab only).\"\"\"\n",
    "    if IN_COLAB:\n",
    "        from google.colab import drive\n",
    "        drive.flush_and_unmount()\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"‚úÖ Synced to Google Drive\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Not in Colab, skipping Drive sync\")\n",
    "\n",
    "def download_submission(run_id: str):\n",
    "    \"\"\"Download submission file (Colab only).\"\"\"\n",
    "    if IN_COLAB:\n",
    "        from google.colab import files\n",
    "        submission_path = Path(run_config['paths']['submissions_dir']) / f\"submission_{run_id}.csv\"\n",
    "        if submission_path.exists():\n",
    "            files.download(str(submission_path))\n",
    "            print(f\"‚úÖ Downloaded: {submission_path.name}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Submission not found: {submission_path}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Not in Colab, file is available locally\")\n",
    "\n",
    "def upload_data():\n",
    "    \"\"\"Upload data files to raw directory (Colab only).\"\"\"\n",
    "    if IN_COLAB:\n",
    "        from google.colab import files\n",
    "        print(\"üì§ Select files to upload...\")\n",
    "        uploaded = files.upload()\n",
    "        for filename, content in uploaded.items():\n",
    "            dest_path = data_mgr.raw_dir / filename\n",
    "            with open(dest_path, 'wb') as f:\n",
    "                f.write(content)\n",
    "            print(f\"  ‚úÖ Saved: {dest_path}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Not in Colab. Place files directly in data/raw/\")\n",
    "\n",
    "def show_gpu_info():\n",
    "    \"\"\"Display GPU information.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"üíæ Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        print(f\"üíæ Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "        print(f\"üíæ Cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"‚ùå No GPU available\")\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    \"\"\"Clear GPU memory cache.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"‚úÖ GPU cache cleared\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GPU to clear\")\n",
    "\n",
    "print(\"üõ†Ô∏è Utility functions available:\")\n",
    "print(\"  - sync_to_drive(): Sync changes to Google Drive\")\n",
    "print(\"  - download_submission(run_id): Download submission CSV\")\n",
    "print(\"  - upload_data(): Upload data files to raw directory\")\n",
    "print(\"  - show_gpu_info(): Display GPU information\")\n",
    "print(\"  - clear_gpu_cache(): Clear GPU memory cache\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
