# XGBoost model configuration for Novartis Datathon 2025

model:
  name: "xgboost"
  task: "regression"  # regression, binary, multiclass

# XGBoost hyperparameters
params:
  # Core parameters
  booster: "gbtree"           # gbtree, gblinear, dart
  objective: "reg:squarederror"  # reg:squarederror, binary:logistic, multi:softprob
  eval_metric: "rmse"
  
  # Tree parameters
  max_depth: 6
  min_child_weight: 1
  max_leaves: 0               # 0 means no limit
  
  # Learning parameters
  learning_rate: 0.05
  n_estimators: 1000
  
  # Regularization
  gamma: 0                    # Minimum loss reduction for split
  reg_alpha: 0                # L1 regularization
  reg_lambda: 1               # L2 regularization
  subsample: 0.8
  colsample_bytree: 0.8
  colsample_bylevel: 1.0
  colsample_bynode: 1.0
  
  # Early stopping
  early_stopping_rounds: 50
  
  # Other
  verbosity: 0
  n_jobs: -1
  seed: 42
  
  # GPU (uncomment for GPU training)
  # tree_method: "gpu_hist"
  # gpu_id: 0

# Training settings
training:
  use_early_stopping: true
  eval_metric: "rmse"
  verbose_eval: 100

# Hyperparameter tuning (Optuna)
tuning:
  enabled: false
  n_trials: 100
  timeout: 3600
  search_space:
    max_depth: [3, 12]
    learning_rate: [0.01, 0.3]
    subsample: [0.5, 1.0]
    colsample_bytree: [0.5, 1.0]
    reg_alpha: [0.0, 10.0]
    reg_lambda: [0.0, 10.0]
    min_child_weight: [1, 10]
