# ğŸ¥ Novartis Datathon 2025 - Generic Erosion Forecasting

> **Predict pharmaceutical brand sales erosion after generic drug entry**

[![Python 3.10+](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Problem Statement](#-problem-statement)
- [Project Structure](#-project-structure)
- [Quick Start](#-quick-start)
- [Data Description](#-data-description)
- [Pipeline Workflow](#-pipeline-workflow)
- [Models](#-models)
- [Evaluation Metric](#-evaluation-metric)
- [Results](#-results)
- [Usage Examples](#-usage-examples)

---

## ğŸ¯ Overview

When generic drugs enter the market, branded drug sales typically declineâ€”this is called **generic erosion**. This project predicts the **24-month post-generic sales trajectory** for pharmaceutical brands.

### Two Scenarios:

| Scenario | Task | Available Data |
|----------|------|----------------|
| **Scenario 1** | Predict months 0-23 | Only pre-entry data (months -24 to -1) |
| **Scenario 2** | Predict months 6-23 | Pre-entry + actual months 0-5 |

---

## ğŸ”¬ Problem Statement

**Goal:** Predict monthly `volume` (sales units) for 24 months after generic entry.

**Key Challenge:** Bucket 1 brands (high erosion, mean normalized volume â‰¤ 0.25) are weighted **2Ã—** in the evaluation metric.

```
Normalized Volume = Actual Volume / Pre-Entry Average (Avg_j)

Bucket 1: mean(normalized volume) â‰¤ 0.25  â†’  High erosion, 2Ã— weight
Bucket 2: mean(normalized volume) > 0.25   â†’  Lower erosion, 1Ã— weight
```

---

## ğŸ“ Project Structure

```
Main_project/
â”‚
â”œâ”€â”€ src/                          # ğŸ”´ CORE LOGIC (Python modules)
â”‚   â”œâ”€â”€ config.py                 # Paths, constants, model parameters
â”‚   â”œâ”€â”€ data_loader.py            # Load and validate data
â”‚   â”œâ”€â”€ bucket_calculator.py      # Compute Avg_j, buckets, normalization
â”‚   â”œâ”€â”€ feature_engineering.py    # Create 40+ features
â”‚   â”œâ”€â”€ models.py                 # Baseline + ML models
â”‚   â”œâ”€â”€ evaluation.py             # PE metric computation
â”‚   â”œâ”€â”€ submission.py             # Generate submission files
â”‚   â”œâ”€â”€ pipeline.py               # End-to-end CLI pipeline
â”‚   â””â”€â”€ eda_analysis.py           # EDA computations
â”‚
â”œâ”€â”€ scripts/                      # ğŸŸ¡ EXECUTION SCRIPTS
â”‚   â”œâ”€â”€ run_demo.py               # Quick demo (test everything)
â”‚   â”œâ”€â”€ train_models.py           # Train and compare all models
â”‚   â”œâ”€â”€ generate_final_submissions.py  # Create competition submissions
â”‚   â””â”€â”€ validate_submissions.py   # Validate before upload
â”‚
â”œâ”€â”€ notebooks/                    # ğŸŸ¢ VISUALIZATION (Jupyter)
â”‚   â”œâ”€â”€ 01_eda_visualization.ipynb
â”‚   â”œâ”€â”€ 02_feature_exploration.ipynb
â”‚   â””â”€â”€ 03_model_results.ipynb
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                      # Input CSV files
â”‚   â””â”€â”€ processed/                # Generated intermediate files
â”‚
â”œâ”€â”€ models/                       # Saved trained models (.joblib)
â”œâ”€â”€ submissions/                  # Generated submission files
â”œâ”€â”€ reports/                      # Model comparison results
â”‚   â””â”€â”€ figures/                  # Saved plots
â”‚
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # This file
```

---

## ğŸš€ Quick Start

### 1. Setup Environment

```powershell
# Navigate to project
cd D:\Datathon\novartis_datathon_2025\Main_project

# Create virtual environment (if not exists)
python -m venv saeed_venv

# Activate environment
.\saeed_venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt
```

### 2. Run Quick Demo

```powershell
python scripts/run_demo.py
```

This will:
- Load data
- Create features
- Train baseline model
- Evaluate predictions
- Generate sample submission

### 3. Train All Models

```powershell
# Train for Scenario 1
python scripts/train_models.py --scenario 1

# Train for Scenario 2
python scripts/train_models.py --scenario 2
```

### 4. Generate Final Submissions

```powershell
python scripts/generate_final_submissions.py --model baseline
```

### 5. Validate Submissions

```powershell
python scripts/validate_submissions.py
```

---

## ğŸ“Š Data Description

### Input Files (in `data/raw/`)

| File | Description | Key Columns |
|------|-------------|-------------|
| `df_volume_train.csv` | Monthly sales volume | `country`, `brand_name`, `months_postgx`, `volume` |
| `df_generics_train.csv` | Generic competitor info | `country`, `brand_name`, `months_postgx`, `num_generics` |
| `df_medicine_info_train.csv` | Brand metadata | `country`, `brand_name`, `ther_area`, `hospital_rate` |

### Time Reference

```
months_postgx:
  -24 to -1  â†’  Pre-generic entry (historical)
    0        â†’  Generic entry month
    1 to 23  â†’  Post-generic entry (forecast period)
```

---

## ğŸ”„ Pipeline Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Load Data     â”‚  data_loader.py
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Compute Avg_j   â”‚  bucket_calculator.py
â”‚ Assign Buckets  â”‚  (Pre-entry average volume)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Engineer      â”‚  feature_engineering.py
â”‚   Features      â”‚  (40+ features)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Train Models   â”‚  models.py
â”‚  - Baseline     â”‚
â”‚  - LightGBM     â”‚
â”‚  - XGBoost      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Evaluate      â”‚  evaluation.py
â”‚   (PE Metric)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Generate      â”‚  submission.py
â”‚   Submission    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤– Models

### 1. Baseline Models

| Model | Formula | Best For |
|-------|---------|----------|
| **No Erosion** | `volume = avg_j` | Upper bound baseline |
| **Linear Decay** | `volume = avg_j Ã— (1 - Î» Ã— month)` | Simple decay |
| **Exponential Decay** | `volume = avg_j Ã— exp(-Î» Ã— month)` | âœ… **Best performer** |

### 2. Machine Learning Models

| Model | Type | Features Used |
|-------|------|---------------|
| **LightGBM** | Gradient Boosting | 40+ engineered features |
| **XGBoost** | Gradient Boosting | 40+ engineered features |

### Feature Categories

- **Lag Features:** `volume_lag_1`, `volume_lag_3`, `volume_lag_6`, `volume_lag_12`
- **Rolling Features:** `rolling_mean_3`, `rolling_std_3`, `rolling_mean_6`, etc.
- **Competition:** `num_generics`, `months_with_generics`, `generics_growth_rate`
- **Time Features:** `months_postgx`, `month_sin`, `month_cos`
- **Pre-entry:** `pre_entry_slope`, `pre_entry_volatility`, `avg_vol`

---

## ğŸ“ Evaluation Metric

The competition uses a custom **Prediction Error (PE)** metric:

```
PE_brand = wâ‚Ã—|monthly_errors| + wâ‚‚Ã—|sum_0_5| + wâ‚ƒÃ—|sum_6_11| + wâ‚„Ã—|sum_12_23|
```

### Scenario 1 Weights:
| Component | Weight |
|-----------|--------|
| Monthly errors | 20% |
| Sum months 0-5 | **50%** â† Focus here! |
| Sum months 6-11 | 20% |
| Sum months 12-23 | 10% |

### Scenario 2 Weights:
| Component | Weight |
|-----------|--------|
| Monthly errors | 20% |
| Sum months 6-11 | **50%** â† Focus here! |
| Sum months 12-23 | 30% |

### Final Score:
```
Final PE = (2 Ã— avg_PE_bucket1 + 1 Ã— avg_PE_bucket2) / (2 Ã— n_bucket1 + 1 Ã— n_bucket2)
```

**Lower is better!**

---

## ğŸ“ˆ Results

### Model Comparison

| Model | Scenario 1 PE | Scenario 2 PE |
|-------|---------------|---------------|
| No Erosion Baseline | 1.84 | 2.18 |
| **Exponential Decay (Î»=0.05)** | **1.18** âœ… | **1.10** âœ… |
| XGBoost | 2.84 | 3.39 |
| LightGBM | 14.93 | 14.96 |

**Best Model:** Exponential Decay baseline with Î»=0.05

### Final Submissions

| File | Rows | Brands |
|------|------|--------|
| `scenario1_baseline_final.csv` | 8,160 | 340 |
| `scenario2_baseline_final.csv` | 6,120 | 340 |

---

## ğŸ’» Usage Examples

### Using the Pipeline Module

```python
from src.pipeline import run_pipeline

# Run full pipeline for Scenario 1
run_pipeline(scenario=1, model_type='lightgbm', generate_test_submission=True)
```

### Loading and Merging Data

```python
from src.data_loader import load_all_data, merge_datasets

# Load training data
volume, generics, medicine = load_all_data(train=True)
merged = merge_datasets(volume, generics, medicine)
print(merged.shape)  # (93744, 11)
```

### Computing Buckets

```python
from src.bucket_calculator import create_auxiliary_file

aux_df = create_auxiliary_file(merged, save=True)
print(aux_df['bucket'].value_counts())
# Bucket 1: 130 brands (high erosion)
# Bucket 2: 1823 brands (lower erosion)
```

### Creating Features

```python
from src.feature_engineering import create_all_features, get_feature_columns

featured = create_all_features(merged, avg_j)
feature_cols = get_feature_columns(featured)
print(f"Total features: {len(feature_cols)}")  # ~40 features
```

### Training a Model

```python
from src.models import GradientBoostingModel

model = GradientBoostingModel(model_type='lightgbm')
model.fit(X_train, y_train, X_val, y_val)
predictions = model.predict(X_test)
model.save("scenario1_lightgbm")
```

### Evaluating Predictions

```python
from src.evaluation import evaluate_model

results = evaluate_model(actual_df, pred_df, aux_df, scenario=1)
print(f"Final Score: {results['final_score']:.4f}")
```

---

## ğŸ”§ Configuration

Key settings in `src/config.py`:

```python
# Bucket threshold
BUCKET_1_THRESHOLD = 0.25  # Mean erosion â‰¤ 0.25 = Bucket 1

# Metric weights - Scenario 1
S1_SUM_0_5_WEIGHT = 0.5    # 50% weight on early months

# Model parameters
LGBM_PARAMS = {
    'n_estimators': 500,
    'learning_rate': 0.05,
    'max_depth': 6,
    ...
}
```

---

## ğŸ““ Notebooks

Open in Jupyter or VS Code:

| Notebook | Purpose |
|----------|---------|
| `01_eda_visualization.ipynb` | Data quality, distributions, erosion curves |
| `02_feature_exploration.ipynb` | Feature correlations and importance |
| `03_model_results.ipynb` | Model comparison and submission analysis |

```powershell
jupyter notebook notebooks/
```

---

## ğŸ¯ Key Insights

1. **Exponential decay baseline outperforms ML models** - The decay pattern is well-captured by a simple formula
2. **Bucket 1 is critical** - Only ~7% of brands but 2Ã— weight in metric
3. **Early months matter most** - 50% weight on months 0-5 (S1) or 6-11 (S2)
4. **ML models need tuning** - Current implementation predicts raw volume; should predict normalized volume

---

## ğŸ“š References

- [Novartis Datathon 2025 Guidelines](Docs/novartis_datathon_2025_guide.md)
- [Question Set](Docs/question-set.md)
- [Metric Calculation](src/metric_calculation.py)

---

## ğŸ‘¥ Team

**Branch:** Saeed

---

## ğŸ“„ License

This project is for the Novartis Datathon 2025 competition.

---

<div align="center">

**ğŸ† Good luck with the competition! ğŸ†**

</div>
