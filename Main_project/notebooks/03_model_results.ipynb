{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0141c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Add src to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path for imports\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f348ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from src modules\n",
    "from config import *\n",
    "\n",
    "print(\"‚úÖ Config imported!\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Reports directory: {REPORTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd178ed",
   "metadata": {},
   "source": [
    "## 1. Load Model Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load comparison results\n",
    "try:\n",
    "    comparison_s1 = pd.read_csv(REPORTS_DIR / 'model_comparison_scenario1.csv')\n",
    "    print(\"‚úÖ Loaded Scenario 1 results\")\n",
    "    display(comparison_s1)\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Scenario 1 results not found. Run scripts/train_models.py --scenario 1 first.\")\n",
    "    comparison_s1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a118c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    comparison_s2 = pd.read_csv(REPORTS_DIR / 'model_comparison_scenario2.csv')\n",
    "    print(\"‚úÖ Loaded Scenario 2 results\")\n",
    "    display(comparison_s2)\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Scenario 2 results not found. Run scripts/train_models.py --scenario 2 first.\")\n",
    "    comparison_s2 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953d034",
   "metadata": {},
   "source": [
    "## 2. Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb2c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Scenario 1\n",
    "if comparison_s1 is not None:\n",
    "    colors = ['#2ecc71' if i == comparison_s1['final_score'].idxmin() else '#3498db' \n",
    "              for i in range(len(comparison_s1))]\n",
    "    bars1 = axes[0].barh(comparison_s1['model'], comparison_s1['final_score'], \n",
    "                         color=colors, edgecolor='black')\n",
    "    axes[0].set_xlabel('Final Score (PE, lower is better)', fontsize=11)\n",
    "    axes[0].set_title('Scenario 1: Model Comparison\\n(Predict months 0-23)', fontsize=12)\n",
    "    axes[0].invert_yaxis()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars1, comparison_s1['final_score']):\n",
    "        axes[0].text(val + 0.02, bar.get_y() + bar.get_height()/2, \n",
    "                     f'{val:.4f}', va='center', fontsize=10)\n",
    "    \n",
    "    # Mark best model\n",
    "    best_idx = comparison_s1['final_score'].idxmin()\n",
    "    axes[0].annotate('üèÜ BEST', xy=(comparison_s1.loc[best_idx, 'final_score'], best_idx),\n",
    "                     xytext=(10, 0), textcoords='offset points', fontsize=10, color='green')\n",
    "\n",
    "# Scenario 2\n",
    "if comparison_s2 is not None:\n",
    "    colors = ['#2ecc71' if i == comparison_s2['final_score'].idxmin() else '#e74c3c' \n",
    "              for i in range(len(comparison_s2))]\n",
    "    bars2 = axes[1].barh(comparison_s2['model'], comparison_s2['final_score'], \n",
    "                         color=colors, edgecolor='black')\n",
    "    axes[1].set_xlabel('Final Score (PE, lower is better)', fontsize=11)\n",
    "    axes[1].set_title('Scenario 2: Model Comparison\\n(Predict months 6-23, given 0-5)', fontsize=12)\n",
    "    axes[1].invert_yaxis()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars2, comparison_s2['final_score']):\n",
    "        axes[1].text(val + 0.02, bar.get_y() + bar.get_height()/2, \n",
    "                     f'{val:.4f}', va='center', fontsize=10)\n",
    "    \n",
    "    # Mark best model\n",
    "    best_idx = comparison_s2['final_score'].idxmin()\n",
    "    axes[1].annotate('üèÜ BEST', xy=(comparison_s2.loc[best_idx, 'final_score'], best_idx),\n",
    "                     xytext=(10, 0), textcoords='offset points', fontsize=10, color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"‚úÖ Saved to {FIGURES_DIR / 'model_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e972d46",
   "metadata": {},
   "source": [
    "## 3. Bucket-Level Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a8d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket-level analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Scenario 1 bucket breakdown\n",
    "if comparison_s1 is not None and 'bucket1_pe' in comparison_s1.columns:\n",
    "    x = np.arange(len(comparison_s1))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = axes[0].bar(x - width/2, comparison_s1['bucket1_pe'].fillna(0), width, \n",
    "                        label='Bucket 1 (2√ó weight)', color='#ff6b6b', edgecolor='black')\n",
    "    bars2 = axes[0].bar(x + width/2, comparison_s1['bucket2_pe'].fillna(0), width,\n",
    "                        label='Bucket 2', color='#4ecdc4', edgecolor='black')\n",
    "    \n",
    "    axes[0].set_xlabel('Model')\n",
    "    axes[0].set_ylabel('Prediction Error (PE)')\n",
    "    axes[0].set_title('Scenario 1: Bucket-Level Performance', fontsize=12)\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(comparison_s1['model'], rotation=45, ha='right')\n",
    "    axes[0].legend()\n",
    "\n",
    "# Scenario 2 bucket breakdown  \n",
    "if comparison_s2 is not None and 'bucket1_pe' in comparison_s2.columns:\n",
    "    x = np.arange(len(comparison_s2))\n",
    "    \n",
    "    bars1 = axes[1].bar(x - width/2, comparison_s2['bucket1_pe'].fillna(0), width,\n",
    "                        label='Bucket 1 (2√ó weight)', color='#ff6b6b', edgecolor='black')\n",
    "    bars2 = axes[1].bar(x + width/2, comparison_s2['bucket2_pe'].fillna(0), width,\n",
    "                        label='Bucket 2', color='#4ecdc4', edgecolor='black')\n",
    "    \n",
    "    axes[1].set_xlabel('Model')\n",
    "    axes[1].set_ylabel('Prediction Error (PE)')\n",
    "    axes[1].set_title('Scenario 2: Bucket-Level Performance', fontsize=12)\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(comparison_s2['model'], rotation=45, ha='right')\n",
    "    axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'bucket_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f5dea3",
   "metadata": {},
   "source": [
    "## 4. Feature Importance (LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5711c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LightGBM model and get feature importance\n",
    "import joblib\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "for idx, scenario in enumerate([1, 2]):\n",
    "    model_path = MODELS_DIR / f'scenario{scenario}_lightgbm.joblib'\n",
    "    \n",
    "    if model_path.exists():\n",
    "        data = joblib.load(model_path)\n",
    "        model = data['model']\n",
    "        feature_names = data['feature_names']\n",
    "        \n",
    "        # Get feature importance\n",
    "        importance = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=True).tail(15)\n",
    "        \n",
    "        axes[idx].barh(importance['feature'], importance['importance'], \n",
    "                       color='steelblue', edgecolor='black')\n",
    "        axes[idx].set_xlabel('Importance')\n",
    "        axes[idx].set_title(f'Scenario {scenario}: Top 15 Features (LightGBM)', fontsize=12)\n",
    "    else:\n",
    "        axes[idx].text(0.5, 0.5, f'Model not found\\nRun train_models.py first',\n",
    "                       ha='center', va='center', transform=axes[idx].transAxes)\n",
    "        axes[idx].set_title(f'Scenario {scenario}: Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8511b23a",
   "metadata": {},
   "source": [
    "## 5. Submission Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2230bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze submissions\n",
    "submission_files = list(SUBMISSIONS_DIR.glob('scenario*_final.csv'))\n",
    "\n",
    "print(f\"üìÅ Found {len(submission_files)} submission files:\")\n",
    "for f in submission_files:\n",
    "    print(f\"   - {f.name}\")\n",
    "\n",
    "submissions = {}\n",
    "for f in submission_files:\n",
    "    df = pd.read_csv(f)\n",
    "    scenario = 1 if 'scenario1' in f.name else 2\n",
    "    submissions[scenario] = df\n",
    "    print(f\"\\nüìä {f.name}:\")\n",
    "    print(f\"   Rows: {len(df):,}\")\n",
    "    print(f\"   Brands: {df[['country', 'brand_name']].drop_duplicates().shape[0]}\")\n",
    "    print(f\"   Volume range: [{df['volume'].min():.2f}, {df['volume'].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8efb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize submission predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (scenario, df) in enumerate(submissions.items()):\n",
    "    # Average predicted volume by month\n",
    "    avg_by_month = df.groupby('months_postgx')['volume'].mean()\n",
    "    \n",
    "    axes[idx].plot(avg_by_month.index, avg_by_month.values, marker='o', \n",
    "                   linewidth=2, color='steelblue')\n",
    "    axes[idx].fill_between(avg_by_month.index, 0, avg_by_month.values, alpha=0.3)\n",
    "    axes[idx].set_xlabel('Months Post Generic Entry')\n",
    "    axes[idx].set_ylabel('Average Predicted Volume')\n",
    "    axes[idx].set_title(f'Scenario {scenario}: Predicted Erosion Curve', fontsize=12)\n",
    "    \n",
    "    # Add annotations\n",
    "    if scenario == 1:\n",
    "        axes[idx].axvspan(0, 5, alpha=0.2, color='red', label='High weight (50%)')\n",
    "        axes[idx].axvspan(6, 11, alpha=0.1, color='orange', label='Medium weight (20%)')\n",
    "    else:\n",
    "        axes[idx].axvspan(6, 11, alpha=0.2, color='red', label='High weight (50%)')\n",
    "        axes[idx].axvspan(12, 23, alpha=0.1, color='orange', label='Medium weight (30%)')\n",
    "    axes[idx].legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'submission_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97946040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (scenario, df) in enumerate(submissions.items()):\n",
    "    # Log scale for better visualization\n",
    "    log_volume = np.log1p(df['volume'])\n",
    "    \n",
    "    axes[idx].hist(log_volume, bins=50, alpha=0.7, color='coral', edgecolor='black')\n",
    "    axes[idx].axvline(x=log_volume.median(), color='red', linestyle='--', \n",
    "                      label=f'Median: {np.expm1(log_volume.median()):,.0f}')\n",
    "    axes[idx].set_xlabel('Log(Volume + 1)')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].set_title(f'Scenario {scenario}: Predicted Volume Distribution', fontsize=12)\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'prediction_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd6c58b",
   "metadata": {},
   "source": [
    "## 6. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c6cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"üìä MODEL RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if comparison_s1 is not None:\n",
    "    best_s1 = comparison_s1.loc[comparison_s1['final_score'].idxmin()]\n",
    "    print(f\"\\nüèÜ SCENARIO 1 BEST MODEL:\")\n",
    "    print(f\"   Model: {best_s1['model']}\")\n",
    "    print(f\"   Final Score: {best_s1['final_score']:.4f}\")\n",
    "    if 'bucket1_pe' in best_s1:\n",
    "        print(f\"   Bucket 1 PE: {best_s1['bucket1_pe']:.4f}\")\n",
    "        print(f\"   Bucket 2 PE: {best_s1['bucket2_pe']:.4f}\")\n",
    "\n",
    "if comparison_s2 is not None:\n",
    "    best_s2 = comparison_s2.loc[comparison_s2['final_score'].idxmin()]\n",
    "    print(f\"\\nüèÜ SCENARIO 2 BEST MODEL:\")\n",
    "    print(f\"   Model: {best_s2['model']}\")\n",
    "    print(f\"   Final Score: {best_s2['final_score']:.4f}\")\n",
    "    if 'bucket1_pe' in best_s2:\n",
    "        print(f\"   Bucket 1 PE: {best_s2['bucket1_pe']:.4f}\")\n",
    "        print(f\"   Bucket 2 PE: {best_s2['bucket2_pe']:.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ SUBMISSIONS:\")\n",
    "for scenario, df in submissions.items():\n",
    "    print(f\"   Scenario {scenario}: {len(df):,} predictions\")\n",
    "\n",
    "print(f\"\\n‚úÖ All figures saved to: {FIGURES_DIR}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüì§ Ready for competition submission!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
