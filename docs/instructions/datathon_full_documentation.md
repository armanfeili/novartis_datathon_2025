**Exact content of the page**

---

**The Generics Problem**

In the pharmaceutical industry, when a company develops a new drug, it is granted a patent that provides exclusive rights to produce and commercialize the product for a specific period. Once the patent expires, the drug reaches its *Loss of Exclusivity (LOE)* date. From that moment onward, other manufacturers are allowed to produce generic versions of the drug.

A generic drug is defined as a product that is equivalent to the brand-name medication in terms of dosage form, strength, route of administration, quality, performance, and intended use. Although generic products may contain different inactive ingredients (e.g., fillers or binders), these differences do not affect the therapeutic effect of the medication.

Generic manufacturers benefit from significantly lower development costs, as they are not required to repeat extensive clinical research. Instead, they must demonstrate *bioequivalence* to the original product by comparing pharmacokinetic properties such as absorption, distribution, metabolism, and elimination.

The entry of generics into the market typically leads to several consequences:

1. **Increased competition:** Multiple manufacturers producing the same medication increases market competition, which generally drives prices downward.

2. **Improved affordability:** Generic versions are usually sold at lower prices, making treatments more accessible for patients and healthcare systems.

3. **Greater access to medication:** Reduced prices expand access to treatment for a larger population, which may improve disease management outcomes.

4. **Prescribing and substitution practices:** Healthcare professionals can prescribe generic alternatives, and in some cases, pharmacies may substitute the brand-name drug with a generic equivalent when permitted.

A well-known example within Novartis is *Diovan*, whose active ingredient is valsartan and is used for the treatment of hypertension and heart failure. When Diovan’s patent expired in 2012, multiple manufacturers entered the market with generic versions of valsartan, significantly increasing competition and reducing prices.

The Novartis Digital Finance Hub oversees the application of advanced analytics to financial processes. Among its responsibilities is the forecasting of future sales, enabling strategic planning and assessment of the financial impact of events such as generic entries. These forecasts support country organizations in generating monthly and annual sales reports, which are later consolidated and used in company-wide financial accounting.

2

---

### Detailed explanation of the page

There are no plots or formulas on this page—only text. I will go through each part in order.

#### Title: “The Generics Problem”

The title signals that the focus is on the *impact* of generic drugs on the pharmaceutical market and on Novartis in particular. The “problem” is mainly from the point of view of an originator company whose exclusive rights eventually end, changing revenues and market dynamics.

---

#### Paragraph 1: Patents and Loss of Exclusivity (LOE)

* When a company invents a new drug, it receives a **patent**.

  * A patent gives the company **exclusive rights** for a limited time (typically 20 years from filing, though effective market exclusivity may be shorter).
  * During this period, no one else can legally manufacture or sell the same drug (same active ingredient and indication) without permission.

* When this period ends, the drug reaches its **Loss of Exclusivity (LOE)** date.

  * *Loss of Exclusivity (LOE)* means the legal protection that kept competitors away is gone.
  * After LOE, **generic manufacturers** are legally allowed to enter the market with copies of the drug.

So this paragraph sets up the timeline:
innovation → patent/exclusivity → LOE → open competition.

---

#### Paragraph 2: What is a generic drug?

This paragraph defines **generic drug** precisely:

* It must be **equivalent to the brand-name medication** (also called the “originator” or “reference product”).

* Equivalence is in terms of:

  * **Dosage form** – e.g., tablet, capsule, injectable.
  * **Strength** – amount of active ingredient per dose (e.g., 80 mg).
  * **Route of administration** – oral, intravenous, etc.
  * **Quality** – must meet regulatory standards for purity, stability, etc.
  * **Performance** – must behave similarly in the body (mainly captured by bioequivalence, discussed later).
  * **Intended use** – same indications and patient population.

* The text emphasizes that **inactive ingredients can differ**:

  * Examples: fillers, binders, colorants, coatings.
  * These differences should **not change the therapeutic effect**; regulators require that the active substance behaves the same in the body despite these excipients.

So generics are *therapeutically equivalent*, even if pill appearance or some excipients differ.

---

#### Paragraph 3: Development costs and bioequivalence

This paragraph explains why generics are cheaper to develop:

* **Generic manufacturers do not repeat all the original clinical trials.**

  * The original (brand-name) company has already done extensive pre-clinical and clinical research to prove safety and efficacy.
  * Regulators allow generics to “piggyback” on that evidence.

* Instead, the crucial requirement is to demonstrate **bioequivalence**:

  * *Bioequivalence* means that the generic and the originator show **no significant difference** in how the drug is available in the body when given in the same dose.
  * It is assessed through **pharmacokinetic properties**:

    * **Absorption** – how fast and how much of the drug enters the bloodstream.
    * **Distribution** – how the drug spreads through tissues.
    * **Metabolism** – how the body transforms (breaks down) the drug.
    * **Elimination** – how the drug and its metabolites are excreted (e.g., via kidneys or liver).

* A typical bioequivalence study:

  * Healthy volunteers receive both the brand-name product and the generic product in a crossover study.
  * Blood samples are taken over time, and concentration–time curves are analyzed.
  * Key metrics (AUC, Cmax) are compared; if they fall within an accepted range (often 80–125% for certain ratios), the products are considered bioequivalent.

Because generics only need these smaller, focused studies instead of full clinical development, **development costs are much lower**, which later translates into lower prices.

---

#### Paragraph 4: Consequences of generic entry

The text says: “The entry of generics into the market typically leads to several consequences:” and then lists four.

1. **Increased competition**

   * Once patents expire, multiple companies can produce the same active ingredient.
   * More manufacturers → **more competition** on price and market share.
   * This competition generally **drives prices down** because:

     * Each company wants to offer a lower or at least competitive price.
     * Pharmacies, insurers, and healthcare systems often encourage the cheaper alternatives.

2. **Improved affordability**

   * Because generics have **lower development costs** and face **competitive pressure**, they are usually **sold at lower prices** than the original brand.
   * Lower drug prices make medications:

     * More affordable for individual patients (lower out-of-pocket cost).
     * Less expensive for healthcare systems and insurers.
   * This is critical in chronic diseases, where treatment is long-term and drug spending is high.

3. **Greater access to medication**

   * When prices decrease, **more people can access treatment**, especially in low- and middle-income settings or among underinsured populations.
   * This can:

     * Improve **adherence** (patients are more likely to continue treatment).
     * Improve **disease management outcomes**, because adequate and continuous therapy becomes feasible for larger groups of patients.

4. **Prescribing and substitution practices**

   * **Prescribers (doctors)** can choose to prescribe generics directly.
   * In some healthcare systems, **pharmacists** are allowed (or even required) to substitute a brand-name prescription with a cheaper generic version, as long as:

     * A generic equivalent exists.
     * Substitution is permitted by national regulations.
     * The prescriber has not explicitly forbidden substitution.
   * This mechanism accelerates the shift from branded products to generics after LOE.

Together, these four points highlight why generics are generally *good* for health systems, patients, and payers: more competition, lower prices, and broader access.

---

#### Paragraph 5: Example – Diovan (valsartan)

This paragraph gives a concrete example inside Novartis:

* **Diovan** is a branded drug by Novartis.

  * Its **active ingredient** is **valsartan**, an angiotensin II receptor blocker (ARB).
  * Indications: **hypertension** (high blood pressure) and **heart failure**.

* When Diovan’s **patent expired in 2012**, it reached LOE.

  * After LOE, **multiple manufacturers** launched generic valsartan.
  * Result:

    * **Significantly increased competition** in the valsartan market.
    * **Prices dropped**, affecting Novartis’ sales of Diovan.

So this example illustrates the “generics problem” from an originator’s perspective: a once-exclusive, high-revenue product suddenly faces strong price pressure and shrinking market share when generics arrive.

---

#### Paragraph 6: Role of the Novartis Digital Finance Hub

This final paragraph connects the generics story to the **Digital Finance Hub**:

* The Hub applies **advanced analytics** to **financial processes**.

  * This includes data science, forecasting models, and other quantitative tools.

* One key responsibility is **forecasting future sales**:

  * Forecasts help with **strategic planning**:

    * Deciding production levels.
    * Planning marketing strategies.
    * Allocating resources across markets and products.
  * Forecasts also help assess the **financial impact of events** like:

    * Patent expiries and generic entry.
    * Price changes.
    * New product launches.

* These forecasts are used by **country organizations**:

  * Local Novartis affiliates generate **monthly and annual sales reports**.
  * These reports feed into **consolidated company-wide financial accounting** and decision-making.

So, the page links the medical/regulatory concept of generics to a **data and finance** problem: how to predict and manage the financial consequences when generics enter the market, using analytics developed by the Digital Finance Hub.


**Exact content of the page**

---

**Context**

A drug goes through several stages throughout its lifecycle, from its market introduction to its eventual decline in sales.

When a drug is first launched, its sales volume is typically low, as awareness and adoption are still developing. During the growth phase, sales increase rapidly as the product gains market acceptance, until reaching the maturity phase, where sales tend to stabilize.

After maturity, the entry of new competitors may slow down growth or even trigger a decline in sales volume. Ultimately, once the drug’s patent expires—a moment known as the *Loss of Exclusivity* (LoE)—generic versions enter the market. This often leads to a steep and sudden decline in sales volume, a phenomenon known as *generic erosion*. This *generic erosion* is the central topic of this year’s Datathon.

From a business perspective, anticipating how a drug will behave after generics enter the market is crucial, as it directly affects revenue forecasts, production planning, and strategic decisions. Accurate erosion predictions enable countries and companies not only to prepare for the post-patent period but also to minimize financial losses and adapt their competitive strategies.

*(Figure with labels)*
Vertical axis label: **Volume of sales**
Near the rising curve on the left: **Drug Launch**
Along the bottom left: **New Indication entry**
On the mid-rising portion: **New Competitor entry**
Near the peak: **Loss of Exclusivity (LoE)**
On the bottom right: **Generics entry (Gx)**
At the top of the shaded right-hand region: **DATATHON**
Horizontal axis label at far right: **Time**

*Caption:*
**Figure 1.1: Lifecycle of a drug: evolution of sales across different phases.**

---

**Generic Erosion**

We define the *Mean Generic Erosion* as the average of the normalized volume during the 24 months following the generic entry, as shown in the formula below. Post-entry volumes are normalized using the average monthly volume from the 12 months preceding the entry of generics.

3

---

## Detailed explanation of everything on this page

### 1. Text: “Context”

This section explains how a **drug’s sales evolve over time** and introduces the idea of **generic erosion**.

1. **Lifecycle of a drug**

   * A drug has a **lifecycle**, from launch to eventual decline in sales.
   * This lifecycle is similar to product life cycle in marketing: introduction, growth, maturity, decline.

2. **Introduction and growth**

   * At **launch**, sales are **low** because:

     * Doctors and patients are not yet fully aware of the product.
     * Access and reimbursement may still be expanding.
   * In the **growth phase**, sales rise quickly:

     * More doctors start prescribing it.
     * Guidelines and marketing increase adoption.
     * The product gains **market acceptance**.

3. **Maturity**

   * After a while, the drug reaches **maturity**:

     * Sales **stabilize** at a high level.
     * Growth slows because most potential prescribers already know and use the drug.
     * The market becomes more saturated.

4. **Competition and decline**

   * Once in maturity:

     * New competitor drugs can enter (other branded products in the same therapeutic area).
     * These competitors may **slow down growth or start a decline** in sales.
   * The major turning point is **Loss of Exclusivity (LoE)**:

     * The originator’s patent expires.
     * **Generic versions** are allowed on the market.
     * This typically causes a **steep, sudden drop** in the originator’s sales.

5. **Generic erosion**

   * The **sharp decline in sales after generics enter** is called **generic erosion**.
   * The text emphasizes this twice (in italics and bold) because:

     * It is the **main topic of the Datathon**.
     * Participants will model or predict this erosion.

6. **Business importance**

   * Predicting what happens after generics arrive is **crucial for the company**:

     * It affects **revenue forecasts** (how much money the company expects to earn).
     * It impacts **production planning** (how much to manufacture before and after LoE).
     * It influences **strategic decisions** (pricing strategy, promotion, launching new indications or formulations).
   * Good predictions help:

     * Prepare for the **post-patent period**.
     * **Minimize financial losses** (e.g., avoid overproduction).
     * **Adapt competitive strategies** (for example, shifting focus to other products or markets).

So this section sets the context: the Datathon problem is about **modeling and predicting the drop in sales after generic entry**.

---

### 2. Plot: Lifecycle of a drug (Figure 1.1)

The figure visually represents the lifecycle described in the text.

* **Axes**

  * Vertical axis: **“Volume of sales”** (how much of the drug is sold).
  * Horizontal axis: **“Time”** (from launch through maturity to generic entry and beyond).

* **Curve before LoE**

  * Starts low at the left: **“Drug Launch”**.
  * Sales gradually rise: initial adoption period.
  * Around some point on the bottom axis there is **“New Indication entry”**:

    * This means the drug gains approval for an additional medical use.
    * That typically **boosts sales**, shown by a steeper rise in the curve.
  * Later, a label **“New Competitor entry”** appears:

    * Another drug (branded) enters the market.
    * This flattens or slightly bends the sales curve, indicating competitive pressure.
  * The curve reaches a high level near the top.

* **Loss of Exclusivity (LoE)**

  * Near the peak, there is a label **“Loss of Exclusivity (LoE)”** with an arrow pointing to a point on the curve.
  * At this time:

    * Patent expires.
    * Generics can start to enter.
  * At the bottom axis under that point, there is **“Generics entry (Gx)”** with a vertical dashed line.

    * “Gx” stands for generic products.

* **Post-generic phase and Datathon window**

  * To the right of LoE, the curve **drops steeply**, representing **generic erosion**.
  * After the drop, the curve continues at a **much lower level**, indicating a new, lower sales equilibrium.
  * The entire region after generic entry is highlighted with a **yellow dashed rectangle** and a light background.

    * At the top of this shaded area, it says **“DATATHON”**.
    * This means the **Datathon problem focuses specifically on this post-generic period** and the behavior of sales after LoE.

* **Caption**

  * “**Figure 1.1: Lifecycle of a drug: evolution of sales across different phases.**”
  * It summarizes that the figure shows the full journey:

    * Launch → Growth → Maturity → Competition → LoE → Generic erosion.

In short, the figure turns the text into a timeline: you can “see” when launch, new indication, competitor entry, LoE, and generic erosion occur, and how they shape the sales curve.

---

### 3. Text: “Generic Erosion”

This subsection introduces a **quantitative measure** for erosion.

* **Mean Generic Erosion (MGE)**

  * Defined as:

    > “the average of the normalized volume during the 24 months following the generic entry.”

  * Step-by-step:

    1. Take the **24 months after generics enter** (i.e., after LoE).
    2. For each of those months, compute a **normalized sales volume**.
    3. Then compute the **average** of these 24 normalized values.
    4. That average is called **Mean Generic Erosion**.

* **How normalization works**

  * “Post-entry volumes are normalized using the average monthly volume from the 12 months preceding the entry of generics.”
  * Practically:

    * Compute the **baseline**:

      * Average monthly sales over the **12 months before generic entry**.
    * For each month after generic entry, divide the actual sales volume by this baseline.
    * So a normalized value:

      * **1.0** means “equal to average pre-generic sales”.
      * **0.5** means “half of the pre-generic average”.
      * **0.2** means “only 20% of pre-generic sales”, etc.

* **Why this metric is useful**

  * It allows comparison of **erosion severity** across different drugs and countries, independent of absolute sales levels:

    * A drug that drops from 100 to 20 units and a drug that drops from 1000 to 200 are *both* at 20% of their original sales.
  * It focuses on the **first 24 months after generics enter**, which is usually when the most dramatic changes occur and when strategic decisions matter most.

Although the text mentions “as shown in the formula below”, the actual formula is not on this page (it is presumably on the next one). Conceptually, the formula would look like:

* Let (V_{-12}, \dots, V_{-1}) be the monthly volumes in the 12 months before generic entry.
* Let (V_{1}, \dots, V_{24}) be the monthly volumes in the 24 months after entry.
* Baseline (pre-generic average):
  (\text{Baseline} = \frac{1}{12} \sum_{i=-12}^{-1} V_i).
* Normalized post-entry volume for month (t):
  (v_t = \frac{V_t}{\text{Baseline}}), for (t = 1,\dots,24).
* Mean Generic Erosion:
  (\text{MGE} = \frac{1}{24} \sum_{t=1}^{24} v_t).

(You do not see this math on the page, but this is exactly what the text is describing.)

---

### 4. Page number

* The number **“3”** at the bottom indicates this is page 3 of the document.

---

Overall, this page:

* Describes the **full sales lifecycle** of a drug.
* Introduces the concept of **generic erosion** after LoE.
* Shows a **visual lifecycle curve** with key events.
* Defines a **quantitative metric (Mean Generic Erosion)** that will be used in the Datathon to measure and predict the impact of generics on sales.


**Exact content of the page**

---

[
\text{Mean Generic Erosion} = \frac{1}{24} \sum_{i=0}^{23} \text{vol}_i^{\text{norm}}, \tag{1.1}
]

[
\text{vol}_i^{\text{norm}} = \frac{\text{Vol}_i}{\text{Avg}_j}, \tag{1.2}
]

[
\text{Avg}*j = \frac{1}{12} \sum*{i=-12}^{-1} y_{j,i}^{\text{act}}. \tag{1.3}
]

---

**1.2.1 Classification**

Drugs may be grouped into three distinct categories depending on their erosion pattern:

* **Low Erosion:** Drugs that experience a minimal impact after the generic entry. In this case, volume remains relatively stable, so the mean normalized erosion stays close to 1.

* **Medium Erosion:** Drugs that show a moderate decline in sales volume after generics enter, with mean erosion levels between 0 and 1.

* **High Erosion:** Drugs that experience a sharp drop in volume, resulting in a mean normalized erosion close to 0.

For the purposes of this Datathon, we classify drugs into two erosion buckets:

* **Bucket 1:** High erosion drugs, with mean erosion between 0 and 0.25. These are the primary focus of the Datathon.

* **Bucket 2:** Drugs with mean erosion greater than 0.25.

*(Figure with plot)*
Vertical axis label: **Volume**
Vertical dashed line label above: **Generics entry date**
Legend (top right of plot area):
– Red line: **High Erosion**
– Yellow line: **Medium Erosion**
– Blue line: **Low Erosion**

X-axis label (bottom right): **Month_num**

Right-hand side labels:
**Bucket 2 (B2)**
*Mean Erosion ∈ (0.25, 1]*

**Bucket 1 (B1)**
*Mean Erosion ∈ [0, 0.25]*

*Caption:*
**Figure 1.2: Representation of generic erosion classification.**

4

---

## Detailed explanation of everything on this page

### 1. Formulas for Mean Generic Erosion

This page gives the **precise mathematical definition** of the Mean Generic Erosion introduced on the previous page.

#### Formula (1.1)

[
\text{Mean Generic Erosion} = \frac{1}{24} \sum_{i=0}^{23} \text{vol}_i^{\text{norm}}.
]

* The Mean Generic Erosion is defined as the **average** of 24 values.
* Index (i) runs from **0 to 23**, representing the **24 months after generic entry**:

  * (i = 0): the month of generic entry (or immediately after).
  * (i = 1): first month after, and so on up to (i = 23).
* (\text{vol}_i^{\text{norm}}) is the **normalized sales volume** in month (i) after generic entry.
* The factor (\frac{1}{24}) simply computes the **mean** (sum divided by the number of months).

Interpretation:
If Mean Generic Erosion is **close to 1**, the drug keeps selling at levels close to its pre-generic baseline.
If it is **close to 0**, sales collapse after generics enter.

---

#### Formula (1.2)

[
\text{vol}_i^{\text{norm}} = \frac{\text{Vol}_i}{\text{Avg}_j}.
]

* (\text{Vol}_i) is the **actual sales volume** in month (i) after generic entry.
* (\text{Avg}_j) is a **baseline volume** for drug (j) (defined in (1.3)).
* The normalized volume is the **ratio**:

  * If (\text{Vol}_i = \text{Avg}_j), then (\text{vol}_i^{\text{norm}} = 1).
  * If (\text{Vol}_i) is half of the baseline, (\text{vol}_i^{\text{norm}} = 0.5).
  * If sales drop to 10% of baseline, (\text{vol}_i^{\text{norm}} = 0.1).

This normalization lets us compare erosion **across different drugs and markets** without being affected by absolute scale.

---

#### Formula (1.3)

[
\text{Avg}*j = \frac{1}{12} \sum*{i=-12}^{-1} y_{j,i}^{\text{act}}.
]

* (\text{Avg}_j) is the **average pre-generic sales** for drug (j).
* (y_{j,i}^{\text{act}}) denotes the **actual sales volume** of drug (j) in month (i).
* Here, (i) goes from **–12 to –1**, which are the **12 months before generic entry**:

  * (i = -12): 12 months before generics enter.
  * (i = -1): the last month before generics enter.
* The factor (\frac{1}{12}) computes the mean over those 12 months.

So, **Avg(_j)** is the **baseline monthly volume** before erosion starts. All post-generic volumes are compared to this baseline.

Putting the three formulas together:

1. Compute the **pre-entry average** ( \text{Avg}_j ).
2. Normalize each of the **24 post-entry monthly volumes** by dividing by ( \text{Avg}_j ).
3. Take the **average of those 24 normalized values** to get the Mean Generic Erosion.

---

### 2. Classification of erosion patterns

The section **“1.2.1 Classification”** explains how drugs are categorized based on how strongly their sales drop after generics appear.

#### Three conceptual categories

1. **Low Erosion**

   * Drugs with **minimal impact** after generic entry.
   * Their sales remain relatively **stable**.
   * Therefore, the **mean normalized erosion is close to 1**:

     * Average post-generic volume ≈ pre-generic baseline.

2. **Medium Erosion**

   * Drugs showing a **moderate decline** in sales after generics enter.
   * Their mean normalized erosion lies **between 0 and 1**, but not extremely close to either:

     * They lose some volume, but not a complete collapse.

3. **High Erosion**

   * Drugs that suffer a **sharp drop** in volume.
   * Their mean normalized erosion is **close to 0**:

     * On average, they sell only a small fraction of their pre-generic volume.

These three categories are conceptual and help to interpret the behavior seen in real sales data.

---

#### Two buckets used in the Datathon

For the Datathon’s modeling task, they simplify from three categories to **two numeric “buckets”** based on Mean Generic Erosion:

* **Bucket 1 (High erosion)**

  * Definition: **Mean erosion between 0 and 0.25.**
  * These drugs lose **75% or more** of their pre-generic sales on average.
  * They are stated to be **“the primary focus of the Datathon.”**
  * Reason: predicting severe erosion is both difficult and critical for financial planning.

* **Bucket 2**

  * Definition: **Mean erosion greater than 0.25.**
  * These drugs **retain more than 25%** of their pre-generic sales.
  * They include both medium and low erosion profiles.

So, in the Datathon you effectively have a **two-class problem**:

* Bucket 1 – heavily eroded drugs.
* Bucket 2 – more resilient drugs.

---

### 3. Plot: Representation of generic erosion classification (Figure 1.2)

The figure visualizes how high, medium, and low erosion patterns look over time and how they map to the two buckets.

#### Axes and reference line

* **Vertical axis:** “Volume” – normalized sales volume (implicitly).
* **Horizontal axis:** “Month_num” – months along the timeline, with:

  * Negative values (e.g., –12, –8, –4) indicating **months before generic entry**.
  * Zero around the **generic entry date**.
  * Positive values indicating **months after entry**.
* A **vertical dashed red line** marks **“Generics entry date”**.

  * To the left: pre-generic behavior.
  * To the right: erosion phase.

#### Three colored curves

* **Blue line – Low Erosion**

  * Before generics: rises and then stabilizes at a high level.
  * After generics: slowly declines, staying relatively high over the 24-month period.
  * Corresponds to **mean erosion close to 1** (small change in average volume).

* **Yellow line – Medium Erosion**

  * Initially similar growth and plateau.
  * After generics: declines more noticeably than the blue line.
  * Ends at a mid-level volume: not fully collapsed, not preserved.
  * Corresponds to **mean erosion between 0 and 1**.

* **Red line – High Erosion**

  * Also rises and stabilizes pre-entry.
  * At the generics entry date, it **drops very sharply**, almost vertically, to a very low level near zero.
  * Stays near zero afterwards.
  * This is the typical pattern for **severe generic erosion**.

The legend on the right labels these curves explicitly:
**High Erosion** (red), **Medium Erosion** (yellow), **Low Erosion** (blue).

#### Bucket annotations

On the right side of the figure, two boxes summarize the mapping to buckets:

* **Bucket 2 (B2)**
  “Mean Erosion ∈ (0.25, 1]”

  * Contains **medium and low erosion** curves (mostly the yellow and blue patterns).
  * Means that, on average, post-generic volumes are **more than 25%** of baseline.

* **Bucket 1 (B1)**
  “Mean Erosion ∈ [0, 0.25]”

  * Contains **high erosion** cases (red curve).
  * Means that, on average, post-generic volumes are **between 0% and 25%** of baseline.

The caption:

> **Figure 1.2: Representation of generic erosion classification.**

summarizes that this plot is not real data, but an **idealized illustration** of how different erosion levels look and how they are grouped for the Datathon.

---

### 4. Page number

* The number **“4”** at the bottom indicates this is page 4 of the document.

---

In summary, this page turns the qualitative idea of “generic erosion” into a **precise mathematical metric (Mean Generic Erosion)**, and then defines **numeric thresholds** that classify drugs into buckets for the Datathon. The figure provides an intuitive visual understanding of these categories and buckets.


**Exact content of the page**

---

**The Challenge**

The goal of this Datathon is to forecast the volume erosion that occurs after the entry of generic competitors into the market. Participants are asked to model and predict how the volume of a drug evolves over a 24-month period following the generic entry date.

Forecasts must be produced under two different business scenarios:

* **Scenario 1:** Forecast conducted immediately after the generic entry date, with no observed data post-entry. Participants must forecast monthly volumes from Month 0 to Month 23.

* **Scenario 2:** Forecast conducted six months after the generic entry date. Participants will have access to six months of post-entry actual data and must forecast from Month 6 to Month 23.

**Business Challenge**

Even though this Datathon challenge has a strong technical component, there is also a **business dimension** that drives the question we’re asking participants to solve. So, it’s not only about *how* you solve the problem, but also *why* you approach it that way.

Therefore, all teams presenting in front of the jury will be asked to deliver a deep exploratory analysis of their data preprocessing, with a special **focus on high-erosion cases** — that is, drugs whose sales experience a sharp drop after the generic entry.

Finally, we strongly encourage candidates to use visualization tools to make their findings clear, interpretable, and business-oriented.

*(Plot)*
Title: **Volume Evolution Around Generic Entry — BRAND_75FD (COUNTRY_A67D)**
Blue label inside left shaded area: **← Pre-generic**
Red label inside right shaded area: **Post-generic →**
Y-axis label: **Volume**
X-axis label: **Months Post Generic Entry**

*Caption:*
**Figure 2.1: Example of volume evolution pre and post generic entry**

5

---

## Detailed explanation of everything on this page

### 1. Text: “The Challenge”

This section defines **what participants must predict**.

* The **goal** is to forecast **volume erosion** after generic entry.
  Volume erosion = how much the sales volume of a branded drug declines once generic competitors appear.

* Participants must **model and predict the monthly sales volume of a drug** over a **24-month horizon** after the generic entry date.

So the task is a **time-series forecasting problem** focused on the two years following the arrival of generics.

#### Two business scenarios

The forecasts must be produced under **two different practical situations** that a real business might face:

1. **Scenario 1 – “zero-knowledge” post-entry**

   * The forecast is made **immediately at the generic entry date**.
   * There is **no observed post-entry data** yet (no months with generics on the market).
   * Participants must forecast **Month 0 to Month 23** purely based on:

     * Pre-generic history,
     * Other available features (country, drug class, etc.),
     * And their modeling assumptions.
   * This simulates the situation where the company wants a plan *right at LOE*, before seeing how the market reacts.

2. **Scenario 2 – “six-month update”**

   * The forecast is made **six months after generic entry**.
   * Now we **do have six months of post-entry actual data**.
   * Participants forecast **Month 6 to Month 23**, i.e., the remaining 18 months.
   * This simulates a **forecast update**: the business can see the first six months of erosion and wants to re-project the future with that new information.

In short:

* Scenario 1 = *long-horizon forecast with no post-generic evidence*.
* Scenario 2 = *updated forecast that incorporates early erosion behavior*.

---

### 2. Text: “Business Challenge”

Here they stress that the Datathon is **not just a pure machine-learning exercise**, but also about business understanding.

Key points:

* There is a **strong technical component**, but also a **business dimension**:

  * You must care not only about *how* you build the model (algorithms, features, metrics),
  * But also *why* you choose a certain approach in terms of business value, interpretability, and practicality.

* Teams presenting to the jury must:

  * Provide a **deep exploratory analysis** of the data.
  * Describe **data preprocessing** decisions (handling missing values, outliers, transformations).
  * Put **special emphasis on high-erosion cases**:

    * These are drugs whose sales **plummet sharply** after generic entry.
    * From a business point of view, these are the most critical and risky products.

* The organizers **strongly encourage visualizations**:

  * Plots and dashboards to show:

    * How volumes evolve over time,
    * How erosion differs by drug or country,
    * How predictions compare to actuals.
  * Visuals should make findings **clear, interpretable, and business-oriented** for non-technical stakeholders.

So, good solutions must combine **solid analytics** with **storytelling and insight**.

---

### 3. Plot: Example of volume evolution (Figure 2.1)

The figure provides a **realistic example** of sales behavior around the generic entry date for a specific brand–country pair.

#### Overall structure

* **Title:**
  “Volume Evolution Around Generic Entry — BRAND_75FD (COUNTRY_A67D)”
  This indicates:

  * The data comes from a specific branded product (“BRAND_75FD”).
  * The curve shown is from one particular **country** (“COUNTRY_A67D”).

* **Axes:**

  * Y-axis: **Volume**
    Represents units sold (or some volume measure) per month.
  * X-axis: **Months Post Generic Entry**
    Values include negative numbers (months before entry), 0 (entry), and positive numbers (months after entry).

#### Shaded regions and labels

* The background is divided into two colored regions:

  * **Left side (blue shading) – “Pre-generic”**
    Labeled with text in blue: **“← Pre-generic”**.
    This covers the **pre-entry period** where only the branded drug is sold (no generics yet).

  * **Right side (red/pink shading) – “Post-generic”**
    Labeled with text in red: **“Post-generic →”**.
    This covers the **post-entry period** where generics are present.

* The **vertical dashed red line at x = 0** marks the **generic entry date**:

  * To the left: historical sales before generics.
  * To the right: observed erosion and eventual stabilization.

#### The time-series line

* A single **blue line with dots** shows the monthly sales volume.

* **Pre-generic behavior:**

  * Volume fluctuates between roughly 55,000 and 70,000 units.
  * There is some variability, but the level is **high and relatively stable**.
  * This is typical of the **maturity phase** of a successful drug.

* **Around generic entry (x ≈ 0):**

  * After the vertical line, the volume starts to **drop sharply**.
  * This illustrates a **high-erosion case**: the originator rapidly loses market share to generics.

* **Post-generic behavior:**

  * Over the next months, volume falls from ~50,000 to just a few thousand.
  * After ~10–15 months, it stabilizes at a **very low plateau** (near 5,000 units or less).
  * This is exactly the kind of pattern that produces a **low Mean Generic Erosion** (close to 0) and would be in **Bucket 1**.

#### Caption

“**Figure 2.1: Example of volume evolution pre and post generic entry**”

* This confirms the plot’s purpose:

  * To show qualitatively how volumes behave before and after generic entry.
  * To give participants an intuitive feel for what **high generic erosion** looks like in real data.

---

### 4. Summary

This page defines:

* **What** you must predict: the **shape of the sales curve** over 24 months after generic entry.
* **Under which conditions**: **two forecast scenarios** (at entry and 6 months later).
* **How to think about it**: combine **technical modeling** with **business insight**, with emphasis on **high-erosion drugs**.
* **What the data look like**: a concrete example of **sharp erosion** illustrated in Figure 2.1.


**Exact content of the page**

---

**Evaluation Criteria**

The selection of the Datathon winners will follow a two-phase evaluation process.

**Phase 1: Model Evaluation**

All participating teams must submit volume predictions for the full test dataset, which includes both Scenario 1 and Scenario 2 cases. Phase 1 consists of two sequential steps:

1. **Phase 1-a – Scenario 1 Accuracy**
   All teams will be evaluated based on their forecasting accuracy for Scenario 1. Teams will be ranked according to their prediction errors, and the top 10 teams will advance to Step 2.

2. **Phase 1-b – Scenario 2 Accuracy**
   Only the top 10 teams from Step 1 will be evaluated on their forecasting accuracy for Scenario 2. Based on this second accuracy ranking, the top 5 teams will advance to Phase 2.

**Phase 2: Jury Evaluation**

The final five teams will present their methodologies, modeling decisions, insights, and conclusions to a jury composed of both technical and business experts. After reviewing all presentations, the jury will select the top three winning teams.

6

---

## Detailed explanation

There are no plots or mathematical formulas on this page; it purely describes the **evaluation and ranking process** of the Datathon.

### Overall structure: two phases

* The text states that winner selection follows a **two-phase evaluation**:

  1. **Phase 1 – Model Evaluation** (quantitative, based on prediction accuracy).
  2. **Phase 2 – Jury Evaluation** (qualitative, based on presentations and business insight).

So first your **models** are filtered and ranked, then your **presentation and reasoning** are judged.

---

### Phase 1: Model Evaluation

All teams must submit predictions for the **entire test dataset**, covering:

* **Scenario 1**: forecast from Month 0 to Month 23 with no post-generic data.
* **Scenario 2**: forecast from Month 6 to Month 23 with six months of post-generic data.

Phase 1 has **two sequential steps**:

#### Step 1 – Phase 1-a: Scenario 1 Accuracy

* **Who is evaluated?**
  All participating teams.

* **What is evaluated?**
  The **forecasting accuracy for Scenario 1** — that is, how well each team predicts the 24 months after generic entry when **no post-entry data** are available.

* **How are teams compared?**
  Teams are **ranked by their prediction errors** (the document does not specify the exact error metric here, but typically something like RMSE, MAE, or MAPE).

* **Cutoff:**
  The **top 10 teams** (those with the **lowest errors**, i.e., highest accuracy) move on to Step 2.

Interpretation: Scenario 1 acts as a **first filter**; you must demonstrate strong long-horizon forecasting using only pre-generic information to remain in the competition.

---

#### Step 2 – Phase 1-b: Scenario 2 Accuracy

* **Who is evaluated?**
  Only the **top 10 teams** from Step 1.

* **What is evaluated?**
  Their **forecasting accuracy for Scenario 2**, which uses:

  * Six months of observed post-generic data as input,
  * Forecast horizon from Month 6 to Month 23.

* **How are teams compared?**
  Again via a ranking based on **prediction error** in this second scenario.

* **Cutoff:**
  Based on this ranking, the **top 5 teams** advance to **Phase 2**.

Interpretation: Scenario 2 tests how well teams **update and refine forecasts** once some post-generic behavior is known. Only those who perform well in **both** scenarios are allowed to present to the jury.

---

### Phase 2: Jury Evaluation

Once Phase 1 has narrowed the field to **five finalist teams**, the process becomes more **qualitative and business-oriented**.

* **What finalists must do:**

  * Present their:

    * **Methodologies** (models, features, assumptions),
    * **Modeling decisions** (why certain choices were made),
    * **Insights** (what they learned from the data: patterns, drivers of erosion, etc.),
    * **Conclusions** (business implications, recommendations).
  * Presentations are made to a **jury**.

* **Who is the jury?**

  * A panel of **both technical and business experts**:

    * Technical experts evaluate soundness of models, data science rigor, and interpretation of results.
    * Business experts evaluate relevance, clarity of insights, and impact on decision-making.

* **Outcome:**

  * After the presentations and discussions, the jury **selects the top three winning teams**.

So Phase 2 emphasizes:

* The ability to **communicate** complex modeling choices clearly.
* The **business value** and interpretability of the solution, not just metrics.
* How well teams connect their forecasts to **strategic decisions** and practical use in a pharmaceutical setting.

---

In summary, this page tells you:

1. First you must **excel quantitatively** (forecast accuracy in both scenarios) to reach the final.
2. Then you must **excel qualitatively** (storytelling, justification, business insight) to win.


**Exact content of the page**

---

**The Data**

Participants are provided with datasets containing historical monthly volumes for pharmaceutical products that have experienced generic entry. The *target variable* is monthly volume for 2,293 country-brand combinations that have undergone a generic entry event. The data is organized into train and a test set, as described below.

* **Training Set: 1,953 observations**
  Each observation includes up to 24 months of volume before generic entry and up to 24 months after entry, allowing participants to analyze historical patterns and post-entry dynamics.

* **Test Set: 340 observations**
  The test set is divided into two forecasting scenarios:

  – **Scenario 1 (228 observations):** Forecast from Month 0 to Month 23, immediately following the generic entry.

  – **Scenario 2 (112 observations):** Forecast from Month 6 to Month 23, where the first 6 months of post-entry volume are provided.

Understanding the structure and purpose of these datasets is essential for designing robust forecasting models and tailoring approaches to the two distinct scenarios.

---

**Description of the DataFrames**

The data is provided in three separate DataFrames: a volume dataset, a generics dataset, and a dataset containing descriptive information about each medicine.

**3.1.1 Volume Dataset (df_volume.csv)**

This dataset contains sales volumes before and after the entry of generic competitors. Each record includes:

* **country**: market of reference,

* **brand_name**: product of interest,

* **month**: calendar month of the observation,

* **months_postgx**: number of months relative to the month of generic entry (0 = entry month; negative = prior months; positive = subsequent months),

* **volume**: number of units sold.

The variable *volume* is used as the target variable in the analysis.

7

---

## Detailed explanation of everything on this page

There are no plots or formulas on this page; it purely describes the **data structure** you will work with in the Datathon.

### 1. Overall description of the data

* You are given historical **monthly sales volumes** for pharmaceutical products that have had **generic entry**.

* Each “product” is identified not just by its brand but also by the **country** where it is sold.
  Hence they talk about **2,293 country–brand combinations**:
  e.g., “Drug X in Italy” and “Drug X in Spain” are two separate combinations.

* The **target variable** you must forecast is **monthly volume** (number of units sold) for each of these country–brand pairs after generics enter.

So every row in the main volume dataset represents a **specific brand in a specific country in a specific month**, with the corresponding volume.

---

### 2. Train–test split and observation counts

The 2,293 combinations are split into:

* **Training Set: 1,953 observations**

  * Here, “observation” means one **country–brand pair**.
  * For each of these 1,953 pairs, you get:

    * Up to **24 months of data before generic entry**.
    * Up to **24 months after generic entry**.
  * This rich historical window lets you:

    * Learn typical shapes of pre-generic growth and post-generic erosion.
    * Estimate the Mean Generic Erosion or other features.
    * Train forecasting models.

* **Test Set: 340 observations**

  * Again, each observation is a country–brand pair.
  * These are the pairs on which your **model performance is evaluated**.
  * The test set is further divided according to the two forecast scenarios previously defined:

    1. **Scenario 1 (228 observations)**

       * For these 228 pairs you must forecast **from Month 0 to Month 23**.
       * At Month 0 the generic enters; you do not see any post-entry volumes.
       * Effectively, you have historical pre-generic data and possibly other features, and you must predict the entire 24-month post-generic trajectory.

    2. **Scenario 2 (112 observations)**

       * For these 112 pairs you are given **the first 6 months of post-entry volumes** (Months 0–5).
       * You must forecast **Month 6 to Month 23**.
       * This simulates updating forecasts after observing early erosion behavior.

The paragraph under the bullets stresses that **understanding this structure** is crucial:

* Different scenarios require different modeling strategies (pure extrapolation vs. extrapolation with partial post-entry data).
* Robust models must generalize across both.

---

### 3. Description of the DataFrames

The document then explains that the full dataset is split into **three separate DataFrames**:

1. A **volume dataset** (time series of monthly sales).
2. A **generics dataset** (not yet described on this page, likely containing generic entry date info, number of competitors, etc.).
3. A **descriptive dataset** with **medicine attributes** (e.g., therapeutic class, molecule, dosage form).

This modular separation is typical in data science projects:

* One table for **time-series metrics**,
* One for **events** (generic entry),
* One for **static attributes**.

Participants will usually need to **join** these DataFrames on keys such as `country` and `brand_name`.

---

### 4. Volume Dataset (df_volume.csv)

Subsection **3.1.1** details the main table you will work with: `df_volume.csv`.

Each row represents a **single month** for a given country–brand combination, and includes the following fields:

1. **`country`**

   * Indicates the **market** where the product is sold (e.g., COUNTRY_A67D).
   * Important because erosion patterns may differ significantly by country due to regulations, reimbursement, and competition.

2. **`brand_name`**

   * Identifies the **specific branded product**.
   * Combined with `country`, it uniquely defines a time series you will forecast.

3. **`month`**

   * The **calendar month** corresponding to the observation (e.g., 2015-01, 2015-02).
   * This gives the chronological order and can be used to extract features such as year, seasonality, etc.

4. **`months_postgx`**

   * A relative index of months with respect to the **generic entry month**:

     * `0` = the **entry month** (when generics launch).
     * Negative values (`-1`, `-2`, …) = **months before generic entry**.
     * Positive values (`1`, `2`, …) = **months after generic entry**.
   * This field is extremely important:

     * It aligns all time series around the **generic entry event**.
     * It allows you to easily extract windows such as “24 months pre-entry” or “24 months post-entry”.
     * It provides a natural time axis for modeling generic erosion.

5. **`volume`**

   * The **number of units sold** in that month for that country–brand pair.
   * This is the **dependent variable** you are trying to predict.

The final sentence confirms:

> “The variable *volume* is used as the target variable in the analysis.”

That is, in all forecasting models, `volume` is the **output** you want to predict, while all other fields (plus any engineered features) are **inputs**.

---

### 5. Page number

* The number **“7”** at the bottom indicates this is page 7 of the document.

---

In summary, this page defines:

* How many time series you have in train and test.
* How those series are structured around generic entry.
* The key fields of the main `df_volume.csv` file.
* That **monthly volume** is the central quantity you must model and forecast under the two erosion scenarios.


**Exact content of the page**

---

**3.1.2 Generics Dataset (df_generics.csv)**

This dataset provides information on the presence and evolution of generics for each country–brand combination. It includes:

* **country** and **brand_name**: identifiers,
* **months_postgx**: defined as above,
* **n_gxs**: number of generics available at each point in time.

The number of generics may vary over time as products enter or exit the market.

---

**3.1.3 Medicine Information Dataset (df_medicine_info.csv)**

This dataset provides contextual characteristics for each drug. It includes:

* **country** and **brand_name**,
* **therapeutic_area**: therapeutic category of the medicine,
* **hospital_rate**: proportion of units distributed through hospitals,
* **main_package**: predominant commercial format (e.g., pills, vials),
* **biological**: boolean indicating whether the product is biological,
* **small_molecule**: boolean indicating whether the product is a low–molecular-weight synthetic compound.

This information supports the characterization of products and the interpretation of observed sales dynamics.

---

**Additional Information**

* All historical sales volume is provided at the monthly level, starting from either the brand launch or the first available data point.

* You may use all data provided for model training (e.g., test data from Scenario 2 may be used when training the model for Scenario 1).

* Bucket information is not included in the current dataset but can be derived using Equation 1.1.

* You are free to use any approach or model for your solution; however, explainability and simplicity of the results will be valued.

* Volume may be reported in different units (milligrams, packs, pills, etc.) depending on the country–brand combination.

* Categorical variables can be assumed to remain constant over time.

* Some columns may contain missing values. It is up to you to decide whether to keep them as they are, infer missing values, or apply another preprocessing method.

8

---

## Detailed explanation of everything on this page

There are no plots or new formulas on this page; it extends the **data description** and gives practical modeling guidance.

---

### 1. Generics Dataset (`df_generics.csv`)

This table describes **how many generic competitors exist over time** for each branded product in each country.

Fields:

1. **`country` and `brand_name`**

   * Together, these identify the **originator drug in a specific market**.
   * They are the same keys used in the volume dataset, so you can join tables on these fields.

2. **`months_postgx`**

   * Same definition as before: months relative to generic entry (0 = entry month, negative = before, positive = after).
   * Aligns generic-competition information with the volume time series.

3. **`n_gxs`**

   * This is the **number of generic products currently on the market** for that brand–country–month.
   * It can **change over time**:

     * At first, it may be 0 (before generic entry).
     * It jumps to 1 or more at entry.
     * It can later increase as more generics launch, or decrease if some leave the market.

Implication for modeling:

* `n_gxs` is a crucial explanatory feature: more generics usually means **stronger price competition and higher erosion**.
* Because it is time-varying, it allows you to model dynamic effects like:

  * “Sales drop further when a second or third generic appears.”

---

### 2. Medicine Information Dataset (`df_medicine_info.csv`)

This table contains **static or slowly changing attributes** of each branded product.

Fields:

1. **`country`, `brand_name`**

   * Again, the identifiers for merging with other tables.

2. **`therapeutic_area`**

   * The **therapeutic category** (e.g., cardiovascular, oncology).
   * Different therapeutic areas can show different erosion patterns due to clinical need, prescribing habits, or reimbursement rules.

3. **`hospital_rate`**

   * The **proportion of units distributed via hospitals** versus retail pharmacies.
   * High hospital share may imply:

     * Centralized procurement.
     * Different generic substitution rules.
     * Potentially different erosion dynamics.

4. **`main_package`**

   * The **dominant commercial form** (tablets, capsules, vials, etc.).
   * Packaging type can influence pricing, distribution, and generic competition.

5. **`biological` (boolean)**

   * Indicates whether the product is a **biologic medicine**.
   * Biologics often face different post-LOE behavior (biosimilars, more complex substitution), so erosion may be slower or different in shape.

6. **`small_molecule` (boolean)**

   * Indicates whether the product is a **small-molecule synthetic compound**.
   * Traditional generics are typically small molecules; erosion tends to be faster and deeper.

Use in analysis:

* These variables help you **characterize and segment** drugs.
* They can be used as **features** in models (one-hot encoded or embedded).
* They also help in **interpreting** differences in erosion, e.g.:

  * “Oncology biologics erode slower than cardiovascular small molecules.”

---

### 3. Additional Information (practical modeling notes)

Each bullet gives important rules or hints:

1. **Monthly granularity**

   * All sales are at **monthly** resolution.
   * Sequences start at either:

     * The **brand launch**, or
     * The **first available month** in the data.
   * This defines the time span available for feature engineering (seasonality, trends).

2. **Use of all data for training**

   * You are allowed to use **all provided data** for training purposes.
   * Example: you can use test rows from **Scenario 2** (which include some post-generic data) when training a model for **Scenario 1**.
   * Practically, they are not enforcing a strict time-based “hold-out” for training; they just keep a separate **evaluation set**.

3. **Bucket information not provided**

   * The data you get does **not** contain labels like “Bucket 1 / Bucket 2”.
   * However, you can **derive the bucket** for any series by applying **Equation 1.1** (Mean Generic Erosion definition from earlier pages).
   * This lets you:

     * Analyze erosion distributions.
     * Possibly build classification models in addition to forecasting.

4. **Freedom of modeling approach**

   * You may use **any model or algorithm** (classical time series, ML, deep learning, etc.).
   * However, they explicitly value **explainability and simplicity**:

     * A very complex black-box model may be less attractive than a simpler, interpretable one that delivers similar accuracy.

5. **Volume units may differ**

   * `volume` can be in **different units**:

     * Milligrams, packs, pills, etc., depending on brand and country.
   * This means absolute levels **across different products are not directly comparable**.
   * To avoid mistakes, you might:

     * Normalize per series (as they do with Mean Generic Erosion).
     * Avoid comparing raw volumes of two different drugs unless you account for units.

6. **Categorical variables are time-invariant**

   * Variables like `therapeutic_area`, `main_package`, `biological`, `small_molecule` are assumed **constant over time**.
   * This simplifies preprocessing:

     * You can just merge them once per series and not worry about changes over months.

7. **Missing values**

   * Some columns may have **missing data**.
   * It is **your responsibility** to choose a preprocessing strategy:

     * Keep them as NaN and let certain models handle them,
     * Impute (e.g., with mean/median, forward-fill for time series),
     * Or drop affected rows/columns where appropriate.
   * This choice can significantly impact model quality and must be justified in your methodology.

---

### 4. Page number

* The number **“8”** at the bottom indicates this is page 8 of the document.

---

Overall, this page tells you:

* How to use `df_generics.csv` to track the **intensity of generic competition**.
* How to use `df_medicine_info.csv` to add **contextual and categorical features**.
* Key **rules and freedoms** you have when building models (full data usage, handling of units and missing values, emphasis on interpretability).



**Exact content of the page**

---

**Metric: Prediction Error**

Understanding the evaluation metric is essential for interpreting model performance in this challenge. Forecast accuracy for both scenarios is assessed using a modified version of the Prediction Error (PE), specifically designed to capture short- and long-term deviations between predicted and actual volumes after generic entry. The metric combines monthly errors and accumulated errors across different post-entry periods, normalized for comparability across country–brand combinations.

**Metric Phase 1-a**

**4.1.1 Description**

In the first phase (Scenario 1), participants must provide predictions without access to any actual post-entry volume data. To capture early and mid-term erosion patterns, the prediction error is computed from four components with the following weights:

* Absolute monthly error for all 24 months                     (20%)

* Absolute accumulated error for Months 0–5                   (50%)

* Absolute accumulated error for Months 6–11                  (20%)

* Absolute accumulated error for Months 12–23                 (10%)

All four components are normalized by the average monthly volume of the last 12 months before generic entry (denoted *Avg*(_j)), ensuring comparability across brands.

**4.1.2 Formula**

[
PE_j = 0.2 \left( \frac{\sum_{i=0}^{23} \left| Y^{act}*{j,i} - Y^{pred}*{j,i} \right|}{24 \cdot Avg_j} \right)

* 0.5 \left( \frac{\left| \sum_{i=0}^{5} Y^{act}*{j,i} - \sum*{i=0}^{5} Y^{pred}*{j,i} \right|}{6 \cdot Avg_j} \right)
  ]
  [
  \quad\quad\quad\quad + 0.2 \left( \frac{\left| \sum*{i=6}^{11} Y^{act}*{j,i} - \sum*{i=6}^{11} Y^{pred}_{j,i} \right|}{6 \cdot Avg_j} \right)
* 0.1 \left( \frac{\left| \sum_{i=12}^{23} Y^{act}*{j,i} - \sum*{i=12}^{23} Y^{pred}_{j,i} \right|}{12 \cdot Avg_j} \right)
  \tag{4.1}
  ]

9

---

## Detailed explanation

There are no plots on this page; the focus is entirely on the **prediction error metric** used to evaluate models in Phase 1, Scenario 1.

---

### 1. General idea of the metric

* The metric is a **modified Prediction Error (PE)** tailored to this Datathon.
* Goal: capture **both**:

  * Month-by-month deviations between predicted and actual volumes.
  * Deviations in **cumulative volume** over several key post-generic periods.
* Everything is **normalized** by the pre-generic average volume *Avg*(_j), so that:

  * A small drug and a blockbuster can be compared fairly.
  * The error is interpreted **relative to typical pre-LOE volume**.

Here, index (j) identifies a specific **country–brand combination**, and index (i) refers to the **month after generic entry** (Scenario 1 covers (i = 0,\dots,23)).

* (Y^{act}_{j,i}): actual observed volume for brand–country (j) in month (i).
* (Y^{pred}_{j,i}): model-predicted volume for brand–country (j) in month (i).
* (Avg_j): the **average monthly volume in the 12 months before generic entry**, defined earlier in Equation (1.3).

So (PE_j) is a **single error score** summarizing how good your predictions are for one series (j).

---

### 2. Description of the four components

The description section says the error is a weighted combination of four components:

1. **Absolute monthly error over all 24 months** – weight 20%

   * Measures the average **per-month deviation** between prediction and reality, across the whole 2-year period.
   * It is *local* in time: each month contributes separately through (|Y^{act}*{j,i} - Y^{pred}*{j,i}|).

2. **Absolute accumulated error for Months 0–5** – weight 50%

   * Measures the error in **total volume over the first 6 months** after generic entry.
   * This early period is critical because:

     * It tells you how steep the initial erosion is.
     * Business decisions (production, pricing) are very sensitive here.
   * It gets the **largest weight (50%)**, reflecting its importance.

3. **Absolute accumulated error for Months 6–11** – weight 20%

   * Measures the error in **total volume between Months 6 and 11** (second half of the first year).
   * Captures medium-term behavior after the initial shock.
   * Its weight is lower than the 0–5 period but still significant.

4. **Absolute accumulated error for Months 12–23** – weight 10%

   * Measures the error in **total volume in the second year**.
   * This period is important but has the **smallest weight**, since long-term forecasts are inherently uncertain and usually less critical than the early months.

Note that the weights sum to 1:

[
0.2 + 0.5 + 0.2 + 0.1 = 1.0,
]

so (PE_j) is a **weighted average** of these normalized error terms.

---

### 3. Detailed breakdown of the formula (4.1)

The formula translates the textual description into exact mathematics.

[
PE_j = T_1 + T_2 + T_3 + T_4
]

where:

#### Term (T_1): average normalized monthly absolute error (20%)

[
T_1 = 0.2 \cdot \frac{\sum_{i=0}^{23} \left| Y^{act}*{j,i} - Y^{pred}*{j,i} \right|}{24 \cdot Avg_j}.
]

* The numerator sums the **absolute error** (|Y^{act} - Y^{pred}|) for each month (i = 0,\dots,23).
* Dividing by 24 gives the **average absolute monthly error**.
* Dividing by (Avg_j) **normalizes** this error relative to typical pre-LOE volume for that brand.

  * Example: a value of 0.1 means on average you miss by 10% of pre-generic monthly volume.
* The factor 0.2 applies the **20% weight** assigned to this component.

This term rewards models that get **each month’s level roughly right**, regardless of cumulative totals.

---

#### Term (T_2): accumulated error in Months 0–5 (50%)

[
T_2 = 0.5 \cdot
\frac{\left| \sum_{i=0}^{5} Y^{act}*{j,i} - \sum*{i=0}^{5} Y^{pred}_{j,i} \right|}
{6 \cdot Avg_j}.
]

* (\sum_{i=0}^{5} Y^{act}_{j,i}): actual **total volume** over months 0 to 5.
* (\sum_{i=0}^{5} Y^{pred}_{j,i}): predicted total over the same months.
* The absolute value around their difference gives the **absolute cumulative error** in that early window.
* Dividing by (6 \cdot Avg_j) converts this to a **relative error**, since (6 \cdot Avg_j) approximates what total volume would have been over 6 months if the pre-LOE level had continued.
* The weight 0.5 reflects that this period is **most important**.

This term focuses on **getting the early erosion trajectory right in aggregate**, even if small month-to-month fluctuations are slightly off.

---

#### Term (T_3): accumulated error in Months 6–11 (20%)

[
T_3 = 0.2 \cdot
\frac{\left| \sum_{i=6}^{11} Y^{act}*{j,i} - \sum*{i=6}^{11} Y^{pred}_{j,i} \right|}
{6 \cdot Avg_j}.
]

* Same structure as (T_2), but for months 6–11.
* Again normalized by (6 \cdot Avg_j) because this window covers 6 months.
* Weight 0.2, aligning with the **20% importance** of this mid-term period.

This term measures how well your model captures **continued erosion in late Year 1**.

---

#### Term (T_4): accumulated error in Months 12–23 (10%)

[
T_4 = 0.1 \cdot
\frac{\left| \sum_{i=12}^{23} Y^{act}*{j,i} - \sum*{i=12}^{23} Y^{pred}_{j,i} \right|}
{12 \cdot Avg_j}.
]

* Totals and compares actual vs. predicted volume for months 12–23 (12 months).
* Normalized by (12 \cdot Avg_j) because a full year is 12 months.
* Weighted by 0.1 (10%).

This emphasizes **long-term cumulative accuracy**, but with a smaller influence on the overall score.

---

### 4. Interpretation of the metric

* **Scale-free:**
  Because every term is divided by (Avg_j), a PE value is roughly interpretable as a **relative error** with respect to typical pre-LOE sales.

* **Early months prioritized:**
  The high weight on Months 0–5 (50%) means that **errors just after generic entry** are penalized much more than errors two years later.
  This reflects the **business relevance**: companies care most about early erosion to manage inventory, pricing, and financial expectations.

* **Combination of local and global views:**

  * The first term (T_1) looks at **all months individually** (local accuracy).
  * The other three terms look at **cumulative volumes** over larger windows (global behavior).
  * This prevents a model from “cheating” by being right only on totals but very wrong month-to-month, or vice versa.

* **Brand-by-brand score:**
  (PE_j) is computed **per country–brand combination**.
  In evaluation, they will typically **average PE across all series** to rank teams.

In summary, this page introduces a **carefully designed error metric** that measures how well your model predicts both the detailed monthly path and the key cumulative impacts of generic erosion, with particular emphasis on the **first six months after generic entry**.



**Exact content of the page**

---

**Metric Phase 1-b**

**4.2.1 Description**

In the second phase (Scenario 2), participants receive the first six months of actual post-entry data. Predictions are required from Month 6 onward. Because this phase benefits from partial information, the metric uses three components with the following weights:

* Absolute monthly error for Months 6–23                     (20%)

* Absolute accumulated error for Months 6–11                  (50%)

* Absolute accumulated error for Months 12–23                 (30%)

All components are normalized by the average monthly volume of the last 12 months before generic entry (Avg(_j)).

**4.2.2 Formula**

[
PE_j = 0.2 \left( \frac{\sum_{i=6}^{23} \left| Y^{act}*{j,i} - Y^{pred}*{j,i} \right|}{18 \cdot Avg_j} \right)

* 0.5 \left( \frac{\left| \sum_{i=6}^{11} Y^{act}*{j,i} - \sum*{i=6}^{11} Y^{pred}*{j,i} \right|}{6 \cdot Avg_j} \right)
  ]
  [
  \quad\quad\quad\quad + 0.3 \left( \frac{\left| \sum*{i=12}^{23} Y^{act}*{j,i} - \sum*{i=12}^{23} Y^{pred}_{j,i} \right|}{12 \cdot Avg_j} \right)
  \tag{4.2}
  ]

---

**Final Metric and Interpretation**

After computing (PE_j) for each country–brand combination, scenario-level performance is obtained by aggregating errors across two predefined buckets. Bucket 1 (high erosion) is given twice the weight of Bucket 2:

[
PE = \frac{2}{n_{B1}} \sum_{j=1}^{n_{B1}} PE_{j,B1} ; + ; \frac{1}{n_{B2}} \sum_{j=1}^{n_{B2}} PE_{j,B2}. \tag{4.3}
]

Each country–brand pair appears in only one scenario, leading to a distinct ((PE_j,) equation 4.1 and 4.2()) for every observation. These are then aggregated using the weighting shown above to produce a single performance score per scenario.

Ideally, (PE_j) values lie between 0 and 1, where 0 indicates perfect accuracy. Values greater than 1 are possible and indicate poor prediction quality. At the scenario level, if all (PE_j = 0), the final score is 0, and if all (PE_j = 1), the final score equals 3. Because individual errors may exceed 1, scenario-level values can also exceed 3.

10

---

## Detailed explanation of everything on this page

There are no plots on this page; it completes the definition of the **evaluation metric** by:

1. Defining the prediction error for **Scenario 2 (Phase 1-b)**, and
2. Explaining how individual-series errors are aggregated into one **final score** per scenario.

---

### 1. Metric Phase 1-b (Scenario 2)

#### 4.2.1 Description

Scenario 2 corresponds to the case where:

* We already know the **first six months of post-entry volumes** (Months 0–5).
* We must **predict from Month 6 to Month 23**.

Because we have partial post-generic information, the metric only evaluates:

1. **Absolute monthly error for Months 6–23** – weight 20%

   * This is the per-month deviation between predictions and actual volumes across these **18 months**.
   * It checks that your model’s **month-by-month shape** is reasonable.

2. **Absolute accumulated error for Months 6–11** – weight 50%

   * This is the error in **total volume** over the first 6 evaluated months (Months 6–11).
   * It gets the **largest weight (50%)**, meaning:

     * Accurately predicting the **first part of the forecast horizon** is most important in Scenario 2.
     * Business-wise, this period is where updated decisions are most impactful.

3. **Absolute accumulated error for Months 12–23** – weight 30%

   * This covers the **second year of forecast** (12 months).
   * It has lower weight than Months 6–11 but is still significant, reflecting the importance of getting the longer-term path approximately right.

As in Scenario 1, **all components are normalized** by (Avg_j), the average pre-generic monthly volume, to make errors comparable across products of different sizes.

---

#### 4.2.2 Formula (4.2)

For a given country–brand combination (j), the Scenario-2 prediction error (PE_j) is:

[
PE_j = T_1 + T_2 + T_3,
]

with:

1. **Term (T_1): average normalized monthly absolute error (20%)**

   [
   T_1 = 0.2 \cdot \frac{\sum_{i=6}^{23} \left| Y^{act}*{j,i} - Y^{pred}*{j,i} \right|}{18 \cdot Avg_j}.
   ]

   * Summation from (i = 6) to (23) covers the **18 forecast months**.
   * Numerator: sum of absolute monthly errors.
   * Denominator: (18 \cdot Avg_j) turns this into a **relative average monthly error**.
   * Weighted by 0.2 → contributes 20% of the final (PE_j).

2. **Term (T_2): cumulative error Months 6–11 (50%)**

   [
   T_2 = 0.5 \cdot
   \frac{\left| \sum_{i=6}^{11} Y^{act}*{j,i} - \sum*{i=6}^{11} Y^{pred}_{j,i} \right|}
   {6 \cdot Avg_j}.
   ]

   * Compares **total actual vs total predicted volume** over Months 6–11 (6 months).
   * Denominator (6 \cdot Avg_j) is the “expected” total if pre-LOE volume had stayed constant.
   * Weighted by 0.5, reflecting its **dominant importance** in Scenario 2.

3. **Term (T_3): cumulative error Months 12–23 (30%)**

   [
   T_3 = 0.3 \cdot
   \frac{\left| \sum_{i=12}^{23} Y^{act}*{j,i} - \sum*{i=12}^{23} Y^{pred}_{j,i} \right|}
   {12 \cdot Avg_j}.
   ]

   * Looks at total volume over Months 12–23 (12 months).
   * Denominator (12 \cdot Avg_j) normalizes for 12 months of baseline volume.
   * Weighted by 0.3 (30%).

Overall, (PE_j) in Scenario 2 is a **weighted sum of normalized errors** emphasizing:

* Correct early performance in the updated forecast (Months 6–11),
* Reasonable monthly tracking (T1),
* And sensible long-term cumulative behavior (T3).

---

### 2. Final Metric and Interpretation

Once you have (PE_j) for each series in its scenario (using Eq. 4.1 for Scenario 1 or Eq. 4.2 for Scenario 2), you still need **one performance score per scenario** to compare teams. This is where **buckets** come in.

#### Bucket-level aggregation (4.3)

Recall:

* **Bucket 1 (B1):** high-erosion drugs (mean erosion ∈ [0, 0.25]).
* **Bucket 2 (B2):** all other drugs (mean erosion ∈ (0.25, 1]).

Equation (4.3):

[
PE = \frac{2}{n_{B1}} \sum_{j=1}^{n_{B1}} PE_{j,B1}

* \frac{1}{n_{B2}} \sum_{j=1}^{n_{B2}} PE_{j,B2}.
  ]

Here:

* (n_{B1}) = number of series in Bucket 1 for that scenario.
* (n_{B2}) = number of series in Bucket 2.
* (PE_{j,B1}) = prediction error for series (j) in Bucket 1.
* (PE_{j,B2}) = prediction error for series (j) in Bucket 2.

The structure means:

* You compute the **average error in Bucket 1**, then **multiply by 2**.
* You compute the **average error in Bucket 2**, with weight 1.
* Add them to get a **single scenario-level PE**.

Interpretation:

* Bucket 1 (high erosion) is given **twice the importance** of Bucket 2.
  This reflects that high-erosion cases are **more critical** for the business and are the primary focus of the Datathon.

Also note:

* “Each country–brand pair appears in only one scenario”:

  * So every series contributes **one (PE_j)**, either using eq. 4.1 (Scenario 1) or 4.2 (Scenario 2), never both.
* These errors are then **aggregated bucket-wise** using Eq. 4.3.

---

### 3. Range and interpretation of (PE_j) and final PE

The last paragraph clarifies how to interpret values:

* **Ideally**, (PE_j) lies between **0 and 1**:

  * (0) = **perfect forecasts** (no deviation).
  * Values near 0 = very good predictions.
  * Values near 1 ≈ “average error similar in magnitude to baseline volume”.

* **Values > 1 are possible**:

  * They correspond to **very poor predictions**, where cumulative errors exceed typical pre-LOE volumes.

* At the **scenario level**:

  * If **all** (PE_j = 0), then the final PE = 0 (perfect team performance).
  * If **all** (PE_j = 1), then the final PE = 3.
    Why 3? Because:

    * Average error of 1 in both buckets,
    * Bucket 1 gets weight 2, Bucket 2 weight 1 → 2×1 + 1×1 = 3.
  * Since some individual (PE_j) can exceed 1, the **scenario-level PE can exceed 3**.

So, the lower your **final PE**, the better. Teams are effectively competing to **minimize** this bucket-weighted, normalized prediction error.



**Exact content of the page**

---

**Metric Rationale**

The Prediction Error metric provides a comprehensive assessment of model performance by incorporating three key dimensions:

* **Generic erosion:** Captured through the bucket-level weighting (Equation 4.3), where high-erosion cases have double importance.

* **Months since generic entry:** Reflected through differential weighting across time periods in Equations 4.1 and 4.2, emphasizing early post-entry months.

* **Seasonality effects:** Included via the monthly-error terms in both formulas, ensuring that short-term fluctuations contribute appropriately to the final score.

Participants should aim to minimize the final Prediction Error by generating forecasts that accurately capture early erosion dynamics, stabilize over time, and reflect seasonal variation.

11

---

## Detailed explanation

There are no plots or explicit formulas on this page; instead it explains **why** the Prediction Error (PE) metric was designed the way it was, connecting back to Equations 4.1, 4.2, and 4.3 defined on previous pages.

### 1. Generic erosion

* This refers to **how strongly sales drop after generic entry**.
* The metric accounts for generic erosion through **bucket-level weighting** (Equation 4.3):

  * Bucket 1 (high-erosion drugs) is given **twice the weight** of Bucket 2 in the final scenario-level PE.
  * That means errors on high-erosion drugs affect your score roughly **twice as much** as errors on more stable or low-erosion drugs.
* Business rationale:

  * High-erosion products are riskier and more financially critical; predicting their erosion well is therefore more important.
  * The metric encourages teams to **optimize performance specifically on these difficult, high-impact cases**, not just average performance.

### 2. Months since generic entry

* This dimension is about **where in time** the forecast is being evaluated.
* The metric gives **different weights to different post-entry periods**, as specified in:

  * Equation 4.1 (Scenario 1: Months 0–5, 6–11, 12–23, plus monthly errors),
  * Equation 4.2 (Scenario 2: Months 6–11 and 12–23, plus monthly errors).
* In both scenarios:

  * Early months after generic entry (0–5 in Scenario 1, 6–11 in Scenario 2) get the **highest weight (50%)**.
  * Later months get smaller weights.
* This reflects that:

  * **Early post-entry dynamics** (how fast volume collapses or stabilizes) are the most relevant for decision-making (inventory, pricing, planning).
  * Long-term forecasts are still important but **less critical** and more uncertain, hence lower weights.

So the metric is explicitly **time-aware**: mistakes right after LOE are punished more than mistakes two years later.

### 3. Seasonality effects

* Pharmaceutical sales often show **seasonality** and short-term fluctuations (e.g., higher volumes in winter, supply spikes, etc.).
* The metric includes **monthly-error terms** in both formulas:

  * In Eq. 4.1 and 4.2, the first term is the **average absolute monthly error** over the evaluated period.
* These monthly terms ensure that:

  * The model is evaluated on how well it matches the **month-to-month pattern**, not just cumulative totals.
  * Seasonal peaks and troughs, or local deviations, **contribute to the final score**.
* This design encourages models that:

  * Capture **seasonal patterns** and **short-term volatility** realistically,
  * Rather than only getting the overall area under the curve approximately right.

### 4. What participants should optimize for

The closing sentence summarizes the modeling goal:

Participants should try to **minimize the final Prediction Error** by producing forecasts that:

1. **Capture early erosion dynamics**

   * Correct shape and magnitude of the sharp decline after generics appear.
   * Particularly important because of the high weights on early periods.

2. **Stabilize over time**

   * After the initial drop, forecasts should show a **reasonable long-term plateau or slower decline**, consistent with actual behavior.
   * This is reflected in the accumulated-error terms for mid- and long-term windows.

3. **Reflect seasonal variation**

   * The monthly-error components reward models that reproduce **recurrent patterns** (e.g., regular peaks/troughs).
   * Ignoring seasonality would increase monthly errors even if cumulative volumes were roughly correct.

Overall, this page explains that the PE metric is intentionally **multi-dimensional**: it rewards models that are good on **high-erosion drugs**, on **early post-generic periods**, and on **short-term/seasonal patterns**, not just on average volume levels.


