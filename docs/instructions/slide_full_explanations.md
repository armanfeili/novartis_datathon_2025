## 1. Text content of the slide

**Top-left title**

* *The BCN Digital Finance Hub*

**Top-right**

* *NOVARTIS* (company logo)

---

### Left side – “The team”

**Section title**

* *The team*

**Team composition (with icons and numbers)**

* **34** – *Data scientists*
* **8** – *Finance profiles*
* **15** – *Visualization, ML, Software Engineers & Devops*

**Diversity and origin (right side of the team section)**

* **+14** – *Different nationalities in a diverse team*
* **66%** – *Local talent*

(Under these numbers there is a row of silhouettes of people, representing the team.)

---

### Bottom-left – “Barcelona: European pole for tech and AI talent”

**Section title**

* *Barcelona: European pole for tech and AI talent*

**Bullet points**

* *Tech cluster: In the last decade, many companies (Amazon, Microsoft, AstraZeneca,…) located their global AI hubs.*
* *Forefront centers & infrastructures: Bcn supercomputing center, Quantum Computer, Synchrotron,…*
* *Proficiency local universities promoting Data Science, Mathematics or Statistics.*
* *Attractive city for international talent to reallocate.*

---

### Top-right – “Evolution of the hub” (bar chart)

**Section title**

* *Evolution of the hub*

**Bar chart values**

Years on the x-axis: **2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025**

Bars labelled (most likely number of people in the hub):

* 2018 → **10**
* 2019 → **19**
* 2020 → **28**
* 2021 → **34**
* 2022 → **40**
* 2023 → **50**
* 2024 → **53**
* 2025 → **55**

---

### Bottom-right – “Background” (stacked bar + legend)

**Section title**

* *Background*

**Stacked vertical bar**

* At the top: **100%**
* From bottom to top, the bar is divided into segments with percentages:

  * **19%**
  * **20%**
  * **17%**
  * **20%**
  * **24%**

**Legend mapping colors to disciplines**

* *Mathematics & Statistics*
* *Computer Science*
* *Engineering*
* *Economics*
* *Physics & Others*

(Each discipline corresponds to one of the colored segments of the stacked bar; from bottom to top: Physics & Others, Economics, Engineering, Computer Science, Mathematics & Statistics.)

**Additional box**

* *5 PhD*
* *3 Bioinformatics Background*

(Indicating that within the team there are 5 people with PhDs and 3 people with a bioinformatics background.)

---

## 2. Detailed explanation of each element

### 2.1 Main title – “The BCN Digital Finance Hub”

* **“BCN”** stands for Barcelona.
* The title tells you that this slide is describing a specific Novartis structure located in Barcelona focused on **digital finance**.
* A “Digital Finance Hub” is a centralized team where finance experts and technologists use **data, analytics, and AI** to support financial processes and decisions across the company.

The Novartis logo in the top-right corner indicates that this hub is part of **Novartis**, the global pharmaceutical company.

---

### 2.2 The team – composition and roles

This section quantifies the people working in the hub and the types of profiles they have.

1. **34 Data scientists**

   * This is the largest group.
   * A **data scientist** typically:

     * explores and cleans data,
     * builds statistical and machine-learning models,
     * creates predictive and prescriptive analytics for business questions (for example, forecasting costs, optimizing budgets, detecting anomalies).
   * The high number signals that the hub is **strongly analytics-driven**.

2. **8 Finance profiles**

   * These are people with a **finance/business background** (e.g., financial analysts, controllers).
   * They understand accounting, budgeting, cost centers, P&L, and financial regulations.
   * They are crucial because they translate **business questions** into data problems and validate whether analytical results make sense in real financial terms.

3. **15 Visualization, ML, Software Engineers & Devops**

   * This group includes multiple technical roles:

     * **Visualization** specialists: build dashboards and reports (e.g., in Power BI, Tableau, or web apps) to make data understandable for non-technical stakeholders.
     * **ML (Machine Learning) engineers**: take models from data scientists and make them production-ready, scalable, and reliable.
     * **Software Engineers**: develop the applications, services, and tools used by finance and data teams.
     * **DevOps**: manage infrastructure, deployment pipelines, monitoring, and reliability.
   * The number (15) suggests a **solid engineering backbone** to support models, tools, and data pipelines in production.

The combination of these three groups (34 + 8 + 15 = 57 people) shows that the hub is **multidisciplinary**, combining:

* deep data science,
* financial domain expertise,
* and robust software/ML engineering.

The silhouettes at the bottom visually reinforce the idea of a **modern, professional, and diverse** team.

---

### 2.3 Diversity and local presence

On the right side of the “Team” section:

1. **“+14 Different nationalities in a diverse team”**

   * This means that within the team there are people from **more than 14 different countries**.
   * The “+” sign indicates “at least 14, possibly more”.
   * It highlights **cultural diversity**, which is often linked to a variety of perspectives, languages, and experiences, valuable for global financial and data problems.

2. **“66% Local talent”**

   * This states that **66% of the team members are local**, i.e., they come from Barcelona or Spain.
   * It implies that:

     * the hub leverages the **local talent pool**, and
     * at the same time it remains **international**, because 34% are non-local.

You can think of this as a simple proportion:

* Local members: 66% of the total.
* International members: 34% of the total.
  Formally, if the total team size is 57, then approximately 0.66 × 57 ≈ 38 locals and 0.34 × 57 ≈ 19 non-locals (the exact numbers may be rounded, but this is the order of magnitude).

---

### 2.4 Barcelona as a European tech and AI hub

This block answers the question: **Why Barcelona?** Why did Novartis choose to place a digital finance hub there?

1. **“Tech cluster”**

   * A **tech cluster** is a region where many technology companies are concentrated.
   * The slide says: *“In the last decade, many companies (Amazon, Microsoft, AstraZeneca,…) located their global AI hubs.”*
   * This means that in the last 10 years, several global players opened AI or tech centers in Barcelona.
   * Implication: Barcelona offers a **strong ecosystem** — talent, infrastructure, networking — that supports AI and digital innovation.

2. **“Forefront centers & infrastructures”**

   * Examples listed:

     * *Bcn supercomputing center* – high-performance computing capabilities.
     * *Quantum Computer* – advanced research in quantum computing.
     * *Synchrotron* – large scientific facility used in physics, materials science, biology, etc.
   * These institutions make Barcelona a **scientific and technological hotspot**, which is attractive for data-intensive and research-oriented work.

3. **“Proficiency local universities promoting Data Science, Mathematics or Statistics.”**

   * Barcelona’s universities (UPF, UPC, UB, etc.) have strong programs in:

     * Data Science,
     * Mathematics,
     * Statistics.
   * This ensures a **continuous pipeline of graduates** with the skills needed for data and AI roles.

4. **“Attractive city for international talent to reallocate.”**

   * “Reallocate” here basically means **relocate**.
   * The message: Barcelona is a city that many international professionals are happy to move to, thanks to:

     * quality of life,
     * climate,
     * culture,
     * and cost of living (relative to other tech hubs).
   * This makes it easier to **hire and retain** foreign talent.

Overall, this block argues that Barcelona is a **strategic location** combining tech ecosystem, academic excellence, and lifestyle.

---

### 2.5 Evolution of the hub – bar chart

This chart shows how the hub has **grown over time**, year by year.

* Horizontal axis: years **2018–2025**.
* Each bar height corresponds to the **number of people in the hub** (or similar metric like FTEs).
* The labels on top of the bars:

  * **2018 → 10**
  * **2019 → 19**
  * **2020 → 28**
  * **2021 → 34**
  * **2022 → 40**
  * **2023 → 50**
  * **2024 → 53**
  * **2025 → 55**

**Interpretation:**

* From 2018 (10 people) to 2025 (55 people), the hub has grown more than **fivefold**.
* Growth is roughly **monotonic** (always increasing), with particularly strong jumps:

  * 2018 → 2019: +9
  * 2019 → 2020: +9
  * 2022 → 2023: +10
* The smaller increases at the end (53 → 55) suggest that the hub is **maturing** and approaching a more stable size.

This visually supports the narrative that the hub is **rapidly expanding and consolidating** within Novartis.

---

### 2.6 Background – academic and professional profiles

This section explains **what kinds of degrees and disciplines** the team members come from.

#### 2.6.1 Stacked bar (discipline mix)

* At the top: **“100%”** – this means the whole bar represents **100% of the team**.
* The bar is split into five color bands, each with a percentage:

  * **19%** – *Physics & Others* (darkest band at the bottom)
  * **20%** – *Economics*
  * **17%** – *Engineering*
  * **20%** – *Computer Science*
  * **24%** – *Mathematics & Statistics* (lightest band at the top)

So, if you imagine 100 people in the hub:

* 24 would have a background in **Mathematics & Statistics**,
* 20 in **Computer Science**,
* 17 in **Engineering**,
* 20 in **Economics**,
* 19 in **Physics or other disciplines**.

This composition shows that the hub **mixes quantitative, technical, and economic skills**:

* quantitative theory (math/stat),
* programming and systems (computer science, engineering),
* understanding of markets and finance (economics),
* plus additional perspectives (physics & others).

#### 2.6.2 PhD and Bioinformatics

The small box under the legend says:

* **“5 PhD”**
* **“3 Bioinformatics Background”**

This means:

* At least **5 team members hold a PhD**, indicating a strong **research and advanced analytics** capacity.
* At least **3 members have a background in bioinformatics**, which connects data science with **biology and medicine** — very relevant for a pharmaceutical company like Novartis.

Even though this is a **finance hub**, the presence of bioinformatics and PhDs underlines that Novartis is leveraging **cross-functional scientific expertise**.

---

### 2.7 Formulas

* There are **no explicit mathematical formulas** on this slide.
* All numbers are descriptive statistics: counts (34, 8, 15, etc.) or percentages (66%, 24%, 20%, 17%, 19%), but they are not presented as equations.

---

## 1. Exact content of the slide

**Title**

* `Lifecycle of a drug`

**Axes**

* On the vertical axis (y-axis): `Volume of sales`
* On the horizontal axis (x-axis, right side): `Time`

**Labels along the curve**

* Near the beginning of the curve, close to the x-axis origin: `Drug Launch`
* Further along the x-axis, under the first grey vertical dotted line: `New Indication entry`
* Above the second grey vertical dotted line, on the rising part of the curve: `New Competitor entry`
* Near the peak of the curve, just before the decline:
  `Loss of  
  Exclusivity  
  (LoE)`
* On the x-axis under the orange vertical dashed line: `Generics entry (Gx)`

**Highlighted area**

* At the top of a large yellow dashed rectangle on the right side: `DATATHON`

**Logo and slide number**

* Top-right corner: `NOVARTIS`
* Bottom-right: slide number `4`

There are no explicit formulas on this slide.

---

## 2. Detailed explanation of everything on the slide

### 2.1 Axes and overall idea

The chart shows the **lifecycle of a drug** in the market, represented as a curve of **“Volume of sales”** (y-axis) over **“Time”** (x-axis).

* The **y-axis** (Volume of sales) goes upward: higher on the chart means more units sold / higher revenue.
* The **x-axis** (Time) goes from left to right: it represents the years after the drug is introduced.

The grey line is the **sales curve** of a branded drug over its life.

---

### 2.2 Early phase – Drug launch and growth

* **“Drug Launch”** (bottom left) marks the moment the product reaches the market for the first time.
* Initially, the sales are low but start to climb as:

  * doctors become aware of the product,
  * reimbursement and pricing are established,
  * more patients are prescribed the drug.

The curve rises gradually, reflecting **market uptake**.

---

### 2.3 Expansion – New indication entry

Further along the x-axis, there is a **first grey vertical dotted line** with the label:

* **“New Indication entry”**

This means:

* The same drug is approved to treat an **additional indication** (another disease or patient subgroup).
* As a result, the **eligible patient population increases**, which usually causes:

  * an acceleration in sales growth,
  * a visible upward step or steeper slope in the curve.

On the chart, the sales curve bends upwards after this point.

---

### 2.4 Competitive pressure – New competitor entry

Next, there is a **second grey vertical dotted line** with the label:

* **“New Competitor entry”**

This indicates:

* A **new branded competitor** (another company’s drug) enters the same therapeutic area.
* Competition typically:

  * slows the growth of sales,
  * may flatten the curve,
  * and can even lead to a small decline, depending on how strong the competitor is.

In the figure, after this point the curve continues to grow, but more slowly and with a flatter slope – reflecting **stronger competition and market share pressure**.

---

### 2.5 Maturity and peak – Loss of Exclusivity (LoE)

Near the peak of the curve, there is a label:

* **“Loss of Exclusivity (LoE)”**

This is a key regulatory and commercial milestone:

* **Exclusivity** refers to the legal protection (patents, data exclusivity, etc.) that prevents generic manufacturers from copying the drug.
* **Loss of Exclusivity (LoE)** occurs when these protections expire.
* After LoE:

  * **generic manufacturers** are allowed to launch cheaper copies (same active ingredient),
  * payers and healthcare systems often push prescriptions towards generics to save costs.

Just after this milestone, the grey curve reaches its **maximum** and then turns sharply downward, marking the beginning of **rapid erosion of sales**.

---

### 2.6 Generics entry (Gx) and decline

At the x-axis under the orange dashed vertical line, there is a label:

* **“Generics entry (Gx)”**

This point corresponds to:

* The moment **generic drugs (Gx)** actually enter the market.
* It is typically very close to, or just after, the Loss of Exclusivity.

After **Generics entry**:

* The grey line falls steeply, showing a **fast drop in volume of sales** for the original branded drug.
* Over time, the curve levels off at a much lower level, representing the **residual sales** that remain despite generics (e.g., due to brand loyalty, specific formulations, or slower substitution in some markets).

---

### 2.7 DATATHON window

On the right side, a **yellow dashed rectangle** highlights a region starting around the LoE / Generics entry point and extending forward in time. At its top it says:

* **“DATATHON”**

This shaded area represents:

* The **time window that is the focus of the Datathon challenge**.
* It emphasizes that participants are expected to work on **data and models related to the post-LoE period**, such as:

  * forecasting the decline,
  * understanding price and volume erosion,
  * optimizing strategies during and after generics entry.

So the Datathon is not about the whole drug lifecycle, but specifically about the **critical phase when generics enter and sales collapse**.

---

### 2.8 Summary

* The slide visualizes the **commercial trajectory of a typical branded drug**:

  1. Launch and early growth,
  2. Expansion with new indications,
  3. Competitive pressure from new branded entrants,
  4. Peak near Loss of Exclusivity,
  5. Rapid decline when generics (Gx) enter.
* There are **no mathematical formulas**, only a conceptual curve and key business/clinical milestones.
* The **yellow “DATATHON” box** marks the part of this lifecycle that the Datathon analyses will focus on.

---

## 1. Exact content of the slide

**Title**

* `Generic erosion`

**Section heading**

* `1. Formula`

**Definition text**

* `Mean Generic Erosion is defined as the mean of the normalized volumes after the generic entry considering a 24-month horizon. Volumes after generic entry are normalized by the average monthly volume of the last 12 months before the generic entry.`

**Formulas**

1.

[
\textit{Mean Generic Erosion} = \frac{\sum_{i=0}^{23} Vol_{norm,i}}{24}
]

2.

[
VolNorm_i = \frac{Vol_i}{Avg_j}
]

3.

[
Avg_j = \frac{\sum_{i=-12}^{-1} Y^{act}_{j,i}}{12}
]

**Other**

* Top-right: `NOVARTIS` logo
* Bottom-right: slide number `5`

There are no plots or charts on this slide, only text and formulas.

---

## 2. Detailed explanation of the slide

### 2.1 Concept: “Generic erosion”

“Generic erosion” here means **how much a branded drug’s sales fall after generic competitors enter the market**. The slide defines a quantitative metric called **Mean Generic Erosion** to summarize this effect.

The idea:

* Look at the **24 months after generic entry**.
* Compare those monthly sales to the **average sales in the 12 months before** generics arrived.
* Take the **mean** of those normalized post-generic volumes.

This produces a single number that captures, on average, how much volume remains vs. the pre-generic level.

---

### 2.2 Main metric: Mean Generic Erosion

[
\textit{Mean Generic Erosion} = \frac{\sum_{i=0}^{23} Vol_{norm,i}}{24}
]

* (i) indexes **months after generic entry**:

  * (i = 0) is the month of generic entry (or the first month after, depending on convention),
  * (i = 23) is 24 months later.
* (Vol_{norm,i}) is the **normalized volume** in month (i) (definition below).
* The numerator (\sum_{i=0}^{23} Vol_{norm,i}) adds up all the normalized monthly volumes over the 24-month horizon.
* Dividing by 24 gives their **arithmetic mean**.

Interpretation:

* If Mean Generic Erosion ≈ 1 → on average, post-generic monthly volume is similar to pre-generic average (little erosion).
* If Mean Generic Erosion < 1 → sales dropped; for example, value 0.4 means volume is 40% of pre-generic level on average.
* If Mean Generic Erosion > 1 → very unusual in practice; it would mean volume increased despite generics.

---

### 2.3 Normalized monthly volume: (VolNorm_i)

[
VolNorm_i = \frac{Vol_i}{Avg_j}
]

* (Vol_i): **actual sales volume** of the product in month (i) after generic entry (same month index as before).
* (Avg_j): **reference average volume** for product (or market) (j) in the 12 months before generics.

So (VolNorm_i) is a **ratio**:

* (VolNorm_i = 1) → current month’s volume equals the pre-generic average.
* (VolNorm_i = 0.5) → volume is 50% of the pre-generic average.
* (VolNorm_i = 2) → volume is double the pre-generic average.

Using normalized volumes makes erosion **comparable across drugs and markets**, independent of absolute sales size.

---

### 2.4 Pre-generic reference average: (Avg_j)

[
Avg_j = \frac{\sum_{i=-12}^{-1} Y^{act}_{j,i}}{12}
]

* (j): index for the **drug, country, or market** being analysed.
* (Y^{act}_{j,i}): **actual observed volume** for drug/market (j) in month (i).
* The summation runs from (i=-12) to (i=-1):

  * (i=-12) is 12 months before generic entry,
  * (i=-1) is the last month just before generic entry.
* The numerator (\sum_{i=-12}^{-1} Y^{act}_{j,i}) adds the **12 pre-generic months**.
* Dividing by 12 gives the **average monthly volume before generics**.

Thus (Avg_j) is the **baseline** against which each post-generic month is compared. It captures the “normal” sales level before erosion starts.

---

### 2.5 Putting it together

1. Compute (Avg_j) from the 12 months before generics.
2. For each of the 24 months after generic entry:

   * Take actual volume (Vol_i),
   * Normalize it: (VolNorm_i = Vol_i / Avg_j).
3. Average these 24 normalized values to obtain **Mean Generic Erosion**.

The result is a **single erosion score per drug/market** over a 24-month period that is easy to compare across products, portfolios, or countries.



---

## 1. Exact content of the slide

**Axes**

* Vertical axis label (y-axis): `Volume`
* Horizontal axis label (x-axis): `Month_num`

Tick labels visible on x-axis:

* `-12`, `-8`, `-4`, `0`, `4`, `8`, `12`, `16`, `20`

**Vertical dotted line**

* Red vertical dotted line at x = 0 with label in red above it:
  `Generics entry date`

**Legend (top-right)**

* Red line: `High Erosion`
* Yellow line: `Medium Erosion`
* Blue line: `Low Erosion`

**Bucket labels (right side)**

* Dark blue bracket around the blue and yellow curves, with label:
  `Bucket 2 (B2)`
* Red bracket around the red curve, with label:
  `Bucket 1 (B1)`

**Logo and slide number**

* Top-right corner: `NOVARTIS`
* Bottom-right: slide number `6`

There are **no formulas** on this slide.

---

## 2. Detailed explanation of the slide

This slide visualizes **three typical patterns of volume erosion after generic entry**, and shows how they are grouped into **buckets**.

### 2.1 Axes and time reference

* The **y-axis** is labelled `Volume`, representing the **sales volume** (e.g., units, prescriptions) of a branded drug.
* The **x-axis** is labelled `Month_num`, representing **time in months** relative to generic entry:

  * Negative values (e.g., `-12`, `-8`, `-4`) are **months before** generics enter.
  * `0` is the **Generics entry date**.
  * Positive values (`4`, `8`, `12`, `16`, `20`) are **months after** entry.

A **vertical red dotted line at x = 0** marks the exact moment when **generic competitors launch**. This is the key event that triggers erosion.

### 2.2 Three erosion profiles

The chart shows three coloured curves, each corresponding to a different **erosion intensity**:

1. **High Erosion (red curve)**

   * Before x = 0, the red curve slowly increases: sales grow in the months leading up to generic entry.
   * Right after the **Generics entry date**, the red curve **drops extremely fast**, almost vertically.
   * Within a few months (around x ≈ 4–6), volume is very close to zero and then stays at a very low residual level.
   * Interpretation: in this pattern, **generics capture the market very quickly**, and the original brand loses almost all its sales. This is a **high-erosion** case.

2. **Medium Erosion (yellow curve)**

   * Before generics, the yellow curve is the **highest volume** among the three, relatively flat and stable.
   * After x = 0, the volume **falls sharply but not to zero**, and then continues to decline gradually over time.
   * At 20+ months, the yellow curve is still above the red one but significantly lower than its pre-generic level.
   * Interpretation: this is a **medium-erosion trajectory**: the brand loses a large part of its volume quickly, but retains some market for a longer period.

3. **Low Erosion (blue curve)**

   * Before generics, the blue curve is moderate and slightly decreasing.
   * After x = 0, the volume **declines more slowly and more smoothly** compared with the other two lines.
   * Over time it gradually slopes downward but remains the **highest** volume among the three profiles in the post-generic period.
   * Interpretation: this is a **low-erosion case**: generics impact the brand, but the decline is slower and less severe. The original product retains a **substantial share** of the market over time.

These three curves are **qualitative templates** showing how different markets or products might behave after generics enter.

### 2.3 Buckets (B1 and B2)

On the right side of the slide, two brackets define **buckets**, i.e., groups of erosion patterns:

1. **Bucket 1 (B1)** – red label and bracket

   * Applies to the **red “High Erosion”** curve.
   * Represents products/markets where **volume collapses rapidly** after generic entry and remains minimal.
   * Operational meaning: these are **high-risk, aggressive erosion** situations, where mitigation strategies are most urgent.

2. **Bucket 2 (B2)** – blue label and bracket

   * Applies to both the **yellow “Medium Erosion”** and **blue “Low Erosion”** curves.
   * Groups together patterns where the product still maintains **meaningful sales** after generics, whether the erosion is medium or low.
   * Operational meaning: these markets may still justify **longer-term commercial or pricing strategies**, lifecycle management, and resource allocation.

Essentially, the slide shows that although there are three conceptual erosion profiles (high/medium/low), they can be **clustered into two high-level buckets**:

* B1 → **severe** immediate erosion,
* B2 → **moderate or slow** erosion.

### 2.4 How this links to the rest of the analysis

* Earlier slides defined **Mean Generic Erosion** mathematically.
* This slide uses **curves over time** to illustrate the **different shapes** that erosion can take, not just the average.
* The buckets (B1, B2) can be used later for:

  * classification of real products/markets,
  * modeling or prediction targets,
  * and designing different strategies depending on the erosion pattern.

There are no equations here, but the chart translates the **quantitative concept of erosion** into **intuitive visual trajectories** and **business categories**.



---

## 1. Exact content of the slide

**Axes**

* Vertical axis label (y-axis): `Volume`
* Horizontal axis label (x-axis, bottom right): `Month_num`

Tick labels visible on x-axis:

* `-12`, `-8`, `-4`, `0`, `4`, `8`, `12`, `16`, `20`

**Vertical dotted line**

* Red vertical dotted line at x = 0 with red label above it:
  `Generics entry date`

**Legend (top-right)**

* Red line: `High Erosion`
* Yellow line: `Medium Erosion`
* Blue line: `Low Erosion`

**Bucket labels**

* Next to blue and yellow curves on the right:
  `Bucket 2 (B2)`
* Next to red curve on the right:
  `Bucket 1 (B1)`

**Interval conditions (boxes)**

* To the right of `Bucket 2 (B2)`, in a blue-bordered box:
  `Mean Erosion ∈ (0.25,1]`
* To the right of `Bucket 1 (B1)`, in a red-bordered box:
  `Mean Erosion ∈ [0, 0.25]`

**Logo and slide number**

* Top-right corner: `NOVARTIS`
* Bottom-right: slide number `7`

There are no other texts or formulas on this slide.

---

## 2. Detailed explanation of the slide

This slide builds directly on the previous one. It shows **three erosion curves** over time and now **links each bucket to a numerical range of Mean Erosion**, using the metric defined earlier (*Mean Generic Erosion*).

### 2.1 Axes and time reference

* **Y-axis – Volume**: represents the **sales volume** of the original branded drug (e.g., units, prescriptions).
* **X-axis – Month_num**: represents time in months relative to **generic entry**:

  * Negative numbers (e.g., `-12`, `-8`, `-4`) = months **before** generics enter.
  * `0` = **Generics entry date**.
  * Positive numbers (`4`, `8`, `12`, `16`, `20`) = months **after** generics launch.

The **red vertical dotted line at x = 0**, labelled *Generics entry date*, marks the exact point where generics come onto the market and erosion starts.

### 2.2 The three erosion profiles (same as previous slide)

The three coloured curves show typical shapes of volume over time:

1. **High Erosion – red curve**

   * Before month 0: volume increases modestly.
   * Immediately after generics enter: volume collapses very quickly toward almost zero.
   * This is the **steepest and deepest decline**.

2. **Medium Erosion – yellow curve**

   * Before month 0: volume is highest among the three, quite stable.
   * After generics: volume drops sharply but not to zero; then it continues to decline gradually.
   * Erosion is **significant but not complete**.

3. **Low Erosion – blue curve**

   * Before month 0: volume is moderate and slowly decreasing.
   * After generics: decline is **slower and smoother**, remaining higher than the other two curves in the long term.
   * Erosion is **relatively mild**; the drug keeps a sizable share.

These are qualitative templates representing different **market behaviours** once generics appear.

### 2.3 Buckets B1 and B2

The slide groups these erosion patterns into **two buckets**:

1. **Bucket 1 (B1)** – red label

   * Associated with the **red “High Erosion”** curve.
   * Represents cases where sales **crash quickly** after generic entry and remain very low.

2. **Bucket 2 (B2)** – blue label

   * Associated with the **yellow “Medium Erosion”** and **blue “Low Erosion”** curves.
   * Represents cases where the product **retains more volume** over time (even if erosion is present).

The brackets and labels visually show which curves belong to which bucket.

### 2.4 Numerical criteria using Mean Erosion

On this slide, each bucket is formally defined using the **Mean Erosion** metric (i.e., Mean Generic Erosion from the previous formula slide).

#### 2.4.1 Bucket 2 (B2): Mean Erosion ∈ (0.25,1]

Box next to *Bucket 2 (B2)*:

* `Mean Erosion ∈ (0.25,1]`

This is mathematical interval notation:

* `(0.25,1]` means **greater than 0.25 and up to and including 1**.

  * The round bracket `(` at 0.25 means 0.25 itself is **excluded**.
  * The square bracket `]` at 1 means 1 is **included**.

Interpretation:

* Products/markets in B2 have **Mean Erosion above 0.25** and **no more than 1**.
* Being close to 1 means post-generic volumes are similar to pre-generic volumes (little erosion).
* Being just above 0.25 means they still retain **more than 25%** of their pre-generic average volume over the 24-month horizon.

So B2 covers **medium to low erosion** situations.

#### 2.4.2 Bucket 1 (B1): Mean Erosion ∈ [0, 0.25]

Box next to *Bucket 1 (B1)*:

* `Mean Erosion ∈ [0, 0.25]`

Here:

* `[0, 0.25]` means **greater than or equal to 0 and less than or equal to 0.25**.

  * Both 0 and 0.25 are **included** because of the square brackets.

Interpretation:

* B1 includes drugs/markets where the **average normalized volume over the 24 months after generics is 25% or less** of the pre-generic baseline.
* At the extreme:

  * Mean Erosion = 0 → essentially **no sales left** after generics (total erosion).
  * Mean Erosion = 0.25 → the product retains **only a quarter** of its pre-generic volume.

This matches the **high-erosion red curve**, which drops almost to zero soon after generic entry.

### 2.5 How this ties back to the formula

From the earlier slide, we know:

[
\text{Mean Erosion} = \text{Mean Generic Erosion} = \frac{1}{24}\sum_{i=0}^{23} Vol_{norm,i}
]

* Where each (Vol_{norm,i}) is the **volume in month i normalized by the pre-generic average**.

This slide uses **interval conditions** on that value to classify each product/market:

* If (\text{Mean Erosion} \in [0, 0.25]) → **Bucket 1 (High erosion)**.
* If (\text{Mean Erosion} \in (0.25, 1]) → **Bucket 2 (Medium/Low erosion)**.

There are no new formulas drawn explicitly on the slide, but these interval notations are the **formal link** between the **visual curves** and the **numeric metric**.

---

In summary, this slide:

* Shows again the three erosion trajectories (high, medium, low),
* Groups them into two buckets (B1 and B2),
* And **defines those buckets precisely** via ranges of **Mean Erosion**, so that every real product/market can be assigned to a bucket based on its calculated erosion value.


---

## 1. Exact content of the slide

**Axes**

* Vertical axis label (y-axis): `Volume`
* Horizontal axis label (x-axis, bottom right): `Month_num`

Tick labels visible on x-axis:

* `-12`, `-8`, `-4`, `0`, `4`, `8`, `12`, `16`, `20`

**Vertical dotted line**

* Red vertical dotted line at x = 0 with red label above it:
  `Generics entry date`

**Legend (top-right)**

* Red line: `High Erosion`
* Yellow line: `Medium Erosion`
* Blue line: `Low Erosion`

**Bucket labels and texts on the right**

* Faded grey text near blue and yellow curves: `Bucket 2 (B2)`
* Faded grey box to the right of that: `Mean Erosion ∈ (0.25,1]`
* Red label near the red curve: `Bucket 1 (B1)`
* In bold red text to the right: `Datathon Focus!!!`
* Red box under that: `Mean Erosion ∈ [0, 0.25]`

**Logo and slide number**

* Top-right corner: `NOVARTIS`
* Bottom-right: slide number `8`

There are **no additional formulas** beyond the interval notation for Mean Erosion.

---

## 2. Detailed explanation of the slide

This slide reuses the same **generic-erosion curves** as the previous two slides but now highlights **which bucket is the main target of the Datathon**.

### 2.1 Axes and time reference

* **Y-axis – Volume**: indicates the **sales volume** of the original branded drug (e.g., prescriptions or units sold).
* **X-axis – Month_num**: indicates **time in months** relative to generic entry:

  * Negative months (`-12`, `-8`, `-4`) are **before** generics launch.
  * `0` marks the **Generics entry date**.
  * Positive months (`4`, `8`, `12`, `16`, `20`) are **after** generics have entered.

The red **vertical dotted line at 0**, labelled *Generics entry date*, marks the moment when generic competitors come to the market and erosion begins.

### 2.2 The three erosion profiles (curves)

The three coloured curves are the same conceptual patterns as before:

1. **High Erosion – red curve**

   * Grows slightly before month 0.
   * After generics enter, volume **drops almost vertically** and quickly reaches a very low level that remains almost flat.
   * Represents markets where the brand loses most of its sales almost immediately.

2. **Medium Erosion – yellow curve**

   * Highest and stable volume before month 0.
   * After generics, volume **falls sharply** but then declines more gradually over time.
   * Represents substantial, but not catastrophic, erosion.

3. **Low Erosion – blue curve**

   * Moderate and slightly declining volume before generics.
   * After generics, volume **declines slowly**, staying above the yellow and red curves in the long run.
   * Represents milder erosion, with the brand keeping a significant share.

On this slide, the **blue and yellow curves are partially faded (desaturated)** together with their bucket label, visually de-emphasizing them.

### 2.3 Buckets B1 and B2, with numerical thresholds

As before, the products/markets are grouped into **buckets** using the quantitative metric **Mean Erosion** (the Mean Generic Erosion defined earlier).

#### 2.3.1 Bucket 2 (B2) – de-emphasized

* The label `Bucket 2 (B2)` and the box `Mean Erosion ∈ (0.25,1]` are both in **light grey**, indicating this is **not the main focus**.
* `Mean Erosion ∈ (0.25,1]` means:

  * values **strictly greater than 0.25** and **up to 1** (0.25 excluded, 1 included).
  * These are **medium or low erosion** cases where, on average, more than 25% of pre-generic volume is retained over the 24-month horizon.

These correspond to the **medium (yellow)** and **low (blue)** erosion curves, now visually faded.

#### 2.3.2 Bucket 1 (B1) – highlighted focus

* `Bucket 1 (B1)` is written in **red**, next to the red high-erosion curve.
* Under “Datathon Focus!!!” there is a red box with:
  `Mean Erosion ∈ [0, 0.25]`.

This interval `[0, 0.25]` means:

* Mean Erosion can be **0 or any positive value up to 0.25**, both endpoints included.
* Since Mean Erosion is the **average normalized post-generic volume**:

  * A value close to 0 → almost **complete loss of sales** after generics.
  * A value of 0.25 → the product retains **only 25%** of its pre-generic volume on average.

So **Bucket 1** represents **high-erosion markets**, exactly like the red curve: a sharp collapse in volume after generics enter.

### 2.4 Datathon focus

The key message is in the red annotation:

* **“Datathon Focus!!!”** next to Bucket 1 and its Mean Erosion range.

This clearly states that:

* The Datathon analysis, modelling, and predictions should **concentrate on high-erosion cases (Bucket 1)**.
* Participants are particularly interested in **understanding or predicting when Mean Erosion will be in the [0, 0.25] range**:

  * which markets/products behave this way,
  * which factors drive such severe erosion,
  * and how to act strategically when such patterns are likely.

### 2.5 Formulas and notation

There are **no explicit algebraic formulas** on this slide, but there is important **interval notation**:

* `Mean Erosion ∈ (0.25,1]` for B2.
* `Mean Erosion ∈ [0, 0.25]` for B1.

These intervals connect the **visual time-series curves** to the **numerical Mean Erosion metric** previously defined as:

[
\text{Mean Erosion} = \frac{1}{24}\sum_{i=0}^{23} Vol_{norm,i}
]

The slide uses these ranges to show that **the Datathon is specifically about the most severe erosion scenarios (Bucket 1)**, while Bucket 2 is background context.



## 1. Exact content of the slide

**Title**

* `Datathon Challenge`

---

**1. Data Science Challenge**

`Participants are asked to forecast the volume erosion following generic entry over a 24-month horizon from the generic entry date. Forecasting should be performed at two distinct time points:`

* `Scenario 1: Right after the generic entry date: No actuals post generic entry. Forecast from month 0 to month 23.`
* `Scenario 2: 6 months after the generic entry date: Forecast from month 6 to month 23.`

---

**2. Business Challenge**

`All the teams that present in front of the Jury will be asked to provide a deep exploratory analysis on the preprocess carried out with focus on the high-erosion cases. We encourage the participants to use visualization tools.`

---

Top-right: `NOVARTIS` logo
Bottom-right: slide number `9`

There are **no plots or mathematical formulas** on this slide.

---

## 2. Detailed explanation of the slide

This slide formally describes **what participants must do in the Datathon**, separating the work into a **Data Science Challenge** and a **Business Challenge**.

---

### 2.1 Data Science Challenge

The core modelling task is:

> “forecast the volume erosion following generic entry over a 24-month horizon from the generic entry date.”

* **Volume erosion**: decline in sales volume of the original branded drug after generic competitors enter the market.
* **24-month horizon**: predictions must cover **two full years** after generic entry.
* **From the generic entry date**: time origin is the **month when generics appear** (month 0 on previous plots).

The forecasts must be produced at **two different decision times** (two “vantage points”):

#### Scenario 1 – Right after generic entry

> “Right after the generic entry date: No actuals post generic entry. Forecast from month 0 to month 23.”

* You stand at **month 0**, immediately when generics launch.
* You **do not have any observed data after generic entry**; all post-generic months are unknown.
* You must forecast the entire post-generic path:

  * months 0, 1, 2, …, 23.
* This is the **hardest forecasting setting**, pure ex-ante prediction based only on historical pre-generic data and any static features (country, molecule, competition, etc.).

#### Scenario 2 – Six months after generic entry

> “6 months after the generic entry date: Forecast from month 6 to month 23.”

* Now you stand at **month 6** after generics entered.
* You **do have 6 months of realized post-generic volumes** (months 0–5).
* You must forecast only the **remaining horizon**:

  * months 6, 7, …, 23.
* This scenario reflects a **mid-course update**: you can use the early erosion pattern (first six months) to refine your forecast for the rest of the 2-year period.

In both scenarios, the target of the model is essentially the **future sales time series after generic entry** (from 0–23 or 6–23). From these forecasts Novartis can later compute metrics like **Mean Generic Erosion** and classify markets into high- vs medium/low-erosion buckets.

---

### 2.2 Business Challenge

The second part concerns **interpretation and insight**, not only prediction:

> “All the teams that present in front of the Jury will be asked to provide a deep exploratory analysis on the preprocess carried out with focus on the high-erosion cases. We encourage the participants to use visualization tools.”

Key points:

* **Deep exploratory analysis**:

  * Go beyond running a model; thoroughly explore the dataset:

    * distributions, correlations, trends over time,
    * differences across countries, molecules, therapy areas, etc.
  * Show what you learned about **generic erosion behaviour**.

* **On the preprocess carried out**:

  * Explain and justify how you cleaned and transformed the data:

    * handling missing values,
    * outlier treatment,
    * normalization / scaling,
    * feature engineering (e.g., lag variables, competitor features, price variables).
  * The jury wants to see that your preprocessing choices are **transparent and sound**.

* **Focus on the high-erosion cases**:

  * Connect back to **Bucket 1 (Mean Erosion ∈ [0, 0.25])** from previous slides.
  * Analyse what characterizes those markets where volume collapses quickly:

    * Are there specific countries, therapeutic areas, payer types, competitive situations?
    * How do they differ from medium/low-erosion markets?

* **Use of visualization tools**:

  * Teams are encouraged to present:

    * time-series plots,
    * comparative charts between buckets,
    * feature importance plots,
    * geographic or categorical breakdowns.
  * Visuals should make your conclusions **intuitive and persuasive** for the jury.

---

### 2.3 Overall interpretation

In summary, this slide defines:

* **Modelling task**: build forecasting models of post-generic volume for a 24-month horizon, under two information scenarios (immediately at LoE and 6 months later).
* **Analytical task**: conduct and present a **business-oriented exploratory analysis**, especially on severe/high-erosion cases, clearly explaining data preprocessing and insights, ideally supported by strong visualizations.

There are **no formulas** here; the slide is purely about **requirements and expectations** for participants in the Datathon.



## 1. Exact content of the slide

**Title**

* `Datathon Winner Selection Process`

**Intro sentence**

* `The winner selection will take place in two evaluation phases:`

---

**Phase 1 – Model Evaluation:** `Participants must submit volume predictions for the entire test dataset, which includes both Scenario 1 and Scenario 2 cases.`

* **Phase 1-a: Scenario 1 Evaluation**
  ▪ `All teams will be evaluated on Scenario 1 prediction accuracy.`
  ▪ `The top 10 teams with the lowest prediction error will advance to Phase 1-b.`

* **Phase 1-b: Scenario 2 Evaluation**
  ▪ `Only the top 10 teams from Scenario 1 will be evaluated on Scenario 2 prediction accuracy.`
  ▪ `Among these, the top 5 teams with the lowest Scenario 2 prediction error will advance to the Final Phase.`

---

**Phase 2 – Jury Evaluation**
▪ `The final 5 teams will present their methodology, insights, and conclusions to a Jury composed of both technical and business experts.`
▪ `After reviewing the presentations, the Jury will select the top 3 winning teams.`

Top-right: `NOVARTIS` logo
Bottom-right: slide number `10`

There are **no plots or mathematical formulas** on this slide.

---

## 2. Detailed explanation of the slide

This slide describes **how the Datathon winners are chosen**, in a structured two-phase process combining **quantitative model performance** and **qualitative evaluation** by a jury.

---

### 2.1 Overall structure

* The competition has **two evaluation phases**:

  1. **Phase 1 – Model Evaluation** (automatic, metric-based).
  2. **Phase 2 – Jury Evaluation** (presentation-based).

All teams must first submit **volume forecasts** (time-series predictions) for the **entire test dataset**, covering **both Scenario 1 and Scenario 2** defined earlier:

* Scenario 1: forecasting the full 24 months right after generic entry (no post-generic actuals).
* Scenario 2: forecasting months 6–23 after generic entry (having 6 months of post-generic history).

---

### 2.2 Phase 1 – Model Evaluation

This phase focuses purely on **prediction accuracy**.

#### Phase 1-a: Scenario 1 Evaluation

* **All teams** are evaluated in Scenario 1.
* For each team, the organizers compute a **prediction error** metric (e.g., RMSE, MAPE, or another chosen score) comparing the submitted forecasts to the true test data.
* Teams are **ranked** by this error: **lower error = better model**.
* The **top 10 teams with the lowest prediction error** in Scenario 1 proceed to Phase 1-b.

So Phase 1-a acts as a **first filter**, reducing the field to 10 teams based on their ability to **forecast erosion ex-ante**.

#### Phase 1-b: Scenario 2 Evaluation

* Only these **top 10 teams** are evaluated on **Scenario 2** (the mid-course-update scenario).
* Again, a prediction error metric is computed for Scenario 2 forecasts.
* Teams are ranked by Scenario 2 error, and the **top 5 teams with the lowest Scenario 2 prediction error** advance to the **Final Phase** (Phase 2).

This second filter ensures that finalists perform well in **both scenarios**, not only in one.

---

### 2.3 Phase 2 – Jury Evaluation

This phase evaluates the **quality of the overall solution**, not just the numerical accuracy.

* The **final 5 teams** (those that passed both Scenario 1 and Scenario 2 filters) must present:

  * their **methodology** (data preprocessing, feature engineering, modelling choices, validation strategy),
  * their **insights** (what they learned from the data, especially about high-erosion markets),
  * their **conclusions** (business implications, recommendations, limitations).
* The Jury is **mixed**: it includes both **technical experts** (e.g., data scientists, statisticians) and **business experts** (e.g., finance, commercial, market access professionals).
  This ensures that solutions are judged on:

  * technical robustness,
  * interpretability,
  * and **business relevance**.

After these presentations:

* The Jury deliberates and selects the **top 3 winning teams**, based on both:

  * **model performance** (from Phase 1), and
  * the **quality and clarity** of the Phase 2 presentations.

---

### 2.4 Key implications for participants

* You must deliver **strong predictive performance in both scenarios** to reach the final.
* You also need a **clear, well-structured narrative** around your approach and findings to convince the Jury.
* Good documentation, reproducibility, and meaningful visualizations will be critical in Phase 2.

No graphs or formulas are introduced here; the slide purely defines the **competition pipeline and selection rules**.



## 1. Exact content of the slide

**Title**

* `Challenge: Data Provided (1/2)`

**Main text**

* `Target Variable: Monthly volume for 2,293 country–brand combinations that experienced generic entry.`

---

* `Training Set (1,953 observations):`
  ▪ `Includes up to 24 months of volume data before generic entry`
  ▪ `And up to 24 months after generic entry`

---

* `Test Set (340 observations):`
  ▪ `Two forecasting scenarios are evaluated:`
   • `Scenario 1 (~2/3 of test set; 228 observations): Forecast required from Month 0 to Month 23`
   • `Scenario 2 (~1/3 of test set; 112 observations): Forecast required from Month 6 to Month 23`

Top-right: `NOVARTIS` logo
Bottom-right: slide number `11`

There are **no plots or formulas** on this slide.

---

## 2. Detailed explanation of the slide

This slide explains **what data you receive for the Datathon and how it is split into training and test sets**, plus how the test set is further divided between the two forecasting scenarios.

### 2.1 Target variable

> *“Target Variable: Monthly volume for 2,293 country–brand combinations that experienced generic entry.”*

* The **target** you must model and forecast is **monthly sales volume**.
* Each data row corresponds to a specific **country–brand combination**:

  * “Country” = market (e.g., Italy, Spain, Germany).
  * “Brand” = a specific pharmaceutical brand (original, not generic).
* There are **2,293 such combinations** in total, each having a time series of monthly volumes that covers:

  * some time **before** generics enter,
  * and some time **after** generics enter.

Only combinations that actually **experienced a generic entry** are included—this matters because the task is specifically about **post-generic erosion**.

---

### 2.2 Training set

> *“Training Set (1,953 observations)”*

Here, an “observation” means one **country–brand time series** used for training.

* The training set contains **1,953** of the 2,293 combinations.
* For each of these, you get:

  * **Up to 24 months of volume data before generic entry**
    → at most two years of pre-generic history (months −24 to −1 relative to the entry date).
  * **Up to 24 months after generic entry**
    → at most two years of post-generic data (months 0 to 23).

“Up to” indicates that some series may have fewer months available (e.g., shorter history or shorter follow-up), but 24 months is the maximum.

You use this training set to:

* explore and understand erosion patterns,
* design features (e.g., pre-generic trends, price gaps, competition),
* train your forecasting models for both scenarios.

---

### 2.3 Test set

> *“Test Set (340 observations): Two forecasting scenarios are evaluated…”*

* The remaining **340 country–brand combinations** form the **test set**.
* For these, you will receive partial information and must produce forecasts according to the scenario definitions.
* The test set is split between **Scenario 1** and **Scenario 2**:

#### Scenario 1 – around two-thirds of the test set

> *“Scenario 1 (~2/3 of test set; 228 observations): Forecast required from Month 0 to Month 23”*

* Contains **228** country–brand time series (≈ two-thirds of the 340).
* For each, you will have:

  * the pre-generic history,
  * **no actual volumes after generic entry**.
* Your task: **forecast all 24 months after generic entry** (month 0 to month 23).
* This matches the earlier description of Scenario 1: a **pure ex-ante** forecast when generics just entered.

#### Scenario 2 – around one-third of the test set

> *“Scenario 2 (~1/3 of test set; 112 observations): Forecast required from Month 6 to Month 23”*

* Contains **112** country–brand time series (≈ one-third of the 340).
* For each, you will know:

  * the pre-generic period,
  * plus the **first 6 months after generic entry** (months 0–5).
* Your task: **forecast months 6 through 23**.
* This is the **mid-course-update scenario**, where the early erosion data can be used to refine predictions.

---

### 2.4 What this means for modelling

* You effectively have **almost 2,000 full post-generic trajectories** for training and validation.
* You must design models that work in **two information settings**:

  1. No post-generic data available (Scenario 1).
  2. Six months of post-generic data available (Scenario 2).
* Performance will be assessed on the **340 unseen combinations**, using:

  * 228 for Scenario 1 and
  * 112 for Scenario 2.

No graphs or equations are introduced; the slide only gives **dataset size, structure, and forecast requirements**.



## 1. Exact content of the slide

**Title**

* `Challenge: Data Provided (2/2)`

---

**Left-hand text**

* `Erosion Buckets in Test Set:`

* `The 340 test observations are distributed in the following way across the scenarios and erosion levels:`

  * `0–0.25 (Bucket 1, B1: High erosion)`
  * `0.25–1 (Bucket 2, B2: Low erosion)`

* `This structure holds in both forecasting scenarios (scenario 1 and scenario 2).`

---

**Right-hand chart**

Title above chart:

* `Train and Test Scenario Distribution`

Y-axis label:

* `Number of Observations`

X-axis categories (from left to right):

* `Train`
* `Test - Scenario 1`
* `Test - Scenario 2`

Bars:

* Over the `Train` category: one tall grey bar with the value label `1953`.
* Over `Test - Scenario 1`: a smaller stacked bar with the total value label `228`.
* Over `Test - Scenario 2`: another stacked bar with the total value label `112`.

Legend (top-right of the chart):

* Grey: `Train`
* Red: `B1 - High erosion`
* Dark blue: `B2 - Low erosion`

---

Top-right of slide: `NOVARTIS` logo
Bottom-right: slide number `12`

There are **no formulas** on this slide.

---

## 2. Detailed explanation of the slide

This slide explains **how the 340 test observations are distributed across erosion buckets and scenarios**, and it visualizes the overall split between training and test data.

### 2.1 Erosion buckets in the test set

The text defines two **erosion buckets** based on the **Mean Erosion** metric introduced earlier:

1. **0–0.25 (Bucket 1, B1: High erosion)**

   * Mean Erosion in the interval **[0, 0.25]** (the slide writes “0–0.25” in shorthand).
   * These are **high-erosion cases** where, on average, post-generic volume is at most 25% of the pre-generic level.

2. **0.25–1 (Bucket 2, B2: Low erosion)**

   * Mean Erosion in the interval **(0.25, 1]**, abbreviated as “0.25–1”.
   * These are **medium/low-erosion cases**, where more than 25% of pre-generic volume is retained on average.

The slide states:

> *“The 340 test observations are distributed in the following way across the scenarios and erosion levels.”*

and then:

> *“This structure holds in both forecasting scenarios (scenario 1 and scenario 2).”*

This means:

* In both Scenario 1 (forecast months 0–23) and Scenario 2 (forecast months 6–23), the test set contains **both** B1 (high-erosion) and B2 (low-erosion) cases.
* The proportion or logic of how B1 and B2 are included is **consistent between the two scenarios**, so you do not have, for example, one scenario containing only low-erosion cases.

From a modelling point of view, you must ensure your approach handles **both kinds of erosion dynamics** within each scenario.

---

### 2.2 Bar chart: Train and test scenario distribution

The bar chart on the right provides a **visual summary** of how many observations are in the training set and each test scenario, and how the test observations are split between B1 and B2.

#### Axes and labels

* The **x-axis** shows three groups:

  * `Train`
  * `Test - Scenario 1`
  * `Test - Scenario 2`

* The **y-axis** is labelled `Number of Observations` and runs from 0 up to a value slightly above 2000.

This makes it easy to compare data volumes at a glance.

#### Bars and counts

1. **Train bar (grey, labelled 1953)**

   * Represents the **1,953 observations** in the training set (as stated on the previous slide).
   * Shown as a single grey bar since **all training observations** are used to fit models, independent of bucket.

2. **Test - Scenario 1 bar (stacked, total label 228)**

   * Represents the **228 test observations** used for Scenario 1.
   * The bar is stacked with:

     * a **red segment** for **B1 – High erosion**,
     * a **blue segment** for **B2 – Low erosion**.
   * The exact numeric split between red and blue is not written on the slide, but the stacking shows that Scenario 1 includes **both high- and low-erosion cases**.

3. **Test - Scenario 2 bar (stacked, total label 112)**

   * Represents the **112 test observations** used for Scenario 2.
   * Again, stacked into:

     * a red segment (high erosion, B1),
     * a blue segment (low erosion, B2).
   * This visually confirms that Scenario 2 also contains both erosion levels.

#### Legend

* **“Train” (grey)**: the grey bar is only used in the training set column.
* **“B1 - High erosion” (red)**: identifies red portions of the test bars as high-erosion bucket.
* **“B2 - Low erosion” (blue)**: identifies blue portions as low-erosion bucket.

The plot shows that:

* You have **much more training data (1953)** than test data (340), which is typical for modelling competitions.
* Both **test scenarios** are **mixtures of high and low erosion** markets, which means any model evaluated on these scenarios must be robust across **different erosion profiles**.

---

### 2.3 Implications for participants

* When designing models, you must consider that during evaluation the test set will include **both B1 and B2** in each scenario. A model that works only for slow erosion or only for high erosion will not perform well overall.
* The relatively large training set suggests you can:

  * build more complex models (time-series, panel models, ML models) and still have sufficient data,
  * perform stratified validation (e.g., separate validation for B1 vs B2) to ensure good generalization.
* Because the bucket structure is the same for both scenarios, you can use a common modelling strategy and then **specialize or fine-tune** it per scenario if needed.

There are no formulas on this slide; the focus is on **dataset structure and bucket distribution**, both in text and via the bar chart.


## 1. Exact content of the slide

**Title**

* `Data available: Volume`

---

**Main text**

* `df_volume.csv: includes information regarding the volume of sales before and after the generic entry:`

  * `country: country name`
  * `brand_name: brand name`
  * `month: name of the month`
  * `months_postgx: number of month after generic entry. Month_postgx equal to 0 denotes the generics entry. Negative values refer to months before the generics entry (eg. Month_postgx = -3 denotes three months before the generics entry)`
  * `volume: number of drugs sold`  (the word *volume* is underlined and in red)

---

**Example table (screenshot)**

A small table with columns:

* `country` | `brand_name` | `month` | `months_postgx` | `volume`

Sample rows shown:

1. `COUNTRY_B6AE` | `BRAND_1C1E` | `Jul` | `-24` | `272594.39`
2. `COUNTRY_B6AE` | `BRAND_1C1E` | `Aug` | `-23` | `351859.31`
3. `COUNTRY_B6AE` | `BRAND_1C1E` | `Sep` | `-22` | `447953.48`
4. `COUNTRY_B6AE` | `BRAND_1C1E` | `Oct` | `-21` | `411543.29`
5. `COUNTRY_B6AE` | `BRAND_1C1E` | `Nov` | `-20` | `774594.45`
6. `COUNTRY_B6AE` | `BRAND_1C1E` | `Dec` | `-19` | `442279.18`
7. `COUNTRY_B6AE` | `BRAND_1C1E` | `Jan` | `-18` | `485069.49`
8. `COUNTRY_B6AE` | `BRAND_1C1E` | `Feb` | `-17` | `549902.7`

Top-right: `NOVARTIS` logo
Bottom-right: slide number `13`

There are **no formulas** on this slide.

---

## 2. Detailed explanation of the slide

This slide describes the **main time-series file** provided to participants: `df_volume.csv`. It contains **monthly sales volumes** around the time of generic entry, at the **country–brand level**.

### 2.1 File purpose

`df_volume.csv` is the **core dataset** you will use to:

* reconstruct the **volume trajectory** of each brand in each country,
* identify the **pre-generic** and **post-generic** periods,
* feed time-series models or panel models for forecasting.

Each row in the file corresponds to **one month** for **one (country, brand)** pair.

### 2.2 Column-by-column explanation

1. **`country` – country name**

   * Encodes the market, often with an anonymized ID like `COUNTRY_B6AE`.
   * Multiple rows will share the same `country` if they refer to different brands or different months in that country.

2. **`brand_name` – brand name**

   * Encodes the original brand, again as an anonymized identifier such as `BRAND_1C1E`.
   * For a given `(country, brand_name)` combination you will have a whole sequence of monthly rows.

3. **`month` – name of the month**

   * A textual label such as `Jul`, `Aug`, `Sep`, etc.
   * Mainly for readability; the true time ordering is driven by `months_postgx`.

4. **`months_postgx` – number of month after generic entry**

   * This is the **relative time index** with respect to the **generic entry date (Gx)**.
   * By definition:

     * `months_postgx = 0` → the **month of generic entry**.
     * **Negative values** → months **before** generic entry.
       Example from the slide text: `Month_postgx = -3` means *three months before generics entry*.
     * **Positive values** → months **after** generic entry (not shown in the snippet, but present in the full file).
   * In the sample table, values go from `-24` up to `-17`, showing **24 to 17 months before** generics entry.

   This field is crucial for:

   * splitting data into **pre-generic** and **post-generic** parts,
   * aligning different brands and countries on a common time axis (so “month +6” always means 6 months after generics, regardless of calendar date).

5. **`volume` – number of drugs sold**

   * The **target variable**: the quantity of product sold in that month (e.g., packs, units, scripts).
   * In the example rows it takes values like `272594.39`, `351859.31`, etc.
   * You will use this column:

     * to compute the **Mean Generic Erosion** (after normalization),
     * to train models that forecast future volumes for months 0–23 (Scenario 1) or 6–23 (Scenario 2).

### 2.3 Example rows

The small table illustrates a **single country–brand pair**:

* `COUNTRY_B6AE` – `BRAND_1C1E`

For this pair, you see monthly data:

* From **24 months before generic entry** (`months_postgx = -24`) in `Jul`
* Up to **17 months before generic entry** (`months_postgx = -17`) in `Feb`.

Volumes fluctuate (e.g., from ~272k to ~550k), showing that:

* even before generics arrive, the brand’s sales are **not constant**,
* your models must account for **existing trends and seasonality** in the pre-generic period.

### 2.4 How you will use this file

From `df_volume.csv` you can:

* Aggregate or pivot by `(country, brand_name)` to build **time-series arrays**,
* Separate **training vs test** sets by merging with metadata (which series belong to which split),
* Derive features:

  * pre-generic growth rate,
  * seasonality patterns,
  * normalized volumes (`VolNorm_i`),
* Serve as input to your **forecasting model** whose output will be evaluated in the Datathon.

There are no mathematical formulas on this slide; it is purely a **schema description plus a data example** to clarify how the volume file is structured.



## 1. Exact content of the slide

**Title**

* `Data available: Generics features`

---

**Main text**

* `df_generics.csv: includes information about the country, the drug and the number of generics existing for that specific brand in that country:`

  * `country: country name`
  * `brand_name: brand name`
  * `months_postgx: number of months after generic entry. Month_postgx equal to 0 denotes the generics entry`
  * `n_gxs: number of generics. Note that the number of generics might vary along time`

---

**Example table (screenshot)**

Columns:

* `country` | `brand_name` | `months_postgx` | `n_gxs`

Rows shown:

1. `COUNTRY_B6AE` | `BRAND_DF2E` | `0` | `0.0`
2. `COUNTRY_B6AE` | `BRAND_DF2E` | `1` | `0.0`
3. `COUNTRY_B6AE` | `BRAND_DF2E` | `2` | `1.0`
4. `COUNTRY_B6AE` | `BRAND_DF2E` | `3` | `2.0`
5. `COUNTRY_B6AE` | `BRAND_DF2E` | `4` | `2.0`
6. `COUNTRY_B6AE` | `BRAND_DF2E` | `5` | `2.0`
7. `COUNTRY_B6AE` | `BRAND_DF2E` | `6` | `2.0`
8. `COUNTRY_B6AE` | `BRAND_DF2E` | `7` | `2.0`

Top-right: `NOVARTIS` logo
Bottom-right: slide number `14`

There are **no formulas** on this slide.

---

## 2. Detailed explanation of the slide

This slide describes the second key file provided to participants: **`df_generics.csv`**, which contains **time-varying information on generic competitors** for each brand and country.

### 2.1 Purpose of `df_generics.csv`

The goal of this file is to let you quantify **competitive pressure from generics** over time, for each original brand in each country. Since generic entry and the number of competing generics are major drivers of volume erosion, these variables are powerful **features** for your forecasting models.

Each row represents **one month for one (country, brand)** pair after generics have entered.

### 2.2 Column-by-column explanation

1. **`country` – country name**

   * Identifies the market, in anonymised form (e.g., `COUNTRY_B6AE`).
   * Same coding as in `df_volume.csv`, so you can join the two files on `(country, brand_name, months_postgx)`.

2. **`brand_name` – brand name**

   * Identifies the original brand (again anonymised, e.g., `BRAND_DF2E`).
   * For each `(country, brand_name)` combination, you will have a monthly series describing how many generics are present.

3. **`months_postgx` – number of months after generic entry**

   * This is the same relative time index used before, but **only for months at or after generic entry**:

     * `months_postgx = 0` → **month of generic entry**.
     * `months_postgx = 1` → one month after entry, and so on.
   * There are no negative values here because the file focuses specifically on **post-generic** competition.

4. **`n_gxs` – number of generics**

   * Indicates **how many generic products** are on the market at that time for the given brand in that country.
   * The note *“the number of generics might vary along time”* means:

     * New generic competitors can enter later,
     * some may exit, so the number can increase or (less likely) decrease over time.
   * This makes `n_gxs` a **time-varying feature**, not just a static property.

### 2.3 Example table interpretation

The sample table shows one `(country, brand)` pair:

* `COUNTRY_B6AE` – `BRAND_DF2E`

For this pair:

* At `months_postgx = 0` and `1`, `n_gxs = 0.0`
  → Despite the theoretical generic entry point, there are **no active generic competitors** in the first two months (e.g., regulatory or commercial delays).

* At `months_postgx = 2`, `n_gxs = 1.0`
  → The **first generic** appears two months after entry.

* At `months_postgx = 3`, `n_gxs = 2.0`
  → A **second generic** joins, increasing competition.

* From `months_postgx = 4` to `7`, `n_gxs = 2.0`
  → Two generics remain on the market for at least these months.

This demonstrates how the **intensity of generic competition ramps up** over time, which likely correlates with accelerated volume erosion for the original brand.

### 2.4 How to use these features

In modelling:

* Join `df_generics.csv` with `df_volume.csv` on `(country, brand_name, months_postgx)` to attach `n_gxs` to the corresponding monthly volumes.
* Use `n_gxs` directly or derive additional variables, for example:

  * indicators like “any generics present” (n_gxs > 0),
  * cumulative number of months since first generic appeared,
  * change in `n_gxs` from previous month.

These features help the model learn **how the number and timing of generics influence erosion patterns**, especially for the high-erosion Bucket 1 that is the focus of the Datathon.

There are no charts or formulas here; the slide purely defines the **schema and meaning** of the generics-features file and illustrates it with a small data snippet.



## 1. Exact content of the slide

**Title**

* `Data available: Drug-related features`

---

**Main text**

* `df_medicine_info.csv: includes information regarding each drug and administration within a country:`

  * `country: country name`
  * `brand_name: brand name`
  * `ther_area: refers to the drugs' therapeutical area`
  * `hospital_rate: percentage of the drug being delivered in hospitals`
  * `main_package: most common format in which the drug is dispensed (eg. PILL)`
  * `biological: boolean indicating whether the drug is derived from a living organism (eg. proteins, antibodies, nucleic acids)`
  * `small_molecule: boolean indicating whether the drug is a low molecular weight compound (typically synthesized chemically)`

---

**Example table (screenshot)**

Columns:

* `country` | `brand_name` | `ther_area` | `hospital_rate` | `main_package` | `biological` | `small_molecule`

Sample rows (as displayed):

1. `COUNTRY_0024` | `BRAND_1143` | `Sensory_organs`            | `0.09`  | `EYE DROP` | `False` | `True`
2. `COUNTRY_0024` | `BRAND_1865` | `Muscoskeletal_Rheu...`     | `92.36` | `INJECTION`| `False` | `False`
3. `COUNTRY_0024` | `BRAND_240F` | `Antineoplastic_and...`     | `36.94` | `PILL`     | `False` | `True`
4. `COUNTRY_0024` | `BRAND_2F6C` | `Antineoplastic_and...`     | `0.01`  | `INJECTION`| `True`  | `False`
5. `COUNTRY_0024` | `BRAND_3A67` | `Nervous_system`            | `nan`   | `PILL`     | `False` | `False`
6. `COUNTRY_0024` | `BRAND_3CB9` | `Antineoplastic_and...`     | `1.42`  | `PILL`     | `False` | `True`
7. `COUNTRY_0024` | `BRAND_3E0C` | `Antineoplastic_and...`     | `47.06` | `INJECTION`| `True`  | `False`
8. `COUNTRY_0024` | `BRAND_41B7` | `Nervous_system`            | `0.02`  | `PILL`     | `False` | `True`

(Where the `...` indicates text truncated in the screenshot.)

Top-right: `NOVARTIS` logo
Bottom-right: slide number `15`

There are **no formulas or plots** beyond this example table.

---

## 2. Detailed explanation of the slide

This slide describes the **drug-level metadata file** `df_medicine_info.csv`. These are **static features** for each `(country, brand)` combination, intended to enrich your forecasting models with contextual information about the drug itself and how it is used.

### 2.1 Purpose of `df_medicine_info.csv`

* Each row corresponds to a **specific brand in a specific country**.
* The file tells you:

  * **what kind of drug** it is (therapeutic area, biological vs small molecule),
  * **how it is administered/packaged**,
  * **where it is mainly used** (hospital vs non-hospital).

These characteristics can strongly influence **how erosion behaves** after generics appear. For example, hospital-only injectable oncology drugs may erode differently from retail oral pills for chronic diseases.

### 2.2 Column-by-column explanation

1. **`country` – country name**

   * Same anonymised coding as in the other files (`COUNTRY_0024`, etc.).
   * Use it to join with `df_volume.csv` and `df_generics.csv` on `(country, brand_name)`.

2. **`brand_name` – brand name**

   * Identifies the original drug brand (`BRAND_1143`, `BRAND_3E0C`, etc.).
   * Combined with `country`, it defines the unit for which you forecast volumes.

3. **`ther_area` – therapeutic area**

   * Indicates the **therapeutic class** or **system** the drug targets, e.g.:

     * `Sensory_organs`
     * `Muscoskeletal_Rheu...` (musculoskeletal / rheumatology)
     * `Antineoplastic_and...` (antineoplastic / cancer-related)
     * `Nervous_system`
   * This feature captures **clinical indication** and can correlate with:

     * chronic vs acute use,
     * hospital vs outpatient use,
     * reimbursement rules,
     * typical erosion patterns.

4. **`hospital_rate` – percentage of drug delivered in hospitals**

   * Numeric value (0–100) representing **what proportion of the product’s volume is dispensed in hospital settings**.
   * Example values:

     * `0.09` (almost entirely retail),
     * `92.36` (predominantly hospital),
     * `nan` (missing).
   * Drugs with high hospital_rate are often subject to **tenders and formulary decisions**, which can cause stepwise erosion rather than smooth curves.

5. **`main_package` – most common dosage/dispensing format**

   * Categorical description of **how the drug is usually packaged/administered**, e.g.:

     * `PILL`
     * `INJECTION`
     * `EYE DROP`
   * Package type affects:

     * patient convenience and adherence,
     * the number and type of generic competitors,
     * price differentials.
   * It may also indicate **route of administration** (oral vs parenteral vs topical), which indirectly influences erosion.

6. **`biological` – is the drug a biologic? (boolean)**

   * `True` if the drug is **derived from a living organism or its components**, such as:

     * proteins,
     * monoclonal antibodies,
     * nucleic acids.
   * `False` otherwise.
   * Biologics generally follow different regulatory pathways and may face **biosimilar** rather than classic generic competition, with **often slower or more complex erosion**.

7. **`small_molecule` – is the drug a small molecule? (boolean)**

   * `True` if the drug is a **low molecular weight compound**, typically **chemically synthesized**.
   * `False` otherwise (often when `biological` is `True`).
   * Small molecules tend to have **many inexpensive generics**, leading to **faster and deeper erosion**.

In many cases, `biological` and `small_molecule` will be **mutually exclusive**, but the dataset encodes them as two separate booleans so you can treat them individually or combine them in features.

### 2.3 Example rows interpretation

Looking at the snapshot:

* `BRAND_1143` in `COUNTRY_0024`

  * `ther_area = Sensory_organs`
  * `hospital_rate = 0.09` → mostly dispensed outside hospitals.
  * `main_package = EYE DROP`
  * `biological = False`, `small_molecule = True`
  * Likely a **classic small-molecule eye drop**, with many potential generics, possibly high erosion.

* `BRAND_1865`

  * `ther_area = Muscoskeletal_Rheu...`
  * `hospital_rate = 92.36` → largely hospital-based.
  * `main_package = INJECTION`
  * `biological = False`, `small_molecule = False` (interesting case, may be coded differently or a complex molecule).
  * This suggests a **hospital injectable for musculoskeletal/rheumatology**, which may face tender-driven price/volume changes.

* `BRAND_2F6C`

  * `ther_area = Antineoplastic_and...` (oncology/antineoplastic)
  * `hospital_rate = 0.01` (almost zero, possibly retail oncology or data quirk)
  * `main_package = INJECTION`
  * `biological = True`, `small_molecule = False`
  * A **biologic oncology injection**, where erosion may be slower and shaped by biosimilar entry.

The presence of `nan` in `hospital_rate` demonstrates that **missing values** exist and must be handled during preprocessing (imputation, flags, or exclusion).

### 2.4 How to use these features in modelling

These drug-related attributes enable:

* **Stratified analysis**: comparing erosion patterns by therapeutic area, hospital_rate, or biologic vs small molecule.
* **Feature engineering**:

  * one-hot encode `ther_area` and `main_package`,
  * include `hospital_rate` and its interactions (e.g., hospital_rate × n_gxs),
  * binary indicators for `biological` and `small_molecule`.
* **Better generalization** to unseen `(country, brand)` pairs, because the model can leverage drug characteristics, not just raw volume history.

There are no mathematical formulas on this slide; its role is to **document the schema and meaning** of drug-level metadata and to show a concrete example to clarify how values look in practice.



## 1. Exact content of the slide

**Title**

* `Metric: Prediction Error (Phase 1-a)`

**Body text**

`In this first scenario (Scenario 1), participants will provide predictions without knowing any actual data after the generic entry date. To compute the prediction error for this phase (Phase 1-a), we will evaluate the difference between the predicted values and the actual volumes in four different ways, weighted as follows:`

1. `Absolute monthly error of all 24 months (20%)`
2. `Absolute accumulated error of months 0 to 5 (50%)`
3. `Absolute accumulated error of months 6 to 11 (20%)`
4. `Absolute accumulated error of months 12 to 23 (10%)`

`All the 4 items will be normalized by the average (Avg_j) monthly volume of the last 12 months before the generic entry to consider the magnitude of the brand.`

Top-right: `NOVARTIS` logo
Bottom-right: slide number `16`

There are **no plots** and no explicit formula expressions on this slide (only the textual reference to (Avg_j).

---

## 2. Detailed explanation of the slide

This slide defines **how prediction error is computed in Phase 1-a**, i.e. **Scenario 1** of the Datathon, where you forecast the entire 24 months after generic entry *without seeing any post-generic actuals*.

### 2.1 Context: Scenario 1

* In **Scenario 1**, at the time of prediction you know:

  * all pre-generic volumes,
  * zero post-generic volumes.
* You must forecast months **0 to 23** after generic entry.
* Once the organizers know the true values, they compute **prediction error**.

### 2.2 Four components of the error

The slide states that the **difference between predicted and actual volumes** is evaluated in **four different ways**, each with its own weight in the final score.

Think of it as four “sub-metrics”:

1. **Absolute monthly error of all 24 months (20%)**

   * For each of the 24 months (0–23), compute the **absolute error**:
     (|\text{prediction}_t - \text{actual}_t|).
   * Aggregate these errors across all months (e.g., sum or mean; the exact aggregation is defined in the challenge documentation).
   * This captures **overall month-by-month accuracy** across the full horizon.
   * Its contribution to the final score is **20%**.

2. **Absolute accumulated error of months 0 to 5 (50%)**

   * First 6 months after generic entry are critical.
   * Compute the **total actual volume** in months 0–5 and the **total predicted volume** in months 0–5.
   * Take the **absolute difference** between these two totals → “absolute accumulated error” for that period.
   * This emphasizes **getting the early post-entry level and shape right** (the steep drop region).
   * This component has the **largest weight: 50%**, reflecting business importance of early months.

3. **Absolute accumulated error of months 6 to 11 (20%)**

   * Do the same accumulated-volume comparison for months 6–11.
   * Measures how well your model captures **medium-term erosion** (months 6–11).
   * Weight: **20%**.

4. **Absolute accumulated error of months 12 to 23 (10%)**

   * Compute accumulated predicted vs actual volumes over months 12–23 (second year).
   * Focuses on **long-term tail behaviour** of the erosion curve.
   * Weight: **10%**, the lowest, because errors far in the future are less critical.

In summary:

* 20% weight: detailed **monthly shape** over 24 months.
* 50% + 20% + 10% = 80% weight: **three block-wise accumulated errors** (0–5, 6–11, 12–23), with emphasis on the **first 6 months**.

This design balances:

* local, month-level fit (component 1),
* and business-relevant cumulative sales accuracy in key periods (components 2–4).

### 2.3 Normalization with (Avg_j)

The final paragraph:

> “All the 4 items will be normalized by the average (Avg_j) monthly volume of the last 12 months before the generic entry to consider the magnitude of the brand.”

* (Avg_j) is the **average monthly pre-generic volume** for brand (j), computed over the **12 months before generic entry** (as defined earlier in the Mean Erosion slide).
* Each of the four error components (monthly and accumulated) is **divided by (Avg_j)**.

Why?

* This **scales errors relative to the brand’s typical size**.
* Without normalization:

  * A 10,000-unit error for a huge brand selling 500,000 units/month is relatively minor,
  * but the same 10,000-unit error for a small brand selling 20,000 units/month is huge.
* By dividing by (Avg_j):

  * Errors are expressed roughly as **“multiples of a typical month’s volume”**,
  * making all brands **comparable** regardless of absolute size.

So the overall prediction error for a country–brand in Scenario 1 is a **weighted combination** of four **normalized absolute errors** (one monthly-level, three cumulative-period-level). The lower this combined score, the **better** your model—this is what Phase 1-a uses to rank teams.


## 1. Exact content of the slide

**Title**

* `Metric: Prediction Error (Phase 1-a)`

---

**Left-hand chart**

Top of chart:

* `Accumulated Error`

Three coloured vertical bands with headings:

* Over the pink band:
  `Month 0 to 5`
  `(50%)`
* Over the light-blue band:
  `Month 6 to 11`
  `(20%)`
* Over the light-yellow band:
  `Month 12 to 23`
  `(10%)`

Y-axis label on the left:

* `Volume`

Near the red vertical dashed line:

* `Generics`
  `entry date`

Legend at bottom-left of the plot:

* Blue line: `Bucket 2`
* Yellow line: `Bucket 1`

X-axis tick labels under the plot: `-12`, `-8`, `-4`, `0`, `4`, `8`, `12`, `16`, `20`

To the right of the x-axis:

* `Month_num`

Below the plot, in a grey box:

* `Month 0 to 23 (20%)`
* `Monthly Error`

---

**Formula block on the right**

Main equation:

[
PE_j = 0.2
\left(
\frac{\sum_{i=0}^{23} \left| Y^{act}*{j,i} - Y^{pred}*{j,i} \right|}
{24 \cdot Avg_j}
\right)

* 0.5
  \left(
  \frac{\left|\sum_{i=0}^{5} Y^{act}*{j,i} - \sum*{i=0}^{5} Y^{pred}_{j,i}\right|}
  {6 \cdot Avg_j}
  \right)
* 0.2
  \left(
  \frac{\left|\sum_{i=6}^{11} Y^{act}*{j,i} - \sum*{i=6}^{11} Y^{pred}_{j,i}\right|}
  {6 \cdot Avg_j}
  \right)
* 0.1
  \left(
  \frac{\left|\sum_{i=12}^{23} Y^{act}*{j,i} - \sum*{i=12}^{23} Y^{pred}_{j,i}\right|}
  {12 \cdot Avg_j}
  \right)
  ]

Each of the four terms is drawn in a coloured box:

* Grey box: the first term with weight `0.2`
* Pink box: the second term with weight `0.5`
* Light-blue box: the third term with weight `0.2`
* Light-yellow box: the fourth term with weight `0.1`

Top-right of slide: `NOVARTIS` logo
Bottom-right: slide number `17`

---

## 2. Detailed explanation of the slide

This slide *visualizes and formalizes* the **prediction error metric** for **Phase 1-a (Scenario 1)**.

### 2.1 Left-hand plot – where the errors come from

The graph on the left is a schematic of **volume over time** for two typical erosion patterns:

* **Bucket 1 (yellow line)** – high erosion: volume drops steeply right after generics enter.
* **Bucket 2 (blue line)** – low/medium erosion: volume declines more gradually.

Key elements:

* The **x-axis** is `Month_num`, measured relative to the **generic entry date**:

  * Negative months (−12, −8, −4) are **before** generics.
  * `0` is the **generics entry date** (vertical red dashed line labelled “Generics entry date”).
  * Positive months (4, 8, 12, 16, 20) are **after** generics enter.

* The **y-axis** (`Volume`) represents the brand’s monthly sales volume.

* The post-generic period (months 0–23) is divided into:

  * **Month 0 to 5 (50%)** – pink area
    The first 6 months post-entry, where erosion is usually fastest and most critical.
  * **Month 6 to 11 (20%)** – light-blue area
    The following 6 months, consolidating the erosion pattern.
  * **Month 12 to 23 (10%)** – light-yellow area
    The second year, where volumes generally stabilise at a lower level.

* At the bottom, a grey bar labelled **“Month 0 to 23 (20%) – Monthly Error”** indicates that there is also a **month-by-month error term** covering the whole 24-month horizon (weight 20%).

So, visually, the slide maps **time regions** onto the **four components** of the metric:

* Full 0–23 months → monthly error (20%)
* 0–5 months → accumulated error (50%)
* 6–11 months → accumulated error (20%)
* 12–23 months → accumulated error (10%)

### 2.2 Right-hand formula – precise definition of (PE_j)

The equation defines the **prediction error (PE_j)** for a given brand–country combination (j).

Notation:

* (Y^{act}_{j,i}): **actual** volume for brand (j) at month (i) after generic entry.
* (Y^{pred}_{j,i}): **predicted** volume for brand (j) at month (i).
* (Avg_j): **average monthly volume** of the last 12 months **before** generic entry for brand (j) (normalizing factor).
* (|\cdot|): **absolute value**.
* Sums over (i) span specific month ranges (0–23, 0–5, 6–11, 12–23).

The total error is a **weighted sum of four normalized absolute errors**:

1. **Monthly error over all 24 months (grey term, weight 0.2)**

   [
   0.2 \left(
   \frac{\sum_{i=0}^{23} |Y^{act}*{j,i} - Y^{pred}*{j,i}|}
   {24 \cdot Avg_j}
   \right)
   ]

   * For each month (i) from 0 to 23, we compute the **absolute difference** between prediction and actual volume.
   * These 24 absolute errors are **summed**, then divided by:

     * 24 (number of months), and
     * (Avg_j) (scale of the brand).
   * This term rewards models that match the **detailed shape** of the erosion path month by month.

2. **Accumulated error for months 0–5 (pink term, weight 0.5)**

   [
   0.5 \left(
   \frac{\left|\sum_{i=0}^{5} Y^{act}*{j,i} - \sum*{i=0}^{5} Y^{pred}_{j,i}\right|}
   {6 \cdot Avg_j}
   \right)
   ]

   * Compute the **total actual volume** over months 0–5 and the **total predicted volume** over months 0–5.
   * Take the **absolute difference of these totals** (not month-by-month).
   * Normalize by (6 \cdot Avg_j) (6 months times average pre-generic volume).
   * This heavily penalizes misestimation of **total sales in the crucial first 6 months**, hence the **highest weight (50%)**.

3. **Accumulated error for months 6–11 (blue term, weight 0.2)**

   [
   0.2 \left(
   \frac{\left|\sum_{i=6}^{11} Y^{act}*{j,i} - \sum*{i=6}^{11} Y^{pred}_{j,i}\right|}
   {6 \cdot Avg_j}
   \right)
   ]

   * Same structure as the pink term but for months **6 to 11**.
   * Measures how well the model captures **medium-term cumulative volume**.

4. **Accumulated error for months 12–23 (yellow term, weight 0.1)**

   [
   0.1 \left(
   \frac{\left|\sum_{i=12}^{23} Y^{act}*{j,i} - \sum*{i=12}^{23} Y^{pred}_{j,i}\right|}
   {12 \cdot Avg_j}
   \right)
   ]

   * Same idea, but for months **12 to 23** (12 months).
   * Lower weight (10%) reflects that long-term tail errors are less critical from a business perspective, but still matter.

### 2.3 Intuition

* **Normalization by (Avg_j)** ensures that errors are **relative to brand size**; large and small brands are comparable.
* The metric is **asymmetric in time**:

  * Early months (0–5) dominate the score (50%), because these months are key for pricing and commercial decisions.
  * Mid-term (6–11) and long-term (12–23) cumulative volumes matter, but less.
  * Monthly shape (0–23) adds an extra 20% weight, rewarding models that get the dynamics right, not just totals.

Overall, this slide shows both **what each term of the formula means in time** (via the coloured bands) and **how it is mathematically combined** into the single prediction error (PE_j) used to rank teams in Phase 1-a.


## 1. Exact content of the slide

**Title (top left)**
`Metric: Prediction Error (Phase 1-a)`

---

### Left-hand figure (time series + bands)

Top centre, above coloured bands:
`Accumulated Error`

Three vertical coloured bands over the post-generic period:

* Over the pink band (months 0–5):
  `Month 0 to 5`
  `(50%)`
* Over the light-blue band (months 6–11):
  `Month 6 to 11`
  `(20%)`
* Over the light-yellow band (months 12–23):
  `Month 12 to 23`
  `(10%)`

On the left vertical axis:
`Volume`

Next to the red vertical dashed line at month 0:
`Generics`
`entry date`

Bottom x-axis tick labels: `-12`, `-8`, `-4`, `0`, `4`, `8`, `12`, `16`, `20`

Label at the far right of x-axis:
`Month_num`

Legend under the plot:

* blue line: `Bucket 2`
* yellow line: `Bucket 1`

Below the whole plot, in a grey bar:

* `Month 0 to 23 (20%)`
* `Monthly Error`

---

### Right-hand formula block

Main equation for prediction error of brand (j):

[
PE_j =
0.2
\left(
\frac{\sum_{i=0}^{23} \left| Y^{act}*{j,i} - Y^{pred}*{j,i} \right|}
{24 \cdot Avg_j}
\right)
+
0.5
\left(
\frac{\left|\sum_{i=0}^{5} Y^{act}*{j,i} - \sum*{i=0}^{5} Y^{pred}*{j,i}\right|}
{6 \cdot Avg_j}
\right)
+
0.2
\left(
\frac{\left|\sum*{i=6}^{11} Y^{act}*{j,i} - \sum*{i=6}^{11} Y^{pred}*{j,i}\right|}
{6 \cdot Avg_j}
\right)
+
0.1
\left(
\frac{\left|\sum*{i=12}^{23} Y^{act}*{j,i} - \sum*{i=12}^{23} Y^{pred}_{j,i}\right|}
{12 \cdot Avg_j}
\right)
]

(Each of the four large parentheses is drawn inside a coloured box: grey, pink, light-blue, light-yellow, matching the bands in the plot; the weights 0.2, 0.5, 0.2, 0.1 are printed in front of those boxes.)

At the bottom right, the definition of (Avg_j):

[
Avg_j = \frac{\sum_{i=-12}^{-1} Y^{act}_{j,i}}{12}
]

---

Top-right of slide: `NOVARTIS` logo
Bottom-right: slide number `18`

There are no other words, plots, or formulas.

---

## 2. Detailed explanation of the slide

This slide gives the **full mathematical definition** of the **Prediction Error metric for Phase 1-a (Scenario 1)** and visually links each term to a portion of the post-generic time series.

### 2.1 The time-series plot and coloured bands

On the left you see **two typical erosion profiles**:

* **Bucket 1 (yellow line)** – high erosion: volume collapses quickly after the generics entry date.
* **Bucket 2 (blue line)** – low/medium erosion: volume declines more gradually.

The **x-axis** (`Month_num`) is measured **relative to the generics entry**:

* Negative values (−12, −8, −4) are **months before** generic entry.
* 0 is the **generics entry date**, marked by a red vertical dashed line labelled “Generics entry date”.
* Positive values (4, 8, 12, 16, 20) are **months after** generics enter.

The **y-axis** is `Volume` (sales volume of the originator brand).

The post-generic period **months 0–23** is segmented into three coloured zones:

1. **Month 0 to 5 (50%) – pink**
   The first 6 months after generics arrive. This is usually where the steepest decline occurs and is therefore given the **highest weight** (50%) in the metric.

2. **Month 6 to 11 (20%) – light blue**
   The second 6-month block, where erosion continues and stabilises. Medium importance (20%).

3. **Month 12 to 23 (10%) – light yellow**
   The second year after entry, typically a lower, flatter tail of the curve. Lowest importance (10%).

Below the time-series plot there is a grey bar:

* `Month 0 to 23 (20%) – Monthly Error`

This indicates that, besides the three accumulated-error blocks, there is also a **month-by-month error term over the entire horizon**, weighted at 20%.

Together, the plot maps **time regions** to the **four terms** in the formula on the right.

---

### 2.2 The prediction error formula (PE_j)

The equation defines the **prediction error (PE_j)** for one brand–country combination (j).

#### Notation

* (Y^{act}_{j,i}): actual observed volume for brand (j) at month (i) (relative to generic entry).
* (Y^{pred}_{j,i}): predicted volume from your model for brand (j) at month (i).
* (Avg_j): average monthly pre-generic volume for brand (j) over the **12 months before entry** (defined at the bottom).
* (|\cdot|): absolute value (so errors cannot cancel each other).
* The indices (i) refer to relative months:

  * (i = -12, \dotsc, -1) → pre-generic months,
  * (i = 0, \dotsc, 23) → the 24 post-generic months we forecast.

The metric combines **four normalized absolute errors**, each with a specific **weight** (0.2, 0.5, 0.2, 0.1).

---

#### 2.2.1 First term – monthly error over 0–23 (weight 0.2, grey)

[
0.2
\left(
\frac{\sum_{i=0}^{23} |Y^{act}*{j,i} - Y^{pred}*{j,i}|}
{24 \cdot Avg_j}
\right)
]

* For each month (i) from 0 to 23 you compute the **absolute difference** between prediction and actual volume.
* Sum those 24 absolute differences.
* Divide by (24), the number of months, to get an **average absolute error per month**.
* Divide again by (Avg_j) so that this error is expressed **relative to the brand’s typical monthly size**.
* Multiply by 0.2 – this component counts for **20%** of the final (PE_j).
  It rewards models that reproduce the **month-by-month shape** of the erosion curve.

---

#### 2.2.2 Second term – accumulated error for months 0–5 (weight 0.5, pink)

[
0.5
\left(
\frac{\left|\sum_{i=0}^{5} Y^{act}*{j,i} - \sum*{i=0}^{5} Y^{pred}_{j,i}\right|}
{6 \cdot Avg_j}
\right)
]

* Compute the **total actual volume** in months 0–5 and the **total predicted volume** in months 0–5.
* Take the **absolute difference** between these two totals.
  (Notice the absolute value is around the difference of the sums – we care about net over- or under-estimation for the whole block.)
* Normalize by (6 \cdot Avg_j) (6 months times the pre-generic average).
* Multiply by 0.5 – this component is **half of the final score**, reflecting how critical the first 6 months after entry are for business decisions (pricing, resource allocation, etc.).

This term checks whether your model gets the **overall size of early erosion** right.

---

#### 2.2.3 Third term – accumulated error for months 6–11 (weight 0.2, blue)

[
0.2
\left(
\frac{\left|\sum_{i=6}^{11} Y^{act}*{j,i} - \sum*{i=6}^{11} Y^{pred}_{j,i}\right|}
{6 \cdot Avg_j}
\right)
]

* Same structure as the second term but over months **6–11**.
* Measures how accurately your model predicts **cumulative sales in the second half of year 1**.
* Weight of 20% aligns with the blue band labelled “Month 6 to 11 (20%)”.

---

#### 2.2.4 Fourth term – accumulated error for months 12–23 (weight 0.1, yellow)

[
0.1
\left(
\frac{\left|\sum_{i=12}^{23} Y^{act}*{j,i} - \sum*{i=12}^{23} Y^{pred}_{j,i}\right|}
{12 \cdot Avg_j}
\right)
]

* Same logic, now summing months **12–23** (the second year).
* Checks how well the model captures **long-term tail behaviour**.
* Dividing by (12 \cdot Avg_j) normalizes over 12 months.
* With a weight of 0.1 (10%), this term is the **least influential**, but it still ensures that very poor long-term forecasts are penalized.

---

### 2.3 Definition and role of (Avg_j)

At the bottom right, the slide explicitly defines:

[
Avg_j = \frac{\sum_{i=-12}^{-1} Y^{act}_{j,i}}{12}
]

This is the **average monthly volume before generic entry** for brand (j):

* Sum the **actual volumes** for months (-12, -11, \dotsc, -1) (the last 12 months before the generic entry).
* Divide by 12.

Interpretation:

* (Avg_j) is a measure of the **brand’s baseline size** in the year prior to erosion.
* By dividing all four error components by (Avg_j), the metric becomes **scale-free**:

  * A 50,000-unit error is treated differently for a small brand vs a huge blockbuster.
  * Errors are effectively measured as a **fraction of a typical monthly volume**.

This is crucial because the dataset contains brands with very different absolute sales levels; normalization ensures each brand contributes **fairly and comparably** to the overall evaluation.

---

### 2.4 Summary

This slide combines:

* A **visual breakdown** of the post-generic period (bands and plot),
* A **precise mathematical formula** for the Prediction Error (PE_j),
* And the **formal definition** of the normalization factor (Avg_j).

Together, they fully specify **how your Scenario 1 forecasts will be scored**: a weighted, normalized sum of absolute errors, with the strongest emphasis on getting the **early cumulative erosion (months 0–5)** correct while still respecting monthly detail and longer-term behaviour.



## 1. Exact content of the slide

**Title**

* `Metric: Prediction Error (Phase 1-a)`

---

**Text and formulas**

`Prediction Error per country brand (PE_j):`

[
PE_j = 0.2\left(
\frac{\sum_{i=0}^{23} \left|Y^{act}*{j,i} - Y^{pred}*{j,i}\right|}
{24 \cdot Avg_j}
\right)

* 0.5\left(
  \frac{\left|\sum_{i=0}^{5} Y^{act}*{j,i} - \sum*{i=0}^{5} Y^{pred}_{j,i}\right|}
  {6 \cdot Avg_j}
  \right)
  ]

[
\qquad\qquad

* 0.2\left(
  \frac{\left|\sum_{i=6}^{11} Y^{act}*{j,i} - \sum*{i=6}^{11} Y^{pred}_{j,i}\right|}
  {6 \cdot Avg_j}
  \right)
* 0.1\left(
  \frac{\left|\sum_{i=12}^{23} Y^{act}*{j,i} - \sum*{i=12}^{23} Y^{pred}_{j,i}\right|}
  {12 \cdot Avg_j}
  \right)
  ]

`The final Prediction Error (PE) will be the weighted sum of the averages of all the individual prediction errors (PE_j) across the two buckets. Note that the average of Bucket 1 is weighted twice as much as that from Bucket 2.`

[
PE = \frac{2}{n_{B1}} \sum_{j=1}^{n_{B1}} PE_{j,B1}
+ \frac{1}{n_{B2}} \sum_{j=1}^{n_{B2}} PE_{j,B2}
]

Top-right: `NOVARTIS` logo
Bottom-right: slide number `19`

There are **no plots** on this slide, only text and formulas.

---

## 2. Detailed explanation of this slide

This slide ties together the **per-brand prediction error** and the **overall contest metric** for Phase 1-a.

### 2.1 Per-country-brand prediction error (PE_j)

The first displayed formula is the **Prediction Error for a single country–brand**, denoted (PE_j). It is identical to the metric you saw in the previous slides, now written compactly without the coloured boxes.

Variables:

* (Y^{act}_{j,i}): actual sales volume for brand (j) at month (i) after generic entry.
* (Y^{pred}_{j,i}): predicted sales volume for brand (j) at month (i).
* (Avg_j): average monthly sales of brand (j) in the 12 months before generic entry (normalization factor).
* Index (i) over months:

  * (i = 0,\dots,23): 24 months after generic entry.
  * Sub-ranges 0–5, 6–11, 12–23 correspond to the three post-entry blocks.
* (|\cdot|): absolute value.

The formula is a **weighted sum of four normalized error terms**:

1. **Monthly error over 0–23** (weight 0.2)
   [
   0.2\left(
   \frac{\sum_{i=0}^{23} |Y^{act}*{j,i} - Y^{pred}*{j,i}|}
   {24\cdot Avg_j}
   \right)
   ]
   This measures the average absolute month-by-month error over the full 24-month horizon, scaled by the brand size. It captures how well you match the detailed shape of the erosion curve.

2. **Accumulated error months 0–5** (weight 0.5)
   [
   0.5\left(
   \frac{\left|\sum_{i=0}^{5} Y^{act}*{j,i} - \sum*{i=0}^{5} Y^{pred}_{j,i}\right|}
   {6\cdot Avg_j}
   \right)
   ]
   This compares **total actual volume vs total predicted volume** in the first 6 months after generic entry. Because these early months are most critical commercially, this term has the highest weight (50%).

3. **Accumulated error months 6–11** (weight 0.2)
   [
   0.2\left(
   \frac{\left|\sum_{i=6}^{11} Y^{act}*{j,i} - \sum*{i=6}^{11} Y^{pred}_{j,i}\right|}
   {6\cdot Avg_j}
   \right)
   ]
   This checks the cumulative error in the second 6-month block, mid-term erosion. Weight 20%.

4. **Accumulated error months 12–23** (weight 0.1)
   [
   0.1\left(
   \frac{\left|\sum_{i=12}^{23} Y^{act}*{j,i} - \sum*{i=12}^{23} Y^{pred}_{j,i}\right|}
   {12\cdot Avg_j}
   \right)
   ]
   This looks at the long-term tail (second year) and has the lowest weight (10%).

All four pieces are **normalized by (Avg_j)** so that errors are relative to the typical size of the brand; a given absolute error counts more for a small brand than for a large one.

Thus (PE_j) is a single score summarizing your forecast quality for one country–brand in Scenario 1.

---

### 2.2 From per-brand errors to the final contest metric (PE)

After computing (PE_j) for every brand–country combination in the test set, the organizers then combine them into one overall score (PE) for your model.

The brands are divided into **two erosion buckets**:

* **Bucket 1 (B1)** – high erosion cases (steep declines).
* **Bucket 2 (B2)** – low/medium erosion cases (slower declines).

Let:

* (n_{B1}): number of country–brands in Bucket 1.
* (n_{B2}): number of country–brands in Bucket 2.
* (PE_{j,B1}): prediction error for the (j)-th brand in Bucket 1.
* (PE_{j,B2}): prediction error for the (j)-th brand in Bucket 2.

The final metric is:

[
PE =
\frac{2}{n_{B1}} \sum_{j=1}^{n_{B1}} PE_{j,B1}
+
\frac{1}{n_{B2}} \sum_{j=1}^{n_{B2}} PE_{j,B2}
]

Interpretation:

* (\frac{1}{n_{B1}} \sum_{j=1}^{n_{B1}} PE_{j,B1}) is the **average prediction error across all high-erosion brands**.
* (\frac{1}{n_{B2}} \sum_{j=1}^{n_{B2}} PE_{j,B2}) is the **average prediction error across all low-erosion brands**.
* The factor **2** in front of the first term means the **Bucket 1 average counts twice as much** as the Bucket 2 average.

So the final score is a **weighted average of bucket-wise mean errors**, emphasizing **high-erosion (Bucket 1) cases**, because those are more critical for the business problem.

In short:

1. Compute (PE_j) for each country–brand using the four-component formula.
2. Split those (PE_j) into Bucket 1 and Bucket 2.
3. Compute the average in each bucket.
4. Form the final score (PE) as “2 × Bucket-1 average + 1 × Bucket-2 average”.

Lower (PE) means **better performance** in the Datathon.



## 1. Exact content of the slide

**Title**

* `Metric: Prediction Error (Phase 1-b)`

**Body text**

`In this second scenario (Scenario 2), participants will provide predictions based on 6 actual data points available after the generic entry date. To compute the prediction error of Phase 1-b, we will evaluate the difference between the predicted values vs the actual volume in three different ways weighted as follows:`

1. `Absolute monthly error of months 6 to 23 (20%)`
2. `Absolute accumulated error of months 6 to 11 (50%)`
3. `Absolute accumulated error of months 12 to 23 (30%)`

`All the 3 items will be normalized by the average (Avg_j) monthly volume of the last 12 months before the generic entry to consider the magnitude of the brand.`

Top-right: `NOVARTIS` logo
Bottom-right: slide number `20`

There are **no plots or explicit formulas** on this slide.

---

## 2. Detailed explanation of the slide

This slide defines the **evaluation metric for Phase 1-b**, which corresponds to **Scenario 2** of the Datathon.

### 2.1 Scenario 2 context

* In **Scenario 2**, unlike Scenario 1, you **do have some actual post-generic data** available when you forecast:

  * Exactly **6 actual data points** after the generic entry date (months 0–5).
* Your task is to **predict volumes from month 6 through month 23**.
* The metric evaluates how close your predictions are to the actual volumes in this later period.

### 2.2 Three components of the prediction error

The difference between predicted and actual volumes is summarised in **three different error components**, each with a specific **weight** in the final score:

1. **Absolute monthly error of months 6 to 23 (20%)**

   * For each month (i = 6, 7, \dots, 23) you compute the **absolute error**:
     (|\text{prediction}_i - \text{actual}_i|).
   * These monthly absolute errors are aggregated across months 6–23 (e.g., summed or averaged).
   * This component captures how well your model matches the **month-by-month shape** over the prediction horizon in Scenario 2.
   * It contributes **20%** to the final Phase 1-b prediction error.

2. **Absolute accumulated error of months 6 to 11 (50%)**

   * You compare the **total predicted volume** vs the **total actual volume** over months 6–11:

     * Sum actual volumes from 6 to 11,
     * Sum predicted volumes from 6 to 11,
     * Take the **absolute difference** between these two totals.
   * This measures whether your model gets **overall sales in the 6–11 month block** correct, not just individual months.
   * It has the **highest weight (50%)**, indicating that in Scenario 2 the organizers care most about the **cumulative volume in the first 6 months of the prediction window** (months 6–11 after generic entry).

3. **Absolute accumulated error of months 12 to 23 (30%)**

   * Similarly, for months 12–23 you:

     * Sum actual volumes over 12–23,
     * Sum predicted volumes over 12–23,
     * Take the absolute difference between these sums.
   * This evaluates how well your model captures the **longer-term cumulative volume** in the tail of the erosion curve.
   * It accounts for **30%** of the final error.

Altogether, the weights (20% + 50% + 30%) = 100%, so the final Phase 1-b metric is a **weighted combination** of:

* detailed monthly accuracy (20%),
* early block cumulative accuracy (months 6–11, 50%),
* later block cumulative accuracy (months 12–23, 30%).

### 2.3 Normalization with (Avg_j)

The slide states:

> “All the 3 items will be normalized by the average (Avg_j) monthly volume of the last 12 months before the generic entry to consider the magnitude of the brand.”

Here:

* (Avg_j) is the same quantity defined earlier:

  * the **average monthly volume** of brand (j) in the **12 months prior to generic entry**.
* Each of the three error components will be **divided by (Avg_j)**.

Why normalize?

* Brands differ greatly in absolute sales levels.
* Without normalization, a given numerical error would penalize large and small brands very differently.
* Dividing by (Avg_j) transforms errors into **relative errors** (roughly “fractions of a typical pre-generic month”), making all brands **comparable** in the metric.

### 2.4 Conceptual comparison vs Phase 1-a

* Phase 1-a (Scenario 1) included:

  * monthly error over months 0–23, and
  * three cumulative blocks (0–5, 6–11, 12–23).
* Phase 1-b (Scenario 2) shifts focus:

  * You already know months 0–5, so error is computed only from **month 6 onward**.
  * There are **three components instead of four**, all starting at month 6.
  * Weights differ: there is no 0–5 term, and cumulative blocks 6–11 and 12–23 now carry 50% and 30%, respectively.

In summary, this slide tells you **how your Scenario-2 forecasts will be graded**: by a normalized, weighted combination of monthly errors and cumulative errors over months 6–23, scaled by pre-generic average volume to fairly compare brands of different sizes.


## 1. Exact content of the slide

**Title**

* `Metric: Prediction Error (Phase 1-b)`

---

### Left-hand figure (time series and bands)

At the top of the plot:

* `Accumulated Error`

Over the coloured bands:

* Over a small grey band covering months 0–5: `Seen actuals`
* Over the light-blue band (months 6–11):
  `Month 6 to 11`
  `(50%)`
* Over the light-pink band (months 12–23):
  `Month 12 to 23`
  `(30%)`

Y-axis label on the left:

* `Volume`

Near the red vertical dashed line at month 0:

* `Generics`
  `entry date`

Legend at bottom-left of the plot:

* Blue line: `Bucket 2`
* Yellow line: `Bucket 1`

X-axis tick labels: `-12`, `-8`, `-4`, `0`, `4`, `8`, `12`, `16`, `20`

Label at the far right of x-axis:

* `Month_num`

Below the plot, in a pale-yellow bar:

* `Month 6 to 23 (20%)`
* `Monthly Error`

---

### Right-hand formula

[
PE_j =
0.2\left(
\frac{\sum_{i=6}^{23} \left|Y^{act}*{j,i} - Y^{pred}*{j,i}\right|}
{18 \cdot Avg_j}
\right)

* 0.5\left(
  \frac{\left|\sum_{i=6}^{11} Y^{act}*{j,i} - \sum*{i=6}^{11} Y^{pred}_{j,i}\right|}
  {6 \cdot Avg_j}
  \right)
* 0.3\left(
  \frac{\left|\sum_{i=12}^{23} Y^{act}*{j,i} - \sum*{i=12}^{23} Y^{pred}_{j,i}\right|}
  {12 \cdot Avg_j}
  \right)
  ]

Each term is shown inside a coloured rectangle:

* Yellow box for the first term with the factor `0.2`
* Blue box for the second term with the factor `+ 0.5`
* Pink box for the third term with the factor `+ 0.3`

---

Top-right: `NOVARTIS` logo
Bottom-right: slide number `21`

There are no other texts or plots.

---

## 2. Detailed explanation of the slide

This slide gives both a **visual** and a **mathematical** definition of the **Prediction Error metric in Phase 1-b (Scenario 2)**.

### 2.1 Time-series plot and bands

On the left you see the same stylised erosion curves as before:

* **Bucket 1 (yellow)** – high erosion (sharp drop after generics).
* **Bucket 2 (blue)** – lower erosion (gradual decline).

The **x-axis** (`Month_num`) is relative to the **generics entry date**:

* Negative values (−12, −8, −4) are months before entry.
* `0` is the **Generics entry date**, marked with the red dashed line.
* Positive values (4, 8, 12, 16, 20) are months after entry.

The **y-axis** is `Volume` (monthly sales volume).

Key annotations:

* From **month 0 to 5**, a bracket at the top labelled **“Seen actuals”** indicates that, in Scenario 2, these six post-entry months are **observed** and are **not part of the error calculation** (they are inputs to your model).
* Starting at **month 6**, the plot is coloured to show the evaluation windows:

  * **Month 6 to 11 (50%)** – blue band, the first 6 months of the *prediction* horizon.
  * **Month 12 to 23 (30%)** – pink band, the second year of the prediction horizon.

At the bottom, a yellow bar labelled **“Month 6 to 23 (20%) – Monthly Error”** shows that there is also a **monthly error component** over **all months 6–23**, weighted 20%.

So visually:

* Months 0–5: known actuals, no error terms.
* Months 6–23: forecast area; split into:

  * **monthly error** (20% weight),
  * **cumulative error 6–11** (50%),
  * **cumulative error 12–23** (30%).

### 2.2 Formula for per-brand error (PE_j)

The equation on the right defines the **Prediction Error** for one **country–brand combination (j)** in Scenario 2.

#### Notation

* (Y^{act}_{j,i}): actual volume for brand (j) at month (i) (relative to entry).
* (Y^{pred}_{j,i}): predicted volume for brand (j) at month (i).
* (Avg_j): the **average monthly volume** of the 12 months **before** generic entry for brand (j).
* Indices:

  * (i = 6,\dots,23): months **after** the 6 observed post-entry months.
* (|\cdot|) is absolute value.

The metric is a **weighted sum of three normalized error components**:

---

#### 1) Monthly error over months 6–23 (weight 0.2, yellow term)

[
0.2\left(
\frac{\sum_{i=6}^{23} \left|Y^{act}*{j,i} - Y^{pred}*{j,i}\right|}
{18 \cdot Avg_j}
\right)
]

* For each future month (i = 6,\dots,23) (18 months total), you compute the **absolute difference** between prediction and actual.
* These 18 absolute errors are summed, then divided by:

  * 18 (number of months) and
  * (Avg_j) (pre-generic scale of the brand).
* The 0.2 factor gives this **20% weight** in the final score.
* This term rewards models that correctly capture the **month-to-month trajectory** of erosion during the forecast window.

---

#### 2) Accumulated error months 6–11 (weight 0.5, blue term)

[
0.5\left(
\frac{\left|\sum_{i=6}^{11} Y^{act}*{j,i} - \sum*{i=6}^{11} Y^{pred}_{j,i}\right|}
{6 \cdot Avg_j}
\right)
]

* Compute the total **actual** volume and total **predicted** volume for the first 6 forecast months (6–11).
* Take the **absolute difference** between these totals.
* Normalize by (6 \cdot Avg_j).
* Weight = 0.5 → **50% of the final error**.
* Because you already observed months 0–5, the organisers care most about your ability to get the **overall amount of volume in months 6–11** correct—this is when the erosion pattern is still evolving but strongly influences annual sales.

---

#### 3) Accumulated error months 12–23 (weight 0.3, pink term)

[
0.3\left(
\frac{\left|\sum_{i=12}^{23} Y^{act}*{j,i} - \sum*{i=12}^{23} Y^{pred}_{j,i}\right|}
{12 \cdot Avg_j}
\right)
]

* Similarly, for the second year of the forecast (months 12–23) you:

  * sum actual volumes,
  * sum predicted volumes,
  * take the absolute difference,
  * normalize by (12 \cdot Avg_j).
* This term has **30% weight** and assesses how well your model reproduces the **long-term cumulative volume** after erosion has stabilised.

---

### 2.3 Normalization with (Avg_j)

All three components divide by (Avg_j), the pre-generic average volume:

* This normalization adjusts for **brand size** so that absolute errors are interpreted as **relative errors**.
* A given numeric error counts more for a small brand and less for a large one, making the metric fair across a heterogeneous portfolio.

---

### 2.4 Overall interpretation

For Scenario 2 / Phase 1-b:

* You already know months 0–5; you forecast months 6–23.
* Your per-brand score (PE_j) combines:

  * **Detailed monthly fit** over 6–23 (20%),
  * **Cumulative accuracy for months 6–11** (50%),
  * **Cumulative accuracy for months 12–23** (30%),
    all normalized by brand size.
* Lower (PE_j) means better forecasts. These per-brand scores will then be aggregated across buckets (B1, B2) in the same way as described earlier to obtain your final competition score.


## 1. Exact content of the slide

**Title**

* `Metric: Prediction Error (Phase 1-b)`

---

**Text and formulas**

`Prediction Error per country brand (PE_j):`

[
PE_j =
0.2\left(
\frac{\sum_{i=6}^{23} \left|Y^{act}*{j,i} - Y^{pred}*{j,i}\right|}
{18 \cdot Avg_j}
\right)

* 0.5\left(
  \frac{\left|\sum_{i=6}^{11} Y^{act}*{j,i} - \sum*{i=6}^{11} Y^{pred}_{j,i}\right|}
  {6 \cdot Avg_j}
  \right)
* 0.3\left(
  \frac{\left|\sum_{i=12}^{23} Y^{act}*{j,i} - \sum*{i=12}^{23} Y^{pred}_{j,i}\right|}
  {12 \cdot Avg_j}
  \right)
  ]

`The final Prediction Error (PE) will be the weighted sum of the averages of all the individual prediction errors (PE_j) across the two buckets. Note that the average of Bucket 1 is weighted twice as much as that from Bucket 2.`

[
PE
==

\frac{2}{n_{B1}} \sum_{j=1}^{n_{B1}} PE_{j,B1}
+
\frac{1}{n_{B2}} \sum_{j=1}^{n_{B2}} PE_{j,B2}
]

Top-right: `NOVARTIS` logo
Bottom-right: slide number `22`

There are no plots or tables on this slide.

---

## 2. Detailed explanation of the slide

This slide gives the **formal definition of the scoring metric for Phase 1-b (Scenario 2)** and explains how per-brand errors are aggregated into a single competition score.

### 2.1 Per-country-brand prediction error (PE_j)

The first formula defines the **prediction error for one country–brand (j)**.

Notation:

* (Y^{act}_{j,i}): actual sales volume for brand (j) at month (i) after generic entry.
* (Y^{pred}_{j,i}): your model’s predicted volume for brand (j) at month (i).
* (Avg_j): average monthly volume of brand (j) in the 12 months **before** generic entry.
* Months (i = 6,\dots,23) are the **forecast horizon** in Scenario 2 (you already saw months 0–5).
* (|\cdot|): absolute value.

The formula is a **weighted sum of three normalized error components**:

1. **Monthly error, months 6–23 (weight 0.2)**
   [
   0.2\left(
   \frac{\sum_{i=6}^{23} |Y^{act}*{j,i} - Y^{pred}*{j,i}|}
   {18 \cdot Avg_j}
   \right)
   ]

   * For each of the 18 forecast months (6–23), compute the absolute difference between actual and predicted.
   * Sum these 18 absolute errors.
   * Divide by (18) and by (Avg_j) to normalize.
   * Gives 20% of the final (PE_j) and measures **month-by-month fit** of the trajectory.

2. **Accumulated error, months 6–11 (weight 0.5)**
   [
   0.5\left(
   \frac{\left|\sum_{i=6}^{11} Y^{act}*{j,i} - \sum*{i=6}^{11} Y^{pred}_{j,i}\right|}
   {6 \cdot Avg_j}
   \right)
   ]

   * Sum actual volumes over months 6–11 and predicted volumes over the same months.
   * Take the absolute difference between these totals.
   * Normalize by (6 \cdot Avg_j).
   * This has **50% weight**, reflecting that getting the **total volume in the first 6 forecast months** right is most important.

3. **Accumulated error, months 12–23 (weight 0.3)**
   [
   0.3\left(
   \frac{\left|\sum_{i=12}^{23} Y^{act}*{j,i} - \sum*{i=12}^{23} Y^{pred}_{j,i}\right|}
   {12 \cdot Avg_j}
   \right)
   ]

   * Same structure for the second year (months 12–23).
   * Weight 30%, focusing on **long-term cumulative volume**.

All three components are **normalized by (Avg_j)** so that errors are expressed **relative to brand size**, allowing fair comparison between large and small brands.

Thus (PE_j) is a single number summarizing how well you forecast months 6–23 for that specific country–brand in Scenario 2.

---

### 2.2 Aggregating across buckets to get the final score (PE)

The second formula defines the **overall competition metric** for Phase 1-b.

Here:

* Brands are split into two erosion buckets:

  * **Bucket 1 (B1)**: high-erosion cases.
  * **Bucket 2 (B2)**: low/medium-erosion cases.
* (n_{B1}): number of test observations in Bucket 1.
  (n_{B2}): number in Bucket 2.
* (PE_{j,B1}): prediction error for the (j)-th brand in Bucket 1.
  (PE_{j,B2}): prediction error for the (j)-th brand in Bucket 2.

The formula:

[
PE
==

\frac{2}{n_{B1}} \sum_{j=1}^{n_{B1}} PE_{j,B1}
+
\frac{1}{n_{B2}} \sum_{j=1}^{n_{B2}} PE_{j,B2}
]

Interpretation:

* (\frac{1}{n_{B1}} \sum_{j=1}^{n_{B1}} PE_{j,B1}) is the **average error across all high-erosion brands**.
* (\frac{1}{n_{B2}} \sum_{j=1}^{n_{B2}} PE_{j,B2}) is the **average error across all low-erosion brands**.
* The factor **2** in front of the Bucket-1 average means that **Bucket 1 is given double the weight of Bucket 2**.

So:

* High-erosion cases, which are more business-critical, influence the final score twice as much.
* A good model must perform particularly well on Bucket 1 to achieve a low final (PE).

A lower value of (PE) indicates **better forecasting performance** and leads to a higher ranking in the Datathon.

