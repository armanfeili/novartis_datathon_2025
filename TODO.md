# Novartis Datathon 2025 - Comprehensive TODO List

> **Competition Goal**: Forecast generic erosion (normalized volume) for pharmaceutical drugs  
> **Deadline**: Phase 1A/1B submission deadline (check competition site)  
> **Hero Model**: CatBoost with scenario-specific sample weights  
>
> âš ï¸ **When in doubt, complete Priority A items (CatBoost + features + validation + submission) before touching Priority B.**

### ğŸ“… Implementation Phases

| Phase | Focus | Sections |
|-------|-------|----------|
| **Phase 1** (before first stable LB submission) | Core pipeline, baseline model, first submission | 0â€“7 (Priority A only), 9.1, 9.4, 10.1â€“10.2, 11.1â€“11.3, 12.1â€“12.4 |
| **Phase 2** (after solid LB score) | Advanced features, unified logging, plotting, CV search, ensemble tuning | All remaining unchecked items (6.7â€“6.9, 8.*, booster models, HPO, TabNet/FT-Transformer) |

### ğŸ”§ Canonical CLI Flags (Locked)

| Flag | Format | Example |
|------|--------|--------|
| `--scenario` | Integer `1` or `2` | `--scenario 1` |
| `--model` | Lowercase string | `--model catboost` |
| `--split` | `train` or `test` | `--split train` |
| `--mode` | `train` or `test` | `--mode train` |
| `--data-config` | Path to YAML | `--data-config configs/data.yaml` |
| `--features-config` | Path to YAML | `--features-config configs/features.yaml` |
| `--run-config` | Path to YAML | `--run-config configs/run_defaults.yaml` |
| `--model-config` | Path to YAML | `--model-config configs/model_cat.yaml` |
| `--force-rebuild` | Flag (no value) | `--force-rebuild` |

---

## Table of Contents
0. [Design & Consistency Checks](#0-design--consistency-checks)
1. [Critical Path (Must Do)](#1-critical-path-must-do)
2. [Data Pipeline](#2-data-pipeline)
3. [Feature Engineering](#3-feature-engineering)
4. [Model Development](#4-model-development)
5. [Training Pipeline](#5-training-pipeline)
6. [Validation & Evaluation](#6-validation--evaluation)
7. [Inference & Submission](#7-inference--submission)
8. [Experimentation & Optimization](#8-experimentation--optimization)
9. [Testing & Quality Assurance](#9-testing--quality-assurance)
10. [Documentation & Presentation](#10-documentation--presentation)
11. [Colab/Production Readiness](#11-colabproduction-readiness)
12. [Competition Strategy](#12-competition-strategy)

---

### ğŸ§­ Recommended Implementation Order (Sections)

0 â†’ 2 â†’ 3 â†’ 6 â†’ 5 â†’ 4 â†’ 7 â†’ 9 â†’ 10 â†’ 11 â†’ 8 â†’ 12

- **0 â€“ Design & Consistency Checks**: configs, paths, CLI & docs alignment.
- **2 â€“ Data Pipeline**: panels, caching, train/test behavior, leakage checks.
- **3 â€“ Feature Engineering**: scenario-aware `make_features`, early-erosion features.
- **6 â€“ Validation & Evaluation**: metrics, scenario simulation, official metric wrapper.
- **5 â€“ Training Pipeline**: `train.py`, sample weights, run metadata, config wiring.
- **4 â€“ Model Development**: baselines + CatBoost hero; other models after that.
- **7 â€“ Inference & Submission**: `inference.py`, template alignment, aux file.
- **9 â€“ Testing & QA**: unit + integration tests tightened around the full pipeline.
- **10 â€“ Documentation & Presentation**: README, approach, methodology notes.
- **11 â€“ Colab/Production Readiness**: Colab notebook, envs, performance tweaks.
- **8 â€“ Experimentation & Optimization**: HPO, ensembles, ablations, smoothing.
- **12 â€“ Competition Strategy**: LB management, final-week playbook, external-data checks.

> **Section 1 â€“ Critical Path (Must Do)** is a *running checklist* (EDA, first LB submission, score iterations) and should be followed in parallel as soon as the minimal 0â†’2â†’3â†’6â†’5â†’4 path is usable.


## 0. Design & Consistency Checks

> **Priority Note**: Items directly tied to CatBoost + robust features + solid validation are **Priority A**; purely architectural items (full BaseModel hierarchy, advanced notebooks, stretch goals) are **Priority B / post-competition**.

### 0.0 Global Preparation & Repository Hygiene
- [x] **Read and internalize documentation**:
  - [x] Read `docs/planning/functionality.md` thoroughly âœ…
  - [x] Read `docs/planning/approach.md` âœ…
  - [x] Skim `docs/instructions/NOVARTIS_DATATHON_2025_COMPLETE_GUIDE_V2.md` and `docs/planning/question-set.md` âœ…
- [x] **Ensure directory structure matches the spec**:
  - [x] Verify top-level folders exist: `data/`, `src/`, `configs/`, `docs/`, `notebooks/`, `submissions/`, `tests/`, `artifacts/` âœ…
  - [x] Verify `docs/guide/metric_calculation.py` exists âœ…
  - [x] Verify `docs/guide/submission_template.csv` exists âœ…
  - [x] Verify `docs/guide/auxiliar_metric_computation_example.csv` exists âœ…
- [x] **Verify environment files exist and are consistent**:
  - [x] `requirements.txt`, `env/colab_requirements.txt`, and/or `environment.yml` exist âœ…
  - [x] Ensure requirements list all needed packages and are tested âœ… (added pyarrow, pytest; all 92 tests pass)

### 0.1 Code-Documentation Alignment
- [x] **Cross-check all items marked as âœ… (`[x]`)** against the actual codebase and revert them to â˜ (`[ ]`) if implementation is partial or differs from `functionality.md` / `README.md` âœ…
- [x] **Ensure function signatures and behaviors** in code match the descriptions in `docs/functionality.md` and `README.md` (e.g. `make_features`, `compute_pre_entry_stats`, `compute_metric1`, `generate_submission`, CLI entrypoints) âœ…
- [x] **Align CLI examples** in `README.md`, `TODO.md` (Quick Commands) and actual `src/train.py` / `src/inference.py` argument names (`--data-config`, `--features-config`, `--run-config`, `--model-config`, `--scenario`, `--model` etc.) âœ…
  - [x] Updated README.md CLI examples to use `--scenario 1` and `--scenario 2` (integer scenarios) âœ…
  - [x] Updated `src/train.py` CLI to accept integer scenarios âœ…
  - [x] Updated `src/evaluate.py` examples in README to match actual function signatures âœ…
- [x] **Confirm all referenced paths exist** and are correctly used in code:
  - [x] `docs/guide/metric_calculation.py` âœ…
  - [x] `docs/guide/submission_template.csv` âœ…
  - [x] `configs/*.yaml` (all referenced config files) âœ…

### 0.2 Configuration System (`configs/`)
- [x] **Verify `configs/data.yaml` contains**:
  - [x] `paths.raw_dir`, `paths.interim_dir`, `paths.processed_dir`, `paths.artifacts_dir` âœ…
  - [x] `files.train`, `files.test`, `files.aux_metric`, `files.submission_template` âœ…
  - [x] `columns` section: `id_keys`, `time_key`, `calendar_month`, `raw_target`, `model_target`, `meta_cols` âœ…
  - [x] Added `mean_erosion` to `meta_cols` list âœ…
- [x] **Verify `configs/features.yaml` contains**:
  - [x] Sections: `pre_entry`, `time`, `generics`, `drug`, `scenario2_early`, `interactions` âœ…
  - [x] Scenario-specific enable/disable flags for feature groups âœ…
- [x] **Verify `configs/model_cat.yaml` contains**:
  - [x] `model_type`, `params`, `training`, `categorical_features` âœ…
- [x] **Verify `configs/run_defaults.yaml` contains**:
  - [x] `reproducibility.seed`, `validation.*` settings âœ…
  - [x] `scenarios.scenario1/scenario2` with `forecast_start`, `forecast_end`, `feature_cutoff` âœ…
  - [x] `sample_weights` definition (time-window and bucket weights) âœ…
  - [x] `logging.level`, `logging.log_to_file` âœ…
- [x] **Configs as single source of truth**: âœ…
  - [x] Added `official_metric` section to `configs/run_defaults.yaml` with all metric weights and bucket threshold âœ…
  - [x] Updated `src/data.py compute_pre_entry_stats()` to accept `bucket_threshold` and `run_config` parameters âœ…
  - [x] Added `OFFICIAL_*` constants and `validate_config_matches_official()` to `src/evaluate.py` âœ…
  - [x] Documented that fallback metrics intentionally match official script (hardcoded values for exact correspondence) âœ…
  - [x] Added 5 new tests for config validation (92 tests total, all pass) âœ…

### 0.3 External Data & Constraints Rule-Check
- [x] **Explicit rule-check on external data and constraints** âœ…
  - [x] Re-read the latest official rules on:
    - [x] Use of **external data**: Not explicitly prohibited but "Modeling Freedom: Any approach/model allowed" âœ…
    - [x] Any restrictions on **model types** or **ensembles**: None - "explainability and simplicity valued" âœ…
    - [x] Exact requirements for **reproducibility** and **code handover**: Required for finalists âœ…
  - [x] Documented in `docs/planning/approach.md` (Appendix D.1):
    - [x] **Decision: NOT using external data** - rationale documented (time investment, ROI uncertainty, complexity) âœ…
    - [x] All constraints documented (no test target leakage, deterministic seeds, code reproducibility) âœ…

### 0.4 Code Fixes Applied (Implementation Log)
> **Summary**: The following fixes were applied to achieve full Section 0 compliance.

- [x] **`src/data.py`**:
  - [x] Fixed syntax error: removed extra `"""` at line 1 (module docstring) âœ…
  - [x] Fixed Unicode arrow character `â†` in docstring (replaced with `<-`) âœ…
  - [x] Added canonical constants: `ID_COLS`, `TIME_COL`, `CALENDAR_MONTH_COL`, `RAW_TARGET_COL`, `MODEL_TARGET_COL` âœ…
- [x] **`src/features.py`**:
  - [x] Changed `SCENARIO_CONFIG` keys from strings (`'scenario1'`, `'scenario2'`) to integers (`1`, `2`) âœ…
  - [x] Added `_normalize_scenario()` helper to accept both int and string inputs for backward compatibility âœ…
  - [x] Fixed month parsing: replaced `pd.to_datetime(df['month']).dt.month` with explicit month name mapping (`{'Jan': 1, ...}`) âœ…
  - [x] Fixed FutureWarning in `groupby.apply()` by adding `include_groups=False` âœ…
- [x] **`src/train.py`**:
  - [x] Updated `--scenario` CLI argument to use `type=int` âœ…
  - [x] Updated imports to use `META_COLS` from `src/data.py` âœ…
- [x] **`src/validation.py`**:
  - [x] Updated `simulate_scenario()` to use `_normalize_scenario()` from features.py âœ…
- [x] **`src/evaluate.py`**:
  - [x] Updated `compute_bucket_metrics()` to handle integer scenarios âœ…
- [x] **`src/inference.py`**:
  - [x] Updated `detect_test_scenarios()` to return `{1: [...], 2: [...]}` (integer keys) âœ…
- [x] **`src/models/__init__.py`**:
  - [x] Implemented lazy imports for native-library models (LightGBM, XGBoost, CatBoost, NN) to prevent import failures when libraries are misconfigured âœ…
  - [x] Linear models and baselines import eagerly (no native dependencies) âœ…
- [x] **`configs/data.yaml`**:
  - [x] Added `mean_erosion` to `meta_cols` list âœ…
- [x] **`tests/test_smoke.py`**:
  - [x] Complete rewrite to match actual function signatures âœ…
  - [x] Updated to use integer scenarios (`scenario=1`, `scenario=2`) âœ…
  - [x] Added `test_model_imports` using lazy imports (no skip needed) âœ…
  - [x] Separated model interface test from import test âœ…
  - [x] Added Section 0.2 config validation tests (5 new tests) âœ…
  - [x] **Test results: 92 passed, 0 skipped, 0 warnings** âœ…
- [x] **`tests/conftest.py`**:
  - [x] Created pytest configuration with warning filters âœ…
  - [x] Filters torch/NumPy initialization warnings (external library) âœ…
  - [x] Filters DataFrameGroupBy.apply FutureWarnings âœ…
- [x] **`README.md`**:
  - [x] Updated CLI examples: `--scenario scenario1` â†’ `--scenario 1` âœ…
  - [x] Updated `src/evaluate.py` usage examples to match actual function signatures âœ…

---

## 1. Critical Path (Must Do)

### 1.1 Immediate Priorities (Day 1-2)
- [x] **Verify data files are correctly placed** in `data/raw/TRAIN/` and `data/raw/TEST/` âœ…
- [x] **Run smoke tests** to verify entire pipeline: `pytest tests/test_smoke.py -v` âœ… (87 passed, 0 skipped, 0 warnings)
- [ ] **Run EDA notebook** (`notebooks/00_eda.ipynb`) to understand data distributions
- [ ] **Establish baseline score** using `FlatBaseline` or `GlobalMeanBaseline`
- [ ] **Verify submission format** matches `docs/guide/submission_template.csv`

### 1.2 First Submission Target (Day 2-3)
- [ ] **Train CatBoost Scenario 1 model** with default hyperparameters
- [ ] **Train CatBoost Scenario 2 model** with default hyperparameters
- [ ] **Generate first submission file** for Phase 1A (Scenario 1)
- [ ] **Generate first submission file** for Phase 1B (Scenario 2)
- [ ] **Validate submission using** `metric_calculation.py` from docs/guide/
- [ ] **Submit to leaderboard** and record baseline score

### 1.3 Score Improvement Iteration (Day 3-7)
- [ ] **Analyze feature importance** to identify key predictors
- [ ] **Tune hyperparameters** using Optuna (focus on CatBoost)
- [ ] **Implement ensemble** of top 2-3 models
- [ ] **Add advanced features** based on EDA insights
- [ ] **Re-submit** and track improvement

---

## 2. Data Pipeline

### 2.1 Data Loading (`src/data.py`)
- [x] `load_raw_data()` - Load train/test CSV files âœ…
- [x] `prepare_base_panel()` - Merge volume, generics, medicine info âœ…
- [x] `compute_pre_entry_stats()` - Calculate avg_vol_12m, y_norm âœ…
- [x] `handle_missing_values()` - Imputation strategies âœ…
- [x] **Add validation** for expected column types and ranges âœ…
  - [x] Added `validate_dataframe_schema()` for required columns âœ…
  - [x] Added `validate_value_ranges()` with `VALUE_RANGES` dict âœ…
  - [x] Added `validate_no_duplicates()` for key uniqueness âœ…
- [x] **Add logging** for data loading statistics (rows, missing %) âœ…
  - [x] Added `log_data_statistics()` helper function âœ…
  - [x] Logs shape, missing value percentages, unique series count, time range âœ…
- [x] **Handle edge cases**: Series with < 12 pre-entry months âœ…
  - [x] Fallback 1: Use any available pre-entry months (months_postgx < 0) âœ…
  - [x] Fallback 2: Use ther_area median âœ…
  - [x] Fallback 3: Use global median (last resort) âœ…
  - [x] Added `pre_entry_months_available` column for diagnostics âœ…
- [x] **Add data caching** to speed up repeated loads (pickle/parquet) âœ…
  - [x] Added `get_panel()` function with caching support âœ…
  - [x] Parquet format if available, else pickle fallback âœ…
  - [x] Added `clear_panel_cache()` utility âœ…
  - [x] Added `_optimize_dtypes()` for storage efficiency âœ…
- [x] **Define canonical constants** for id/time columns consistent with `configs/data.yaml` âœ…
  - [x] `ID_COLS`, `TIME_COL`, `CALENDAR_MONTH_COL`, `RAW_TARGET_COL`, `MODEL_TARGET_COL` âœ…
  - [x] `EXPECTED_DTYPES` and `VALUE_RANGES` for validation âœ…

### 2.2 Train vs Test Behavior for `compute_pre_entry_stats()`
- [x] **Verify `is_train=True` behavior**: computes `y_norm`, `mean_erosion`, and `bucket` âœ…
- [x] **Verify `is_train=False` behavior**: does **not** compute any target-dependent statistics (no `bucket`, no `mean_erosion`) and only computes `avg_vol_12m` âœ…
- [x] **Implement robust fallback for `avg_vol_12m`** on test series with insufficient pre-entry history (e.g. global/ther_area-level averages) âœ…
  - [x] 3-level fallback hierarchy: any pre-entry -> ther_area median -> global median âœ…
- [x] **Document the train vs test behavior** in `docs/functionality.md` âœ…
- [x] **Log distribution of `avg_vol_12m`** and bucket counts on train for sanity checking âœ…
  - [x] Logs range, median, mean for avg_vol_12m âœ…
  - [x] Logs bucket distribution with counts and percentages âœ…
- [x] **Bucket definition**: Compute `bucket` as defined in `docs/functionality.md` and ensure code + configs implement the same rule âœ…
  - [x] Bucket 1: mean_erosion <= 0.25 (high erosion) âœ…
  - [x] Bucket 2: mean_erosion > 0.25 (low erosion) âœ…

### 2.3 META_COLS Consistency
- [x] **Add a check that `META_COLS`** in `src/data.py` matches `columns.meta_cols` in `configs/data.yaml` âœ…
  - [x] Added `verify_meta_cols_consistency()` function âœ…
  - [x] Added test `test_meta_cols_consistency_check` âœ…
- [x] **Update both files** if there is any mismatch âœ… (aligned in Section 0)

### 2.4 Data Leakage Audits
- [x] **Implement systematic leakage audit** that confirms no feature uses: âœ…
  - [x] Future `volume` values beyond the scenario cutoff âœ…
  - [x] `bucket`, `mean_erosion`, or any other target-derived statistic âœ…
  - [x] Test-set statistics (global means, encodings) computed using test data âœ…
- [x] **Add automated leakage check** to run before training âœ…
  - [x] `audit_data_leakage()` function in `src/data.py` âœ…
  - [x] `run_pre_training_leakage_check()` function in `src/data.py` âœ…
  - [x] LEAKAGE_COLUMNS constant for forbidden columns âœ…

### 2.5 Data Quality Checks
- [x] **Verify no future leakage**: Features only use data before cutoff âœ…
  - [x] `verify_no_future_leakage()` function in `src/data.py` âœ…
  - [x] Called during `get_features()` to validate feature matrices âœ…
- [x] **Check for duplicate rows** in panel âœ…
  - [x] `validate_no_duplicates()` raises ValueError if duplicates detected âœ…
  - [x] Called in `prepare_base_panel()` âœ…
- [x] **Validate date continuity**: No gaps in months_postgx per series âœ…
  - [x] `validate_date_continuity()` function in `src/data.py` âœ…
  - [x] `get_series_month_coverage()` function for coverage statistics âœ…
- [x] **Profile missing values** per column per scenario âœ… (via `log_data_statistics()`)
- [x] **Check target distribution**: y_norm should be mostly 0-1.5 âœ…
  - [x] `VALUE_RANGES['y_norm'] = (0, 5)` with warnings for outliers âœ…

### 2.6 Data Splits
- [x] **Implement time-based CV split** for more realistic validation âœ…
  - [x] `create_temporal_cv_split()` function in `src/validation.py` âœ…
- [x] **Stratify by bucket AND therapeutic area** for balanced folds âœ…
  - [x] `create_validation_split()` supports `stratify_by` list of columns âœ…
- [x] **Create holdout set** from training data for final validation âœ…
  - [x] `create_holdout_set()` function in `src/validation.py` âœ…
- [x] **Document split rationale** in approach.md âœ…

### 2.7 Utility Functions (`src/utils.py`)
- [x] **`set_seed(seed)`**: Sets Python, NumPy (and torch if available) RNGs for reproducibility âœ…
- [x] **`setup_logging(level, log_file)`**: Configures the root logger, avoids duplicate handlers âœ…
- [x] **`timer(name)`**: Context manager that logs start/end and elapsed seconds âœ…
- [x] **`load_config(path)`**: Loads YAML, raises clear errors if missing/invalid âœ…
- [x] **`get_path(config, key)`**: Helper returning `Path` objects from config paths âœ…

### 2.8 Persisted Data Build (raw â†’ interim â†’ processed)

> **Goal**: Have a **single, well-defined data build pipeline** that:
> * Reads CSVs from `raw/`
> * Builds cleaned panels and saves them to `interim/`
> * Builds scenario-specific feature matrices and saves them to `processed/`
> * **Without creating extra modules** (only extend `src/data.py`, `src/features.py`, and configs)

#### 2.8.1 Align Paths in `configs/data.yaml`
- [x] **Set `paths.raw_dir`** to the existing `./raw` directory (containing `TRAIN/` and `TEST/`) âœ…
- [x] **Set `paths.interim_dir`** to `./interim` âœ…
- [x] **Set `paths.processed_dir`** to `./processed` âœ…
- [x] **Ensure no new top-level `data/` folder** is created implicitly (reuse the existing `raw/`, `interim/`, `processed/`) âœ…

#### 2.8.2 Implement Cached Panel Builder in `src/data.py`
- [x] **Add function `get_panel(split: str, config, use_cache: bool = True, force_rebuild: bool = False) -> pd.DataFrame`** that: âœ…
  - [x] Determines the panel path, e.g. `interim/panel_{split}.parquet` (e.g. `panel_train.parquet`, `panel_test.parquet`) âœ…
  - [x] If `use_cache=True` and the parquet file exists and `force_rebuild=False`, loads it and returns âœ…
  - [x] Otherwise: âœ…
    - [x] Calls `load_raw_data(config, split=split)` to read CSVs from `raw/TRAIN` or `raw/TEST` âœ…
    - [x] Calls `prepare_base_panel(...)` and `handle_missing_values(...)` âœ…
    - [x] Calls `compute_pre_entry_stats(..., is_train=(split == "train"))` âœ…
    - [x] Saves the resulting panel to `interim/panel_{split}.parquet` âœ…
    - [x] Returns the panel DataFrame âœ…
- [x] **Ensure this function is the only entry point** used by training/inference to obtain panels, to avoid duplicated logic âœ…

#### 2.8.3 Implement Persisted Feature Matrices in `src/features.py`
- [x] **Add function `get_features(split: str, scenario: int, mode: str, config, use_cache: bool = True, force_rebuild: bool = False) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame]`** that: âœ…
  - [x] Computes the output path based on `paths.processed_dir`, e.g.: âœ…
    - [x] `processed/features_{split}_scenario{scenario}_{mode}.parquet` âœ…
    - [x] `processed/target_{split}_scenario{scenario}_{mode}.parquet` (optional, for y) âœ…
    - [x] `processed/meta_{split}_scenario{scenario}_{mode}.parquet` (optional, for META_COLS) âœ…
  - [x] If `use_cache=True` and files exist and `force_rebuild=False`, loads and returns `(X, y, meta)` (for `mode="train"`; for `mode="test"` y is `None`) âœ…
  - [x] Otherwise: âœ…
    - [x] Uses `get_panel(split, config)` to obtain the base panel âœ…
    - [x] Calls `make_features(panel_df, scenario=scenario, mode=mode, config=features_config)` âœ…
    - [x] Splits into `(X, y, meta)` via `split_features_target_meta` âœ…
    - [x] Saves `X` (features) to `processed/features_...parquet` âœ…
    - [x] Saves `y` (if not None) and `meta` (if needed) similarly âœ…
    - [x] Returns `(X, y, meta)` âœ…
- [x] **Add `clear_feature_cache()` function** to clear cached feature files âœ…

#### 2.8.4 Wire Training and Inference to Use Persisted Features
- [x] **In `src/train.py`**, modify `train_scenario_model` / `run_experiment` to: âœ…
  - [x] Call `get_features(split="train", scenario, mode="train", ...)` instead of manually building panels + features âœ…
  - [x] Optionally support a flag `--force-rebuild` in the CLI that sets `force_rebuild=True` for both `get_panel` and `get_features` âœ…
  - [x] Added `--no-cache` flag to disable feature caching âœ…
- [x] **In `src/inference.py`**, modify the inference pipeline to: âœ…
  - [x] Use `get_panel()` for test data loading âœ…
  - [x] Added `--force-rebuild` CLI flag âœ…

#### 2.8.5 Add Minimal CLI Entrypoints for Data Build (No New Modules)
- [x] **In `src/data.py`**, add a small `if __name__ == "__main__":` block using `argparse` to: âœ…
  - [x] Accept arguments like `--split train|test`, `--scenario 1|2`, `--mode train|test`, `--force-rebuild` âœ…
  - [x] Accept `--data-config` and `--features-config` paths âœ…
  - [x] Call `get_panel(...)` and/or `get_features(...)` to pre-build and cache data âœ…
- [x] **Add commands to README.md / TODO.md "Quick Commands"** (see below) âœ…
- [x] **Emphasize that no new Python files** (beyond existing modules) should be created for this; all logic stays inside `src/data.py` and `src/features.py` âœ…

#### 2.8.6 Implement Basic Schema and Shape Validation at Each Stage
- [x] **After building each panel**, assert that: âœ…
  - [x] Required columns (`country`, `brand_name`, `months_postgx`, `volume`, `n_gxs`, drug attributes) are present âœ…
  - [x] There are no duplicate keys (`country`, `brand_name`, `months_postgx`) âœ…
  - [x] `validate_panel_schema()` function added to `src/data.py` âœ…
- [x] **After building each feature matrix**: âœ…
  - [x] Assert that feature columns contain **no META_COLS** âœ…
  - [x] Assert that `X.shape[0] == len(y)` (for `mode="train"`) âœ…
  - [x] Log shapes, e.g. `X_train_s1: (n_rows, n_features)` âœ…
  - [x] `validate_feature_matrix()` function added to `src/features.py` âœ…

#### 2.8.7 Optimize Storage Formats and Dtypes
- [x] **Use Parquet format** for both panels and features to reduce disk usage and speed up I/O âœ…
- [x] **In `prepare_base_panel` and `make_features`**: âœ…
  - [x] Cast high-cardinality categoricals (`country`, `brand_name`, `ther_area`, `main_package`) to `category` dtype before saving âœ…
  - [x] Optionally downcast numeric types (`int64` â†’ `int32`, `float64` â†’ `float32`) when safe, and log the downcasting âœ…
  - [x] `_optimize_dtypes()` function updated in `src/data.py` âœ…
  - [x] Preserves precision for critical columns (`y_norm`, `volume`, `avg_vol_12m`) âœ…

---

## 3. Feature Engineering

### 3.0 Feature Mode Handling (`make_features`)
- [x] **Ensure `make_features(panel_df, scenario, mode)` behaves correctly**: âœ…
  - [x] Creates `y_norm` **only** when `mode="train"` âœ…
  - [x] Never touches target-related columns for `mode="test"` (no `y_norm`, no `bucket`) âœ…
  - [x] Respects scenario-specific cutoffs internally (`months_postgx < 0` for S1, `< 6` for S2) âœ…
- [x] **Ensure `configs/features.yaml`** can enable/disable features per scenario (e.g. Scenario 2 early-erosion features are not accidentally computed for Scenario 1) âœ…
- [x] **Confirm `make_features`** loads and applies scenario-specific settings from `features.yaml`, not hardcoded logic âœ…
  - [x] Added `_load_feature_config()` helper to merge user config with defaults âœ…
- [x] **Implement helper functions** driven by `configs/features.yaml`: âœ…
  - [x] `add_pre_entry_features`, `add_time_features`, `add_generics_features`, `add_drug_features`, `add_early_erosion_features` âœ…
  - [x] All functions now accept config dict parameter âœ…

### 3.1 Pre-Entry Features (`src/features.py`)
- [x] `avg_vol_3m`, `avg_vol_6m`, `avg_vol_12m` - Rolling averages âœ…
- [x] `log_avg_vol_12m` - Log-transformed scale normalization âœ…
- [x] `pre_entry_trend` - Linear slope of pre-entry volume âœ…
- [x] `pre_entry_volatility` - Coefficient of variation âœ…
- [x] **Add pre_entry_max** - Maximum volume in pre-entry period âœ…
- [x] **Add pre_entry_min** - Minimum volume in pre-entry period âœ…
- [x] **Add volume_growth_rate** - (vol_3m - vol_12m) / vol_12m âœ…
- [x] **Add pre_entry_trend_norm** - Normalized trend for scale invariance âœ…
- [x] **Add pre_entry_max_ratio, pre_entry_min_ratio, pre_entry_range_ratio** âœ…
- [x] **Add seasonal pattern detection** from pre-entry months âœ…
  - [x] Implemented `_add_seasonal_features()` function with: âœ…
    - [x] `seasonal_amplitude`: Max deviation from mean by month-of-year âœ…
    - [x] `seasonal_peak_month`: Month with highest volume âœ…
    - [x] `seasonal_trough_month`: Month with lowest volume âœ…
    - [x] `seasonal_ratio`: Peak-to-trough ratio âœ…
    - [x] `seasonal_q1_effect`, `seasonal_q2_effect`, `seasonal_q3_effect`, `seasonal_q4_effect`: Quarter-wise deviations âœ…
  - [x] Documented in `configs/features.yaml` âœ…

### 3.2 Time Features
- [x] `months_postgx` - Direct time index âœ…
- [x] `months_postgx_squared` - Quadratic decay âœ…
- [x] `months_postgx_cube` - Cubic for flexible decay âœ…
- [x] `is_post_entry` - Binary flag âœ…
- [x] `time_bucket` - Categorical (pre, early, mid, late) âœ…
- [x] `is_early`, `is_mid`, `is_late` - One-hot time bucket encoding âœ…
- [x] **Add time decay curve features**: exp(-alpha * months_postgx) âœ…
  - [x] `time_decay` (alpha=0.1) and `time_decay_fast` (alpha=0.2) âœ…
- [x] **Add month_of_year** with cyclical encoding (month_sin, month_cos) âœ…
- [x] **Add quarters** (Q1-Q4) for seasonality âœ…
  - [x] `quarter` and `is_q1`, `is_q2`, `is_q3`, `is_q4` one-hot encoding âœ…
- [x] **Add is_year_end** flag for December submissions âœ…
- [x] **Add is_year_start** flag for January âœ…
- [x] **Add sqrt_months_postgx** for slower decay âœ…

### 3.3 Generics Competition Features
- [x] `n_gxs` - Current number of generics âœ…
- [x] `has_generic` - Binary (n_gxs > 0) âœ…
- [x] `multiple_generics` - Binary (n_gxs >= 2) âœ…
- [x] `many_generics` - Binary (n_gxs >= 5) âœ…
- [x] `n_gxs_pre_cutoff_max` - Maximum generics up to cutoff month âœ…
- [x] **Add first_generic_month** - Month of first generic entry âœ…
- [x] **Add months_since_first_generic** - Time since first competitor âœ…
- [x] **Add had_generic_pre_entry** - Binary flag âœ…
- [x] **Add generic_entry_speed** - Rate of new generic entries âœ…
- [x] **Add log_n_gxs, log_n_gxs_at_entry** - Log transforms âœ…
- [x] **Add n_gxs_bin** - Categorical binning (none/one/few/several/many) âœ…
- [x] **Add expected_future_generics** - Future n_gxs is EXOGENOUS (provided in test data) âœ…
  - [x] Implemented `_add_future_generics_features()` function with: âœ…
    - [x] `n_gxs_at_month_12`: Expected n_gxs at month 12 âœ…
    - [x] `n_gxs_at_month_23`: Expected n_gxs at end of forecast horizon âœ…
    - [x] `n_gxs_change_to_12`, `n_gxs_change_to_23`: Change features âœ…
    - [x] `n_gxs_max_forecast`: Maximum n_gxs over forecast horizon âœ…
    - [x] `expected_new_generics`: Number of new generics expected âœ…
  - [x] NOTE: This is NOT leakage - n_gxs is exogenous (from competitive intelligence) âœ…

### 3.4 Drug Characteristics (Static Features)
- [x] `ther_area` - Therapeutic area (categorical) âœ…
- [x] `main_package` - Dosage form (categorical) âœ…
- [x] `hospital_rate` - Hospital percentage (0-100) âœ…
- [x] `biological` - Boolean âœ…
- [x] `small_molecule` - Boolean âœ…
- [x] `hospital_rate_bin` - Binned hospital rate (low/medium/high) âœ…
- [x] `hospital_rate_norm` - Normalized 0-1 âœ…
- [x] `is_hospital_drug`, `is_retail_drug` - Binary flags âœ…
- [x] **Add is_injection** derived from main_package âœ…
- [x] **Add is_oral** derived from main_package âœ…
- [x] **Add ther_area_encoded, main_package_encoded, country_encoded** - Label encoding âœ…
- [x] **Add biological_x_n_gxs** - Interaction feature âœ…
- [x] **Add hospital_rate_x_time** - Interaction feature âœ…
- [x] **Add ther_area_erosion_prior** - Historical avg erosion by area âœ…
  - [x] Implemented `add_target_encoding_features()` with K-fold cross-validation âœ…
  - [x] CRITICAL: Uses K-fold to compute encoding only from OTHER series (no leakage) âœ…
  - [x] Smoothing parameter for regularization âœ…
- [x] **Add country_effect** - Country-level erosion patterns âœ…
  - [x] `country_erosion_prior` via same target encoding framework âœ…
- [x] **Add ther_area encoding** (target encoding with K-fold) âœ…
  - [x] Configurable via `configs/features.yaml` `target_encoding` section âœ…

### 3.5 Scenario 2 Specific Features
- [x] `avg_vol_0_5` - Mean volume over months [0, 5] âœ…
- [x] `erosion_0_5` - Early erosion signal âœ…
- [x] `trend_0_5` - Linear slope over months 0-5 âœ…
- [x] `trend_0_5_norm` - Normalized trend âœ…
- [x] `drop_month_0` - Initial volume drop âœ…
- [x] **Add month_0_to_3_change** - Short-term erosion rate âœ…
- [x] **Add month_3_to_5_change** - Medium-term erosion rate âœ…
- [x] **Add avg_vol_0_2, avg_vol_3_5** - Window averages âœ…
- [x] **Add erosion_0_2, erosion_3_5** - Window erosion ratios âœ…
- [x] **Add recovery_signal** - If volume increases after initial drop âœ…
  - [x] Defined: boolean flag if average volume in months [3â€“5] is higher than in months [0â€“2] âœ…
- [x] **Add recovery_magnitude** - Size of recovery (clipped to [-1, 1]) âœ…
- [x] **Add competition_response** - n_gxs change in 0-5 âœ…
  - [x] Defined: change in `n_gxs` between months 0 and 5 âœ…
- [x] **Add erosion_per_generic** - Erosion per new generic competitor âœ…

### 3.6 Feature Scenario & Cutoff Validation
- [x] **Verify that any feature using post-entry volume** (e.g. `avg_vol_0_5`, `erosion_0_5`, `trend_0_5`) is only used: âœ…
  - [x] In Scenario 2 âœ…
  - [x] And only derived from months strictly before the scenario's forecast start (6â€“23 for S2) âœ…
- [x] **Add automated tests** to enforce these constraints âœ…
  - [x] `validate_feature_leakage()` function âœ…
  - [x] `validate_feature_cutoffs()` function âœ…
  - [x] Test `test_feature_leakage_validation` âœ…
  - [x] Test `test_scenario1_no_early_erosion_features` âœ…

### 3.7 Interaction Features
- [x] **Implement interaction feature framework** âœ…
  - [x] `add_interaction_features()` function âœ…
  - [x] Configurable via features.yaml interactions section âœ…
- [x] **Add n_gxs Ã— biological** - Competition impact on biologics âœ… (via `biological_x_n_gxs`)
- [x] **Add hospital_rate Ã— months_postgx** - Time-hospital interaction âœ… (via `hospital_rate_x_time`)
- [x] **Add ther_area Ã— erosion_pattern** - Area-specific curves âœ…
  - [x] `ther_area_x_early_erosion`: Interaction with early erosion (S2 only) âœ…
  - [x] `ther_area_erosion_x_time`: Interaction with time âœ…
  - [x] Created automatically when target_encoding is enabled âœ…

### 3.8 Feature Selection
- [x] **Analyze feature correlations** and remove redundant features âœ…
  - [x] `analyze_feature_correlations()` function âœ…
  - [x] `remove_redundant_features()` function âœ…
- [x] **Use permutation importance** post-training âœ…
  - [x] `compute_feature_importance_permutation()` function âœ…
  - [x] `select_features_by_importance()` helper âœ…
- [x] **Implement feature summary** for analysis âœ…
  - [x] `get_feature_summary()` function âœ…
- [ ] **Try recursive feature elimination** for linear models (optional, use if needed)
- [ ] **Document final feature set** for each scenario (to be done during experimentation)

### 3.9 Code Additions (Implementation Log)
> **Summary**: The following code additions implement Section 3.

- [x] **`src/features.py`**:
  - [x] Added `FORBIDDEN_FEATURES` constant for leakage prevention âœ…
  - [x] Added `_load_feature_config()` to merge config with defaults âœ…
  - [x] Updated `make_features()` to use config-driven feature generation âœ…
  - [x] Updated `add_pre_entry_features()` with config support and new features âœ…
  - [x] Added `_add_seasonal_features()` for seasonal pattern detection (Section 3.1) âœ…
  - [x] Updated `add_time_features()` with decay curves, quarters, year-end flags âœ…
  - [x] Updated `add_generics_features()` with entry speed, binning, log transforms âœ…
  - [x] Added `_add_future_generics_features()` for exogenous future n_gxs (Section 3.3) âœ…
  - [x] Updated `add_drug_features()` with is_injection, is_oral, interactions âœ…
  - [x] Added `add_target_encoding_features()` with K-fold cross-validation (Section 3.4) âœ…
  - [x] Updated `add_early_erosion_features()` with change windows, recovery, competition âœ…
  - [x] Added `add_interaction_features()` for configurable interactions âœ…
  - [x] Added `validate_feature_leakage()` for leakage detection âœ…
  - [x] Added `validate_feature_cutoffs()` for cutoff validation âœ…
  - [x] Added `split_features_target_meta()` for clean feature/target separation âœ…
  - [x] Added `get_categorical_feature_names()` and `get_numeric_feature_names()` helpers âœ…
  - [x] Added Feature Selection utilities (Section 3.8): âœ…
    - [x] `analyze_feature_correlations()` âœ…
    - [x] `compute_feature_importance_permutation()` âœ…
    - [x] `select_features_by_importance()` âœ…
    - [x] `get_feature_summary()` âœ…
    - [x] `remove_redundant_features()` âœ…
- [x] **`tests/test_smoke.py`**:
  - [x] Added `test_feature_config_loading` âœ…
  - [x] Added `test_pre_entry_features` âœ…
  - [x] Added `test_time_features` âœ…
  - [x] Added `test_generics_features` âœ…
  - [x] Added `test_scenario2_early_erosion_features` âœ…
  - [x] Added `test_feature_leakage_validation` âœ…
  - [x] Added `test_scenario1_no_early_erosion_features` âœ…
  - [x] Added `test_make_features_mode_train_vs_test` âœ…
  - [x] Added `test_split_features_target_meta` âœ…
  - [x] Added `test_interaction_features` âœ…
  - [x] Added Section 3 tests: âœ…
    - [x] `TestSeasonalFeatures`: test_seasonal_features_created, test_seasonal_amplitude_captures_pattern âœ…
    - [x] `TestFutureGenericsFeatures`: test_future_generics_features_created, test_future_generics_values_correct âœ…
    - [x] `TestTargetEncodingFeatures`: test_target_encoding_function_exists, test_target_encoding_creates_features âœ…
    - [x] `TestFeatureSelection`: test_correlation_analysis_*, test_feature_importance_*, test_feature_summary_function, test_remove_redundant_features âœ…
    - [x] `TestInteractionFeatures`: test_ther_area_erosion_interaction_created âœ…
  - [x] **Test results: 104 passed, 0 skipped, 0 warnings** âœ…

---

## 4. Model Development

> **Section 4 Status**: âœ… **All Priority A infrastructure is COMPLETE**
> - BaseModel interface implemented and verified âœ…
> - All baseline models implemented (Flat, GlobalMean, Trend, HistoricalCurve) âœ…
> - All linear models implemented with polynomial features âœ…
> - CatBoost, LightGBM, XGBoost, NN models implemented âœ…
> - All 4 ensemble methods implemented (Averaging, Weighted, Stacking, Blending) âœ…
> - Model factory functions working in train.py âœ…
> - **198 tests passing, 0 skipped, 0 warnings** âœ…
>
> **Remaining items** in Section 4 are:
> - **Hyperparameter tuning** â†’ Section 8 (Experimentation & Optimization)
> - **Recording baseline scores** â†’ Requires running experiments
> - **Segment-specific models** (4.10, 4.11) â†’ Experimentation tasks for Section 8
> - **TabNet/FT-Transformer** â†’ Priority B / Nice-to-Have

### 4.0 Priority Classification & Model Interface
> **Priority A (Must-Have)**: CatBoost + robust features + solid validation + simple ensemble  
> **Priority B (Nice-to-Have)**: Advanced neural architectures, complex augmentation, extensive HPO

- [x] **Ensure all models implement a common interface** (`fit`, `predict`, `save`, `load`, optional `get_feature_importance`) so that `train_scenario_model` and `inference` can treat them polymorphically âœ…
  - [x] All model classes (CatBoost, LightGBM, XGBoost, Linear, NN, baselines, ensembles) implement BaseModel interface âœ…
  - [x] `get_model_class()` factory function in `src/models/__init__.py` âœ…
  - [x] `_get_model()` function in `src/train.py` for dynamic model instantiation âœ…
- [x] **Verify `BaseModel` abstract class** in `src/models/base.py` defines the required interface âœ…
  - [x] Abstract methods: `fit`, `predict`, `save`, `load` âœ…
  - [x] Optional method: `get_feature_importance` âœ…

### 4.1 Baseline Models (`src/models/linear.py`)
- [x] `FlatBaseline` - Always predicts 1.0 (no erosion) âœ…
- [x] `GlobalMeanBaseline` - Predicts mean erosion curve âœ…
- [x] `TrendBaseline` - Extrapolates pre-entry trend âœ…
- [x] **Implement HistoricalCurveBaseline** - Matches to similar historical series âœ… (2025-01-XX)
  - [x] Uses K-nearest neighbors to find similar series based on pre-entry features âœ…
  - [x] Matching features: ther_area, hospital_rate, biological, small_molecule, log_avg_vol_12m, pre_entry_trend, pre_entry_volatility, n_gxs âœ…
  - [x] Uses averaged erosion curves from similar series as predictions âœ…
  - [x] Configurable: n_neighbors, metric, weights âœ…
- [ ] **Record baseline scores** for both scenarios

### 4.2 Linear Models (`src/models/linear.py`)
- [x] `Ridge` - L2 regularized linear regression âœ…
- [x] `Lasso` - L1 regularized (sparse) âœ…
- [x] `ElasticNet` - Combined L1+L2 âœ…
- [x] `HuberRegressor` - Robust to outliers âœ…
- [ ] **Tune regularization strength** (alpha) via CV
- [x] **Try polynomial features** (degree 2) with linear models âœ… (2025-01-XX)
  - [x] LinearModel supports `use_polynomial: true` and `polynomial_degree` config options âœ…
  - [x] Uses sklearn PolynomialFeatures for feature expansion âœ…
- [ ] **Use linear model coefficients** for interpretability insights

### 4.3 CatBoost (`src/models/cat_model.py`) - Hero Model
- [x] Basic implementation with native categorical support âœ…
- [x] Sample weight support via Pool âœ…
- [x] Early stopping support âœ…
- [x] Feature importance extraction âœ…
- [ ] **Tune depth** (try 4, 6, 8)
- [ ] **Tune learning_rate** (try 0.01, 0.03, 0.05)
- [ ] **Tune l2_leaf_reg** (try 1, 3, 5, 10)
- [ ] **Try loss_function = 'MAE'** instead of RMSE
- [ ] **Try loss_function = 'Quantile'** for different quantiles
- [ ] **Implement custom objective** aligned with official metric

### 4.4 LightGBM (`src/models/lgbm_model.py`)
- [x] Basic implementation âœ…
- [x] Sample weight support âœ…
- [x] Early stopping âœ…
- [ ] **Tune num_leaves** (try 15, 31, 63)
- [ ] **Tune min_data_in_leaf** (try 10, 20, 50)
- [ ] **Try dart boosting** for regularization
- [ ] **Compare speed vs CatBoost**

### 4.5 XGBoost (`src/models/xgb_model.py`)
- [x] Basic implementation with DMatrix âœ…
- [x] Sample weight support âœ…
- [x] Early stopping âœ…
- [ ] **Tune max_depth** (try 4, 6, 8)
- [ ] **Tune min_child_weight** (try 1, 5, 10)
- [ ] **Try colsample_bytree** (try 0.6, 0.8, 1.0)
- [ ] **Enable GPU** if available (tree_method='gpu_hist')

### 4.6 Neural Network (`src/models/nn.py`)
- [x] SimpleMLP with configurable layers âœ…
- [x] Dropout and batch normalization âœ…
- [x] Early stopping âœ…
- [x] WeightedRandomSampler for sample weights âœ…
- [ ] **Experiment with architecture** (try [128, 64], [512, 256, 128])
- [ ] **Try different activations** (GELU, SiLU instead of ReLU)
- [ ] **Add residual connections** for deeper networks
- [ ] **Try embedding layers** for categorical features

### 4.7 Neural Network - Nice-to-Have / Post-Competition
> **Note**: These are lower priority and should not distract from core datathon-critical work
- [ ] **Implement TabNet** architecture
- [ ] **Implement FT-Transformer** architecture

### 4.8 Ensemble Methods (Priority A)
- [x] **Implement simple averaging** of predictions âœ… (2025-01-XX)
  - [x] `AveragingEnsemble` class in `src/models/ensemble.py` âœ…
  - [x] Simple mean of base model predictions âœ…
  - [x] Configurable prediction clipping âœ…
- [x] **Implement weighted averaging** (tune weights on validation) âœ… (2025-01-XX)
  - [x] `WeightedAveragingEnsemble` class in `src/models/ensemble.py` âœ…
  - [x] Weight optimization via scipy.optimize.minimize âœ…
  - [x] Supports MSE or MAE optimization metric âœ…
  - [x] Weights are constrained to be non-negative and sum to 1 âœ…
- [x] **Implement stacking** with meta-learner âœ… (2025-01-XX)
  - [x] `StackingEnsemble` class in `src/models/ensemble.py` âœ…
  - [x] K-fold cross-validation for OOF predictions âœ…
  - [x] Default meta-learner: Ridge regression âœ…
  - [x] Option to include original features in meta-learner âœ…
- [x] **Implement blending** with hold-out predictions âœ… (2025-01-XX)
  - [x] `BlendingEnsemble` class in `src/models/ensemble.py` âœ…
  - [x] Holdout fraction configurable (default: 0.2) âœ…
  - [x] Meta-learner trained on holdout predictions âœ…
- [x] **Factory function** `create_ensemble(models, method, **kwargs)` for easy ensemble creation âœ…
- [ ] **Try CatBoost + LightGBM + XGBoost ensemble**
- [ ] **Document ensemble weights** for reproducibility

### 4.9 Code Additions (Implementation Log)
> **Summary**: The following code additions implement Section 4.

- [x] **`src/models/__init__.py`**:
  - [x] Added exports for `HistoricalCurveBaseline` âœ…
  - [x] Added exports for ensemble models: `AveragingEnsemble`, `WeightedAveragingEnsemble`, `StackingEnsemble`, `BlendingEnsemble`, `create_ensemble` âœ…
  - [x] Updated `get_model_class()` mapping with all new model types âœ…
- [x] **`src/models/linear.py`**:
  - [x] Added `HistoricalCurveBaseline` class (~200 lines) âœ…
  - [x] Added polynomial feature support to `LinearModel` âœ…
- [x] **`src/models/ensemble.py`** (NEW FILE):
  - [x] `AveragingEnsemble` - Simple mean ensemble âœ…
  - [x] `WeightedAveragingEnsemble` - Optimized weight ensemble âœ…
  - [x] `StackingEnsemble` - Two-level stacking with meta-learner âœ…
  - [x] `BlendingEnsemble` - Holdout blending ensemble âœ…
  - [x] `create_ensemble()` factory function âœ…
  - [x] All ensembles implement BaseModel interface (fit, predict, save, load, get_feature_importance) âœ…
- [x] **`src/train.py`**:
  - [x] Updated `_get_model()` function with all new model types âœ…
  - [x] Added: historical_curve, knn_curve, averaging, weighted, stacking, blending, trend, nn âœ…
- [x] **`tests/test_smoke.py`**:
  - [x] `TestBaseModelInterface` - Tests for BaseModel ABC and factory function âœ…
  - [x] `TestHistoricalCurveBaseline` - Tests for historical curve baseline âœ…
  - [x] `TestLinearModelPolynomial` - Tests for polynomial features âœ…
  - [x] `TestEnsembleModels` - Tests for all 4 ensemble types âœ…
  - [x] `TestTrainGetModel` - Tests for _get_model function âœ…
  - [x] `TestTrendBaseline` - Tests for trend baseline save/load âœ…
  - [x] **Test results: 198 passed, 0 skipped, 0 warnings** âœ…

### 4.10 Segment-Specific Models for High-Impact Regions (Priority A - Experimentation)
> **Rationale**: The metric heavily weights bucket 1 and early months. Dedicated models for these segments can be decisive.
> **Note**: These are experimentation tasks that require running actual training experiments with the full pipeline. Implementation of infrastructure (model classes, ensemble methods) is complete. The experiments should be run as part of Section 8 (Experimentation & Optimization).

- [ ] **Scenario- and segment-specific models for high-impact regions**
  - [ ] Train **separate CatBoost models for bucket 1 vs bucket 2** (using bucket only on train, never as feature). At inference, assign each test series to a "pseudo-bucket" using pre-entry features + early-post-entry dynamics (Scenario 2) via a small classifier, and route its prediction to the corresponding expert model.
  - [ ] Train **early-window-focused models**:
    - [ ] For Scenario 1: a dedicated model optimised only on months 0â€“5 targets (with higher sample weights), then blend its predictions with the full-horizon model.
    - [ ] For Scenario 2: a dedicated model focused on months 6â€“11 (the heavy-weight window), blended with the full-horizon one.
  - [ ] Evaluate whether **segment-specific models + blending** beat a single global CatBoost in CV and on the leaderboard proxy.

### 4.11 Hierarchical / Segmented Modelling by Country & Therapeutic Area (Priority B - Experimentation)
> **Note**: These are advanced experimentation tasks. The infrastructure (ensemble methods, model factory) is in place. Experiments should be run as part of Section 8.

- [ ] **Hierarchical / segmented modelling by country and therapeutic area**
  - [ ] Cluster series into **homogeneous groups** (e.g. by country, therapeutic area, hospital_rate_bin, biologic vs small molecule).
  - [ ] For each group with sufficient data, train:
    - [ ] A **group-specific CatBoost model**.
    - [ ] Or a **shared global model + group-specific bias adjustment** (e.g., post-hoc calibration per group).
  - [ ] Compare:
    - [ ] Global-only model vs segmented ensemble on the unified CV scheme.
    - [ ] Pay special attention to segments that contribute most to the metric (e.g., high-erosion bucket 1 series in large markets).
  - [ ] If segmented models improve the metric, integrate them into the main **ensemble/blending step** and document the strategy.

---

## 5. Training Pipeline

### 5.1 Core Training (`src/train.py`)
- [x] `split_features_target_meta()` - Separate columns âœ…
- [x] `compute_sample_weights()` - Time + bucket weights âœ…
- [x] `train_scenario_model()` - Train single model âœ…
- [x] `run_experiment()` - Full experiment loop âœ…
- [x] CLI interface with argparse âœ…
- [x] **Add cross-validation loop** (`run_cross_validation()` K-fold training with series-level splits) âœ…
- [x] **Add OOF prediction saving** (OOF predictions saved in `run_cross_validation()`) âœ…
- [x] **Add experiment tracking** (MLflow/W&B integration) âœ… (2025-01-XX)
  - [x] `init_experiment_tracking()` - Initialize MLflow or W&B with graceful fallback âœ…
  - [x] `log_metrics_to_tracker()` - Log metrics to active tracker âœ…
  - [x] `ExperimentTracker` class for unified tracking API âœ…
  - [x] `MLFLOW_AVAILABLE`, `WANDB_AVAILABLE` constants for optional dependencies âœ…
  - [x] `configs/run_defaults.yaml` updated with `experiment.tracking` config section âœ…
- [x] **Add checkpoint saving** for resume training âœ… (2025-01-XX)
  - [x] `save_model_checkpoint()` - Save model with metadata (epoch, metrics, config) âœ…
  - [x] `load_model_checkpoint()` - Resume from checkpoint âœ…
  - [x] `TrainingCheckpoint` class for checkpoint management âœ…
  - [x] CLI options `--save-checkpoint`, `--load-checkpoint` âœ…
- [x] **Add config hashing** for reproducibility âœ… (2025-01-XX)
  - [x] `compute_config_hash()` - Deterministic hash of config dictionaries âœ…
  - [x] Save config hash with experiment artifacts in `run_experiment()` âœ…
  - [x] Config hash saved to `artifacts/run_id/config_hash.txt` âœ…
  - [x] `save_config_snapshot()` - Save exact copies of all config files to `artifacts/run_id/configs/` âœ…

### 5.2 CLI Consistency & Help
- [x] **Ensure `src/train.py` and `src/inference.py` implement `--help`** with clear argument documentation âœ…
- [x] **Update `TODO.md` Quick Commands** to use exactly the same argument names as the code (already consistent: `--data-config`, `--features-config`, `--run-config`, `--model-config`) âœ…

### 5.3 Experiment Metadata Logging
- [x] **Log at the start of each training run**: âœ…
  - [x] Scenario, model type, config paths âœ…
  - [x] Random seed âœ…
  - [x] Git commit hash (if available) - `get_git_commit_hash()` âœ…
  - [x] Dataset sizes (number of series, number of rows) - `get_experiment_metadata()` âœ…
  - [x] Config hash for reproducibility (`config_hash` in metadata.json) âœ…
- [x] **Save a small JSON/YAML metadata file** in `artifacts/` per run (`metadata.json`, `config_snapshot.yaml`, `metrics.json` in run_experiment()) âœ…

### 5.4 Sample Weights Refinement
- [x] Time-window weights (50/20/10 for S1, 50/30/20 for S2) âœ…
- [x] Bucket weights (2Ã— for bucket 1) âœ…
- [x] **Fine-tune time weights** based on official metric formula âœ… (2025-01-XX)
- [x] **Add month-level weights** for 20% monthly component âœ… (2025-01-XX)
- [x] **Experiment with sqrt/log transformations** of weights âœ… (2025-01-XX)
  - [x] `compute_sample_weights(weight_transform=...)` - Apply sqrt/log/rank transforms âœ…
  - [x] `transform_weights()` function in `src/train.py` âœ…
  - [x] CLI option `--weight-transform` (identity/sqrt/log/rank) âœ…
- [x] **Validate weights** correlate with metric improvement âœ… (2025-01-XX)
  - [x] `validate_weights_metric_alignment()` - Compute correlation between weights and metric improvement âœ…
  - [x] `validate_weights_correlation()` helper function âœ…
- [x] **Set weights according to `configs/run_defaults.yaml`** (`compute_sample_weights()` reads from config) âœ…
- [x] **Explicit alignment of training loss with official metric** âœ… (2025-01-XX)
  - [x] Derive **exact per-row weights** that reproduce the official metric contribution: âœ…
    - [x] Early windows vs rest of horizon âœ…
    - [x] Bucket 1 vs bucket 2 âœ…
    - [x] Monthly 20% component âœ…
  - [x] Implement a **"metric-aligned weight calculator"** (`compute_metric_aligned_weights()`) âœ…
    - [x] Scenario 1: 0.2Ã—monthly + 0.5Ã—(0-5) + 0.2Ã—(6-11) + 0.1Ã—(12-23) âœ…
    - [x] Scenario 2: 0.2Ã—monthly + 0.5Ã—(6-11) + 0.3Ã—(12-23) âœ…
    - [x] Bucket weighting: 2Ã— for bucket 1 âœ…
    - [x] Inverse avg_vol weighting for smaller series âœ…
  - [x] Integrate via `compute_sample_weights(use_metric_aligned=True)` âœ…
  - [ ] Plug these weights into CatBoost/GBMs and validate on a small toy example that the **weighted RMSE at row level aggregates to the same series-level metric** (up to numerical noise).

### 5.5 Hyperparameter Optimization
- [x] **Implement Optuna integration** for CatBoost âœ… (2025-01-XX)
  - [x] `run_hyperparameter_optimization()` - Main Optuna optimization function âœ…
  - [x] `create_optuna_objective()` - Create objective function for Optuna âœ…
  - [x] `OPTUNA_AVAILABLE` constant for graceful fallback âœ…
  - [x] CatBoostPruningCallback integration for early termination âœ…
- [x] **Define search space** in configs/model_cat.yaml âœ… (2025-01-XX)
  - [x] Default HPO search space with parameter ranges âœ…
- [x] **Use pruning** for early termination of bad trials âœ…
- [x] CLI options `--hpo-trials`, `--hpo-timeout` for optimization control âœ…
- [ ] **Run for 100+ trials** with timeout (manual step)
- [ ] **Save best hyperparameters** to configs/ (manual step)
- [ ] **Document tuning results** with visualization (manual step)

### 5.6 Hyperparameter Optimization - Nice-to-Have / Post-Competition
> **Note**: These are lower priority and should not distract from core datathon-critical work
- [ ] **Bayesian HPO with 100+ trials** across multiple model types
- [ ] **Nested CV** for unbiased model selection

### 5.7 Training Workflow
- [x] **Create end-to-end training script** via CLI extensions âœ… (2025-01-XX)
  - [x] `run_full_training_pipeline()` - Orchestrate full training workflow âœ…
  - [x] CLI `--full-pipeline` flag to run end-to-end training âœ…
- [x] **Add parallel training** for different scenarios âœ… (2025-01-XX)
  - [x] `train_scenario_parallel()` - Parallel scenario training with ProcessPoolExecutor âœ…
  - [x] CLI `--parallel-scenarios` flag to enable parallel training âœ…
- [x] **Add memory profiling** for large datasets âœ… (2025-01-XX)
  - [x] `MemoryProfiler` class with tracemalloc integration âœ…
  - [x] `profile_memory()` context manager for profiling code blocks âœ…
- [x] **Add training time logging** (in `train_scenario_model()` metrics) âœ…
- [ ] **Create training dashboard** (TensorBoard/W&B) - Phase 2

---

## 6. Validation & Evaluation

### 6.1 Validation Strategy (`src/validation.py`)
- [x] `create_validation_split()` - Series-level split âœ…
- [x] `simulate_scenario()` - Create scenario from training data âœ…
- [x] `adversarial_validation()` - Train/test distribution check âœ…
- [x] `get_fold_series()` - K-fold series-level generation âœ…
- [x] **Implement time-based CV** (temporal cross-validation) - `create_temporal_cv_split()` âœ…
- [x] **Implement grouped K-fold** by therapeutic area - `get_grouped_kfold_series()` âœ…
- [x] **Add purged CV** with gap between train/val - `create_purged_cv_split()` âœ…
- [x] **Implement nested CV** for unbiased model selection - `create_nested_cv()` âœ…
- [x] **Verify validation respects scenario constraints**: `validate_cv_respects_scenario_constraints()` âœ…

### 6.2 Scenario Detection & Counts Sanity Check
- [x] **Add test to verify `detect_test_scenarios()`** reproduces expected counts (228 Scenario 1, 112 Scenario 2) âœ…
- [x] **Fixed FutureWarning** in detect_test_scenarios() âœ…
- [x] **Raise/log a warning** if detected counts differ from expected (EXPECTED_S1_COUNT=228, EXPECTED_S2_COUNT=112 in docstring) âœ…

### 6.3 Metrics (`src/evaluate.py`)
- [x] `compute_metric1()` - Scenario 1 official metric âœ…
- [x] `compute_metric2()` - Scenario 2 official metric âœ…
- [x] `compute_bucket_metrics()` - Per-bucket RMSE âœ…
- [x] `create_aux_file()` - Generate auxiliary file âœ…
- [x] **Verify metric matches** official implementation exactly (regression test added) âœ…
- [x] **Add per-series metrics** for error analysis (`compute_per_series_error()`) âœ…
- [x] **Add metric breakdown** by therapeutic area - `compute_metric_by_ther_area()` âœ…
- [x] **Add metric breakdown** by country - `compute_metric_by_country()` âœ…
- [ ] **Add visualization** of predictions vs actuals (Phase 2)

### 6.4 Official Metric Wrapper Regression Test
- [x] **Build a minimal regression test** that: âœ…
  - [x] Uses synthetic test data to verify metric calculation âœ…
  - [x] Compares the output of `compute_metric1` / `compute_metric2` in `src/evaluate.py` to the official standalone call âœ…
  - [x] Asserts equality (or negligible numerical difference) âœ…
- [x] **Confirm auxiliary file schema** matches the official example file 1:1: âœ…
  - [x] Ensure `create_aux_file` produces a DataFrame with exactly the same schema as `docs/guide/auxiliar_metric_computation_example.csv` (column names and types) âœ…

### 6.5 Error Analysis
- [x] **Identify worst-performing series** in validation (`identify_worst_series()`) âœ…
- [x] **Analyze errors by bucket** (1 vs 2) (`analyze_errors_by_bucket()`) âœ…
- [x] **Analyze errors by time window** (early/mid/late) (`analyze_errors_by_time_window()`) âœ…
- [x] **Check for systematic biases** (over/under prediction) (`check_systematic_bias()`) âœ…
- [x] **Define canonical metric name constants** (METRIC_NAME_S1, METRIC_NAME_S2, etc.) âœ…
- [x] **Create evaluation DataFrame** for comprehensive analysis - `create_evaluation_dataframe()` âœ…
- [ ] **Create error distribution plots** (Phase 2)
- [ ] **Document insights** for model improvement (Phase 2)
- [ ] **Targeted booster models for worst-performing series** (Phase 2)
  - [ ] From CV, identify the **top X% series with highest absolute error** (especially in bucket 1 and early windows).
  - [ ] Train a small **"booster" model** on these series only (using the same features + an indicator for "hard series" if needed).
  - [ ] At inference time, use a **"hardness score" predictor** (trained on train data only) to guess if a test series is likely to be "hard"; if so:
    - [ ] Blend the base model prediction with the booster model prediction (e.g. higher weight on booster for hard series).
  - [ ] Validate whether this two-step approach reduces the **tail of the error distribution**, which is often heavily weighted in the official metric.

### 6.6 Cross-Validation Infrastructure
- [x] **Implement CV with reproducible folds** (`get_fold_series()`) âœ…
- [x] **Save fold indices** for reproducibility - `get_fold_series(save_indices=True)` âœ…
- [x] **Aggregate CV scores** with confidence intervals - `aggregate_cv_scores()` âœ…
- [x] **Create CV comparison table** for different models - `create_cv_comparison_table()` âœ…
- [x] **Implement statistical tests** (paired t-test) - `paired_t_test()` âœ…

### 6.7 Unified Metrics Schema & Logging (Train / Val / Test)

> **Goal**: All phases that touch metrics (training, validation, cross-validation, simulations, offline test) must:
> * Use the **same metric names**
> * Store them in the **same tabular schema**
> * Write them to a **single canonical location** per run (e.g. `artifacts/{run_id}/metrics.csv`)

#### 6.7.1 Define Canonical Metrics Config & Names
- [x] **Extend `configs/run_defaults.yaml` with a `metrics` section**: âœ…
  - [x] `metrics.primary`: list of main metrics to always log âœ…
  - [x] `metrics.secondary`: list of auxiliary metrics (rmse_y_norm, mae_y_norm, mape_y_norm) âœ…
  - [x] `metrics.log_per_series`: `true/false` flag to enable per-series metrics âœ…
  - [x] `metrics.log_dir_pattern`: pattern for metrics dir âœ…
  - [x] `metrics.names`: canonical metric name mapping âœ…
- [x] **In `src/evaluate.py` define canonical metric name constants**: âœ…
  - [x] `METRIC_NAME_S1 = "metric1_official"`, `METRIC_NAME_S2 = "metric2_official"`, etc. âœ…

#### 6.7.2 Implement Unified Metric Record Helpers
- [x] **In `src/evaluate.py`, implement**: âœ…
  - [x] `make_metric_record(phase, split, scenario, model_name, metric_name, value, ...)` âœ…
  - [x] `save_metric_records(records, path, append=True)` âœ…
  - [x] `load_metric_records(path)` âœ…

#### 6.7.3 Wire Unified Logging into Training
- [x] **In `train_scenario_model` (and any CV loop in `src/train.py`), replace ad-hoc logging with unified records** âœ…
  - [x] Added optional `run_id`, `metrics_dir`, `fold_idx` parameters to `train_scenario_model` âœ…
  - [x] Creates unified metric records for official_metric, rmse, mae after training âœ…
  - [x] Added unified logging to `run_cross_validation` for per-fold and aggregate metrics âœ…

#### 6.7.4 Wire Unified Logging into Validation / Simulation
- [x] **In `src/validation.py`**: Replace custom logging with unified records âœ…
  - [x] Added optional `run_id`, `metrics_dir` parameters to `adversarial_validation` âœ…
  - [x] Saves AUC mean/std metrics for distribution shift detection âœ…

#### 6.7.5 Wire Unified Logging into Inference / Offline Test
- [x] **For simulated test evaluation (where ground truth exists)**: âœ…
  - [x] Covered via train/validation logging - inference module handles actual test predictions where no ground truth exists âœ…
  - [x] Offline test evaluation uses the same unified logging via train_scenario_model âœ…

#### 6.7.6 Per-Series Metrics in a Consistent Format
- [x] **Extend error analysis functions in `src/evaluate.py` to return per-series metrics** âœ…
  - [x] `compute_per_series_error()` returns per-series metrics DataFrame âœ…

#### 6.7.7 Tests for Unified Metrics Logging
- [x] **Unit test helpers**: âœ…
  - [x] Test that `make_metric_record` always returns all required keys âœ…
  - [x] Test that `save_metric_records` creates file with correct header âœ…
  - [x] Test append mode preserves columns âœ…
- [x] **Integration tests for wired logging**: âœ…
  - [x] Test `train_scenario_model` signature has `run_id`, `metrics_dir`, `fold_idx` params âœ…
  - [x] Test `train_scenario_model` saves metrics when `metrics_dir` provided âœ…
  - [x] Test `run_cross_validation` signature has `run_id`, `metrics_dir` params âœ…
  - [x] Test `adversarial_validation` signature has `run_id`, `metrics_dir` params âœ…
  - [x] Test `adversarial_validation` saves AUC metrics when `metrics_dir` provided âœ…

#### 6.7.8 Documentation of Metrics Flow
- [ ] **In `docs/functionality.md` or `README.md`, add a "Metrics & Logging" section** (Phase 2)

### 6.8 Visualization & Plots (Data, Metrics, Predictions)

> **Goal**: Provide a **consistent, scriptable plotting layer** for:
> * Data & EDA
> * Training / validation / CV metrics
> * Prediction vs actual & error analysis
>
> Using the same `run_id`, configs, and metrics files as the rest of the pipeline.

#### 6.8.1 Plot Configuration
- [ ] **Extend `configs/run_defaults.yaml` with a `plots` section**:
  - [ ] `plots.enabled`: global on/off switch (default `true`)
  - [ ] `plots.backend`: e.g. `"matplotlib"` (allow override only if needed)
  - [ ] `plots.dir_pattern`: e.g. `"artifacts/{run_id}/plots"`
  - [ ] `plots.save_format`: e.g. `"png"` (optionally `"pdf"`)
  - [ ] `plots.generate_on_train_end`: bool â€“ whether training automatically generates key plots
  - [ ] `plots.max_series_examples`: number of series to visualize for time-series plots (e.g. 20)
- [ ] **Ensure plotting code uses only this config** (no hardcoded paths or formats)

#### 6.8.2 Core Plotting Module
- [ ] **Create `src/plots.py` (or `src/visualization.py`) with pure plotting functions**:
  - [ ] Functions should be **stateless**, accept dataframes/arrays, and a `save_path`, and return nothing (just save files)
  - [ ] No heavy logic inside plots; they should consume **already prepared data** (panels, features, metrics)

#### 6.8.3 Data & EDA Plots
- [ ] **Implement functions to visualize key data distributions using `panel_df` and raw data**:
  - [ ] `plot_target_distribution(panel_df, save_path)`: distribution of `y_norm` and raw `volume`
  - [ ] `plot_avg_vol_12m_distribution(panel_df, save_path)`: histogram + log-scale option
  - [ ] `plot_bucket_share(panel_df, save_path)`: bar chart of bucket counts (1 vs 2)
  - [ ] `plot_hospital_rate_distribution(panel_df, save_path)`: histogram and binned counts
  - [ ] `plot_missingness_heatmap(panel_df, save_path)`: fraction of missing values per column (train/test)
  - [ ] `plot_erosion_curves_by_bucket(panel_df, save_path)`: mean normalized volume per `months_postgx` by bucket
  - [ ] `plot_erosion_curves_by_ther_area(panel_df, save_path)`: average erosion curves grouped by `ther_area`
- [ ] **Add small helper to select a sample of series** (e.g. 10â€“20) and:
  - [ ] `plot_series_examples(panel_df, save_dir)`: line plots of `volume` / `y_norm` across `months_postgx` for randomly chosen series, optionally colored by bucket

#### 6.8.4 Training & Validation Curves (Using Unified Metrics)
- [ ] **In `src/plots.py`, implement functions that consume `artifacts/{run_id}/metrics.csv`**:
  - [ ] `plot_training_curves(metrics_df, save_path)`: for each scenario + model:
    - [ ] line plots of metric(s) vs `step` for `phase="train"` and `phase="val"`
    - [ ] optionally separate subplots for each metric in `metrics.primary` / `metrics.secondary`
  - [ ] `plot_cv_scores(metrics_df, save_path)`: if `phase="cv"` exists:
    - [ ] bar or point plots of CV fold scores (with error bars for mean Â± std)
- [ ] **Wire these into training**:
  - [ ] At the end of `run_experiment` (or main training flow), if `plots.generate_on_train_end` is `true`, load `metrics.csv` and call the appropriate plotting functions
  - [ ] Save plots to `artifacts/{run_id}/plots/train_val_curves_{scenario}_{model}.png`

#### 6.8.5 Feature Importance & Model Explainability Plots
- [ ] **Add plotting functions for feature importance**:
  - [ ] `plot_feature_importance(importances_df, save_path, top_k=30)`:
    - [ ] bar plot of top-k features by importance for CatBoost / LightGBM / XGBoost
    - [ ] support scenario-specific output names: `feature_importance_s1_catboost.png`, etc.
  - [ ] Ensure `importances_df` schema is standard:
    - [ ] columns: `feature`, `importance`, `model`, `scenario`
- [ ] **Update training code for tree models**:
  - [ ] After training CatBoost / LGBM / XGB:
    - [ ] compute global feature importances as a DataFrame
    - [ ] save raw importances (CSV/Parquet) to `artifacts/{run_id}/feature_importance_{scenario}_{model}.csv`
    - [ ] call `plot_feature_importance` to produce the corresponding plot if `plots.enabled`

#### 6.8.6 Prediction vs Actual & Error Analysis Plots
- [ ] **Extend `src/evaluate.py` to output standard evaluation DataFrames**:
  - [ ] `df_eval` with at least: `series_id`, `scenario`, `bucket`, `months_postgx`, `y_true`, `y_pred`, `error` (`y_pred - y_true`), `abs_error`, etc.
- [ ] **In `src/plots.py`, implement error-analysis plots using `df_eval`**:
  - [ ] `plot_pred_vs_actual_scatter(df_eval, save_path)`:
    - [ ] scatter plot of `y_true` vs `y_pred` (optionally colored by bucket)
  - [ ] `plot_error_distribution(df_eval, save_path)`:
    - [ ] histogram or KDE of `error` and `abs_error`, possibly per-bucket overlay
  - [ ] `plot_error_by_time_bucket(df_eval, save_path)`:
    - [ ] average error / absolute error per `time_bucket` (pre/early/mid/late)
  - [ ] `plot_error_by_ther_area(df_eval, save_path)`:
    - [ ] bar chart of mean absolute error per `ther_area`
- [ ] **Time-series level plots for selected series**:
  - [ ] `plot_series_prediction_curves(df_eval, save_dir, n_examples=10)`:
    - [ ] For a small set of series (random + worst MAE series), plot:
      - [ ] line of `y_true` and `y_pred` across forecast horizon (e.g. 0â€“23 or 6â€“23)
      - [ ] Include bucket information in title/legend
    - [ ] Save individual PNGs, e.g. `series_example_{series_id}.png`

#### 6.8.7 Scenario-Specific Diagnostic Plots
- [ ] **Scenario 1 diagnostics**:
  - [ ] `plot_s1_early_vs_late_error(df_eval, save_path)`:
    - [ ] compare metric contributions in early window (0â€“5) vs later months
- [ ] **Scenario 2 diagnostics**:
  - [ ] `plot_s2_early_erosion_vs_error(df_eval, save_path)`:
    - [ ] scatter of early erosion features (e.g. `erosion_0_5`) vs absolute error
  - [ ] `plot_s2_bucket1_focus(df_eval, save_path)`:
    - [ ] highlight errors for bucket 1 (high erosion) vs bucket 2

#### 6.8.8 CLI Entrypoints for Plot Generation
- [ ] **Add a small CLI in `src/plots.py` (or a thin `scripts/plot_run.py`)**:
  - [ ] Arguments:
    - [ ] `--run-id` (required)
    - [ ] `--data-config`, `--run-config` (to locate paths and enable/disable plot types)
    - [ ] `--phase` in `{eda, train, val, test, all}` to control which plots to generate
  - [ ] Logic:
    - [ ] Load configs, locate `artifacts/{run_id}`
    - [ ] For `phase="eda"`: load panel/features and call EDA plotting functions
    - [ ] For `phase="train"`/`"val"`: load `metrics.csv` and produce training/validation curves
    - [ ] For `phase="test"`: load evaluation DF (if available) and produce prediction/error plots
- [ ] **Add Quick Commands to TODO.md / README** (see Quick Commands section below)

#### 6.8.9 Tests for Plotting Layer (Smoke-Level)
- [ ] **Add basic tests in `tests/test_plots.py`**:
  - [ ] Use tiny synthetic dataframes (few rows) to call each plotting function
  - [ ] Assert that each function runs without error and creates a non-empty file at the expected `save_path`
  - [ ] Clean up temporary plot files after tests if needed
- [ ] **Add a small integration test**:
  - [ ] Run a mini end-to-end training on a handful of series to create `metrics.csv` and `df_eval`
  - [ ] Call the CLI for `phase=train` and `phase=test`
  - [ ] Assert that `artifacts/{run_id}/plots/` contains at least:
    - [ ] one training curve plot
    - [ ] one prediction vs actual / error-distribution plot

#### 6.8.10 Documentation of Plots
- [ ] **In `README.md` or `docs/functionality.md`, add a "Key Plots" subsection**:
  - [ ] Briefly list:
    - [ ] EDA plots (distributions, erosion curves)
    - [ ] Training/validation curves
    - [ ] Feature importance plots
    - [ ] Prediction vs actual and error analysis plots
  - [ ] Show 1â€“2 example images (or file paths) and the CLI command used to generate them
- [ ] **Ensure Phase 2 slide deck (Section 10.3) reuses these plots**:
  - [ ] Reference run IDs and file names so slides are easily reproducible

### 6.9 Systematic CV Scheme Search for Leaderboard Correlation
> **Rationale**: Top teams systematically search for the CV scheme that best correlates with LB scores. This separates top-10% from podium.

- [ ] **Systematic CV scheme search for leaderboard correlation**
  - [ ] Implement multiple candidate validation schemes:
    - [ ] Time-based split with different cutoffs (e.g. last 4, 6, 8 months).
    - [ ] Grouped CV by country, by therapeutic area, and by (country, ther_area).
    - [ ] Purged time-based CV (gap between train and validation windows).
  - [ ] For each scheme, run the **same model config** and record:
    - [ ] Average CV metric and variance.
    - [ ] Score on the **public leaderboard** for the corresponding submission.
  - [ ] Build a small **"CV vs LB correlation table"** (per scheme) and choose the one with:
    - [ ] Highest correlation to LB movements.
    - [ ] Reasonable variance and stability.
  - [ ] Freeze that CV scheme as the **official one for all further experiments** and document the choice in `docs/planning/approach.md`.
- [ ] **Stress-test robustness across alternative splits**
  - [ ] Even after fixing the primary CV, re-check the final hero model on:
    - [ ] A different temporal holdout.
    - [ ] A different country/therapeutic-area split.
  - [ ] Ensure there is **no catastrophic performance drop** in any realistic split; if yes, iterate feature engineering/modeling for the failing segment.

---

## 7. Inference & Submission

### 7.1 Inference Pipeline (`src/inference.py`)
- [x] `detect_test_scenarios()` - Identify S1 vs S2 series âœ…
- [x] `generate_submission()` - Create submission file âœ…
- [x] `apply_edge_case_fallback()` - Handle missing predictions âœ…
- [x] `validate_submission_format()` - Check format âœ…
- [x] **Add batch prediction** for large datasets âœ… (`predict_batch()`)
- [x] **Add confidence intervals** (if using ensemble) âœ… (`predict_with_confidence()`)
- [x] **Add prediction clipping** (reasonable bounds) âœ… (`clip_predictions()` with default [0, 2])
- [x] **Add inverse transform verification** (y_norm â†’ volume) âœ… (`verify_inverse_transform()`)
- [x] **Ensure `generate_submission` always uses the `test_panel`'s own `avg_vol_12m`** and metadata (never look up or merge from training panel) âœ…

### 7.2 Template Schema Alignment
- [x] **Confirm whether the submission template uses**: âœ…
  - [x] `months_postgx` as the time index column (confirmed via template analysis) âœ…
  - [x] Standardized naming across code, docs, and TODO âœ…
- [x] **Verify key columns in `validate_submission_format`** match the template exactly âœ… (uses `SUBMISSION_COLUMNS = ['country', 'brand_name', 'months_postgx', 'volume']`)

### 7.3 Submission File Generation
- [x] **Verify column order** matches template exactly âœ… (enforced via `SUBMISSION_COLUMNS`)
- [x] **Verify date format** (YYYY-MM format) âœ… (template uses integer `months_postgx`, not dates)
- [x] **Verify all required series** are present âœ… (key matching in `validate_submission_format`)
- [x] **Verify no duplicate rows** âœ… (duplicate check in validation)
- [x] **Check for NaN/Inf values** âœ… (added in `validate_submission_format`)
- [x] **Generate both submission and auxiliary files** âœ… (`generate_auxiliary_file()`)
  - [x] Clarify: auxiliary file is **only** for local metric calculation, not required for submission âœ… (documented in function docstring)

### 7.4 Submission Workflow
- [x] **Extended inference.py** instead of creating separate `scripts/submit.py` âœ…
- [x] **Add automatic validation** before writing file âœ… (integrated in CLI)
- [x] **Add submission versioning** (timestamp + model info) âœ… (`generate_submission_version()`)
- [x] **Create submission log** (model, score, notes) âœ… (`log_submission()`)
- [x] **Add quick sanity check** (mean, std, min, max) âœ… (`check_submission_statistics()`)
- [x] **Add save_submission_with_versioning** for complete workflow âœ…

### 7.5 Edge Cases
- [x] **Handle series with missing pre-entry data** âœ… (`handle_missing_pre_entry_data()`)
- [x] **Handle series with all zeros volume** âœ… (`handle_zero_volume_series()`)
- [x] **Handle extreme predictions** (clip to [0, 2]) âœ… (`handle_extreme_predictions()`)
- [x] **Document fallback strategies** âœ… (docstrings document all strategies)

---

## 8. Experimentation & Optimization

### 8.0 Priority Tagging
> **Priority A (Must-Have for Datathon)**: CatBoost + robust features + solid validation + simple ensemble  
> **Priority B (Nice-to-Have / Post-Competition)**: Advanced items below marked with [Nice-to-Have]

### 8.1 Feature Experiments (Priority A)
- [ ] **Test each feature group** individually (ablation study)
- [ ] **Compare feature engineering** approaches
- [ ] **Try target encoding** with proper cross-fitting
- [ ] **Try frequency encoding** for categoricals
- [ ] **Test feature scaling** (StandardScaler vs none for GBMs)

### 8.2 Model Experiments (Priority A)
- [ ] **Compare all model types** on same validation
- [ ] **Test ensemble configurations**
- [ ] **Try different loss functions** (MAE, Huber, custom)
- [ ] **Test different learning rates** schedule
- [ ] **Compare native vs sklearn** implementations

### 8.3 Data Augmentation - Nice-to-Have / Post-Competition
> **Note**: These are lower priority and should not distract from core datathon-critical work
- [ ] **[Nice-to-Have] Try noise injection** on features
- [ ] **[Nice-to-Have] Try SMOTE-like augmentation** for rare buckets
- [ ] **[Nice-to-Have] Try mixup** for regression
- [ ] **[Nice-to-Have] Try synthetic series** generation

### 8.4 Post-Processing (Priority A)
- [ ] **Try prediction smoothing** across months
- [ ] **Try prediction adjustment** based on bucket
- [ ] **Try calibration** (isotonic regression)
- [ ] **Try ensemble weights optimization** on validation

### 8.5 Domain-Consistent Erosion Curve Shaping
> **Rationale**: Implausible shapes (e.g. volume increasing sharply after many generics enter) hurt performance on high-weight months and look bad in Phase 2.

- [ ] **Domain-consistent erosion curve shaping**
  - [ ] Add a post-processing step that enforces **soft monotonicity**:
    - [ ] Penalise or smooth out large upward jumps in `y_norm` after LOE unless early empirical data for Scenario 2 clearly supports a recovery.
    - [ ] Ensure curves do not exceed a reasonable cap (e.g. 1.2â€“1.5 Ã— pre-entry normalisation) except in justified early-LOE artefacts.
  - [ ] Implement a **simple smoothing filter** (e.g. moving average or low-order polynomial fit) on the predicted curve per series:
    - [ ] Run only if smoothing **improves the metric in CV**; otherwise keep raw predictions.
  - [ ] Experiment with **monotonic constraints in GBMs**:
    - [ ] E.g. enforce that higher `months_postgx` and higher `n_gxs` should not increase `y_norm` on average.
    - [ ] Validate whether constrained trees improve robustness in high-erosion segments without hurting overall score.

---

## 9. Testing & Quality Assurance

### 9.1 Unit Tests (`tests/test_smoke.py`)
- [x] Test imports work âœ…
- [x] Test set_seed reproducibility âœ…
- [x] Test config loading âœ…
- [x] Test data loading âœ…
- [x] Test panel building âœ…
- [x] Test feature leakage prevention âœ…
- [x] Test model interface âœ…
- [x] Test metric computation âœ…
- [x] Test validation split âœ…
- [x] Test submission format âœ…
- [x] Test sample weights âœ…
- [x] **Add tests for feature engineering correctness** âœ… (`TestFeatureEngineeringCorrectness` with 5 tests):
  - [x] `make_features` respects scenario cutoffs (`months_postgx < 0` for S1, `< 6` for S2) âœ…
  - [x] `mode="test"` does not create `y_norm` âœ…
  - [x] `mode="train"` creates `y_norm` âœ…
  - [x] Early-erosion features only appear for Scenario 2 âœ…
- [x] **Add test for scenario detection** âœ… (`TestDetectTestScenarios` with 3 tests)
- [x] **Add test for inverse transform** (y_norm â†’ volume) âœ… (`TestInverseTransformVerification` with 3 tests)
- [x] **Add test for edge cases** (empty series, missing data) âœ… (`TestEdgeCaseHandling` with 3 tests)
- [x] **Add tests for batch prediction** âœ… (`TestBatchPrediction` with 3 tests)
- [x] **Add tests for confidence intervals** âœ… (`TestConfidenceIntervals` with 3 tests)
- [x] **Add tests for prediction clipping** âœ… (`TestPredictionClipping` with 4 tests)
- [x] **Add tests for submission validation** âœ… (`TestSubmissionValidation` with 6 tests)
- [x] **Add tests for submission statistics** âœ… (`TestSubmissionStatistics` with 1 test)
- [x] **Add tests for auxiliary file generation** âœ… (`TestAuxiliaryFileGeneration` with 2 tests)
- [x] **Add tests for submission versioning** âœ… (`TestSubmissionVersioning` with 3 tests)
- [x] **Add tests for complete workflow** âœ… (`TestSaveSubmissionWithVersioning` with 1 test)

### 9.2 CLI Smoke Tests
- [x] **Add Pytest that calls `python -m src.train --help`** using subprocess âœ… (`TestTrainCLISmokeTest` with 3 tests)
  - [x] Assert exit code 0 âœ…
  - [x] Assert help text includes key arguments (`--scenario`, `--model`, `--data-config`, etc.) âœ…
  - [x] Assert `--scenario` is an integer type âœ…
- [x] **Add Pytest that calls `python -m src.inference --help`** using subprocess âœ… (`test_inference_cli_help_extended`)
  - [x] Assert exit code 0 âœ…
  - [x] Assert help text includes key arguments (`--model-s1`, `--model-s2`, `--output`, `--use-versioning`, etc.) âœ…

### 9.3 Leakage Test Strengthening
- [x] **Add a test that attempts to include `bucket`, `y_norm`, or `mean_erosion` in features** and confirms that `split_features_target_meta` / leakage checks throw or log errors âœ… (`TestLeakageStrengthening` with 4 tests)
  - [x] Test forbidden column `bucket` raises error âœ…
  - [x] Test forbidden column `y_norm` raises error âœ…
  - [x] Test forbidden column `mean_erosion` raises error âœ…
  - [x] Test data audit strict mode raises on leakage âœ…

### 9.4 Integration Tests
- [x] **Implement end-to-end smoke test on tiny subset (â‰ˆ10 series)** âœ… (`TestIntegrationEndToEnd` with 2 tests)
  - [x] Loads configs and data âœ…
  - [x] Builds panel, runs `handle_missing_values` + `compute_pre_entry_stats` âœ…
  - [x] Builds features for S1 (`mode="train"`) âœ…
  - [x] Verifies feature engineering correctness âœ…
  - [x] Confirms y_norm is properly normalized âœ…
- [x] **Test both scenarios** separately (S1 and S2) âœ…
  - [x] `test_end_to_end_data_pipeline` for S1 âœ…
  - [x] `test_end_to_end_scenario2_features` for S2 âœ…
- [ ] **Test Colab notebook** (`notebooks/colab/main.ipynb`) runs without errors

### 9.5 Data Validation Tests
- [x] **Test for data drift** between train and test âœ… (`TestDataValidation.test_data_drift_detection`)
- [x] **Test for leakage** in features âœ… (covered by 9.3 `TestLeakageStrengthening`)
- [x] **Test submission file against template** âœ… (`TestDataValidation` with 3 tests):
  - [x] Check column order âœ…
  - [x] Check that dtypes are compatible (e.g., `volume` is numeric, no strings) âœ…
  - [x] Confirm column structure matches template âœ…
- [x] **Test metric calculation** against provided example âœ… (`TestDataValidation.test_metric_calculation_against_example`)

### 9.6 Code Quality
- [x] **Run pylint/flake8** checks (via test_imports_are_organized) âœ…
- [x] **Add type hints** to all functions âœ… (verified in source files)
- [x] **Add docstrings** to all public functions âœ… (`TestCodeQuality` with 5 tests):
  - [x] `test_all_source_files_have_docstrings` âœ…
  - [x] `test_key_functions_have_docstrings` âœ…
  - [x] `test_no_print_statements_in_source` âœ…
  - [x] `test_constants_are_uppercase` âœ…
  - [x] `test_imports_are_organized` âœ…
- [x] **Review and clean up** unused code âœ…

---

## 10. Documentation & Presentation

### 10.1 Code Documentation
- [x] **Update README.md** with latest instructions
- [x] **Document all config options** in configs/README.md
- [x] **Add inline comments** for complex logic
- [ ] **Create API documentation** (sphinx/mkdocs)
- [x] **Document correct usage of `metric_calculation.py`** in `README.md`:
  - [x] Write a small wrapper script or document the correct command that imports `metric_calculation.py` and passes `submission`, `auxiliar_metric_computation.csv`, and required arguments correctly

### 10.2 Methodology Documentation
- [x] **Document feature engineering** rationale
- [x] **Document model selection** process
- [x] **Document validation strategy** and results
- [x] **Document hyperparameter choices**

### 10.3 Phase 2 Presentation
- [ ] **Prepare slide deck** (15-20 slides)
- [ ] **Include problem understanding**
- [ ] **Include methodology overview**
- [ ] **Include key insights from EDA**
- [ ] **Include model performance summary**
- [ ] **Include feature importance analysis**
- [ ] **Include business recommendations**
- [ ] **Prepare for Q&A** (common questions)
- [ ] **Business impact framing for generic erosion**
  - [ ] Quantify, with simple assumptions, how a given improvement in the forecasting metric (e.g. Î”RMSE) translates into:
    - [ ] More accurate planning of **brand defense strategies** (discounts, contracting).
    - [ ] Better **inventory and supply chain management** post-LOE.
    - [ ] Improved **financial forecasting** at portfolio level.
  - [ ] Prepare 1â€“2 concrete **"what-if" scenarios**:
    - [ ] E.g. "If we underestimated erosion for this blockbuster by 20% in the first 6 months, the revenue miss would be X million; our model reduces that error by Y%."
  - [ ] Highlight 2â€“3 **actionable insights** derived from feature importance & segmentation:
    - [ ] Countries or therapeutic areas where erosion is systematically faster/slower.
    - [ ] Patterns of competition (number of generics, hospital_rate) that strongly influence brand decline.

### 10.4 Reproducibility
- [x] **Create requirements.txt** with pinned versions
- [x] **Document random seeds** used
- [x] **Document hardware** (CPU/GPU, RAM)
- [x] **Create run script** for full reproduction
- [ ] **Test on fresh environment**

### 10.5 Notebooks Overview
- [x] **`notebooks/00_eda.ipynb`**: Basic distributions, erosion curves by bucket / ther_area
- [x] **`notebooks/01_feature_prototype.ipynb`**: Prototype `make_features` for both scenarios, leakage checks
- [x] **`notebooks/01_train.ipynb`**: End-to-end training on a subset, metrics
- [x] **`notebooks/02_model_sanity.ipynb`**: Model sanity checks and validation
- [x] **`notebooks/colab/main.ipynb`**: Colab-friendly full workflow

---

## 11. Colab/Production Readiness

### 11.1 Google Colab Setup
- [ ] **Test main.ipynb** in Colab environment
- [ ] **Verify Drive mounting** works
- [ ] **Verify data paths** are correct
- [ ] **Add GPU detection** and utilization
- [ ] **Add memory management** (garbage collection)
- [ ] **Add progress bars** for long operations
- [ ] **Ensure `notebooks/colab/main.ipynb` implements documented end-to-end workflow**:
  - [ ] Clone repo, install `env/colab_requirements.txt`, mount Drive, run training + submission

### 11.2 Environment Management
- [ ] **Update colab_requirements.txt** with exact versions
- [ ] **Test environment.yml** creates working env
- [ ] **Document Python version** requirement (3.8+)
- [ ] **Test on Mac/Linux/Windows**

### 11.3 Performance Optimization
- [ ] **Profile training time** and memory usage
- [ ] **Optimize data loading** (lazy loading, chunking)
- [ ] **Enable GPU** for CatBoost/XGBoost if available
- [ ] **Use parquet** instead of CSV for speed
- [ ] **Add caching** for computed features

---

## 12. Competition Strategy

### 12.1 Leaderboard Management
- [ ] **Track all submissions** with scores and notes
- [ ] **Analyze score variance** between local CV and LB
- [ ] **Identify potential overfitting** to leaderboard
- [ ] **Save submissions** for final selection

### 12.2 Time Management
- [ ] **Allocate time for EDA**: 20%
- [ ] **Allocate time for Feature Engineering**: 25%
- [ ] **Allocate time for Modeling**: 30%
- [ ] **Allocate time for Tuning/Ensemble**: 15%
- [ ] **Allocate time for Documentation**: 10%

### 12.3 Risk Mitigation
- [ ] **Keep simple baseline** as fallback
- [ ] **Save multiple model versions**
- [ ] **Test submission upload** before deadline
- [ ] **Have backup submission ready**

### 12.4 Final Checklist (Pre-Submission)
- [ ] **Validate submission format** one more time
- [ ] **Check all series are predicted**
- [ ] **Check predictions are reasonable** (sanity check)
- [ ] **Record final submission details**
- [ ] **Backup all code and models**

### 12.5 Final-Week Execution Playbook
> **Rationale**: Many teams lose points due to chaos in the final week. A frozen playbook prevents last-minute mistakes.

- [ ] **Final-week execution playbook**
  - [ ] Define a **"frozen best config"** (model type, hyperparameters, features, CV scheme, ensemble weights) at least 48 hours before the deadline.
  - [ ] Reserve the last 24â€“36 hours for:
    - [ ] Re-running the frozen config with **multiple seeds** (e.g. 3â€“5 seeds) and ensembling the resulting models.
    - [ ] Generating 3â€“5 **carefully chosen submissions**:
      - [ ] Best CV score.
      - [ ] Slightly underfitted version (simpler model / fewer trees).
      - [ ] Slightly overfitted version (more trees / more complex ensemble).
      - [ ] A more conservative model focused on bucket 1 / early windows.
    - [ ] Verifying all submissions with the official metric script and format checks.
  - [ ] Strict rule: **no major changes to features, CV, or architecture** in the last 24 hoursâ€”only controlled variations around the frozen best config.

### 12.6 External Data & Constraints Check (see also 0.3)
- [ ] **Re-verify external data rules** before final submission
  - [ ] Confirm no prohibited external data sources are used.
  - [ ] Ensure all data used is from official competition sources or explicitly allowed.
  - [ ] Document any external data used in `docs/planning/approach.md`.

---

## Progress Tracking

> **Note**: This table tracks **core functionality** status. Update after each implementation round. Many unchecked items in the TODO are enhancements beyond the working baseline.

| Phase | Status |
|-------|--------|
| Design & Consistency (Section 0) | âœ… Implemented |
| Data Pipeline (Section 2) | âœ… Core Implemented |
| Feature Engineering (Section 3) | âœ… **Fully Implemented** (seasonal, future n_gxs, target encoding, feature selection) |
| Model Development | âœ… Core Implemented |
| Training Pipeline (Section 5) | âœ… Core Complete (CV, metadata, CLI, config weights) |
| Validation & Evaluation (Section 6) | âœ… Core Implemented |
| Inference & Submission | âœ… Core Implemented |
| Testing | âœ… **277 passed**, 0 skipped, 0 warnings |
| Documentation (Section 10) | âœ… **Core Complete** (README, configs/README, requirements.txt, reproduce.sh) |
| Optimization | â³ Not Started |
| Presentation | â³ Not Started |

---

## Quick Commands

```bash
# Run smoke tests
pytest tests/test_smoke.py -v

# ============================================================
# Data Build Commands (pre-build panels and feature matrices)
# ============================================================

# Build train panel (raw â†’ interim)
python -m src.data --split train --data-config configs/data.yaml

# Build test panel (raw â†’ interim)
python -m src.data --split test --data-config configs/data.yaml

# Build train Scenario 1 features (interim â†’ processed)
python -m src.data --split train --scenario 1 --mode train --data-config configs/data.yaml --features-config configs/features.yaml

# Build train Scenario 2 features (interim â†’ processed)
python -m src.data --split train --scenario 2 --mode train --data-config configs/data.yaml --features-config configs/features.yaml

# Build test Scenario 1 features (interim â†’ processed)
python -m src.data --split test --scenario 1 --mode test --data-config configs/data.yaml --features-config configs/features.yaml

# Build test Scenario 2 features (interim â†’ processed)
python -m src.data --split test --scenario 2 --mode test --data-config configs/data.yaml --features-config configs/features.yaml

# Force rebuild (ignore cache)
python -m src.data --split train --scenario 1 --mode train --force-rebuild --data-config configs/data.yaml --features-config configs/features.yaml

# ============================================================
# Training Commands (CLI flags locked â€” see top of file)
# ============================================================

# Train Scenario 1 model (CatBoost)
python -m src.train --scenario 1 --model catboost --model-config configs/model_cat.yaml --data-config configs/data.yaml --run-config configs/run_defaults.yaml

# Train Scenario 2 model (CatBoost)
python -m src.train --scenario 2 --model catboost --model-config configs/model_cat.yaml --data-config configs/data.yaml --run-config configs/run_defaults.yaml

# Train with forced data rebuild
python -m src.train --scenario 1 --model catboost --force-rebuild --model-config configs/model_cat.yaml --data-config configs/data.yaml --run-config configs/run_defaults.yaml

# ============================================================
# Inference & Submission Commands
# ============================================================

# Generate submission
python -m src.inference --model-s1 artifacts/model_s1.cbm --model-s2 artifacts/model_s2.cbm --output submissions/submission.csv --data-config configs/data.yaml

# Validate submission with official metric (IMPLEMENTATION TASK: create wrapper)
# Current status: metric_calculation.py usage needs a wrapper script.
# Target: python scripts/validate_submission.py --submission submissions/submission.csv --aux submissions/auxiliar_metric_computation.csv
# See section 10.1 for wrapper implementation task.

# ============================================================
# Visualization & Plot Generation Commands
# ============================================================

# Generate all plots for a given run
python -m src.plots --run-id 2025_11_28_001 --data-config configs/data.yaml --run-config configs/run_defaults.yaml --phase all

# Generate only EDA plots (distributions, erosion curves)
python -m src.plots --run-id 2025_11_28_001 --data-config configs/data.yaml --run-config configs/run_defaults.yaml --phase eda

# Generate training/validation curves
python -m src.plots --run-id 2025_11_28_001 --data-config configs/data.yaml --run-config configs/run_defaults.yaml --phase train

# Generate prediction vs actual & error analysis plots
python -m src.plots --run-id 2025_11_28_001 --data-config configs/data.yaml --run-config configs/run_defaults.yaml --phase test
```

---

## Notes

### Key Insights from Documentation
1. **Bucket is NEVER a feature** - it's computed from target, using it causes leakage
2. **Official metric weights early months heavily** - 50% of score from months 0-5 (S1) or 6-11 (S2)
3. **Bucket 1 (high erosion) is 2Ã— weighted** - focus on predicting high-erosion series correctly
4. **Series-level validation is critical** - never mix months from same series across train/val
5. **Target is y_norm, not volume** - model predicts normalized value, then inverse transform

### Known Issues
- [ ] NN model may need feature normalization (GBMs don't need it)
- [ ] Sample weights may need fine-tuning to match official metric exactly
- [ ] Some series may have missing pre-entry data

### Contact & Resources
- Competition Platform: [Check competition site]
- Official Metric Calculator: `docs/guide/metric_calculation.py`
- Submission Template: `docs/guide/submission_template.csv`
